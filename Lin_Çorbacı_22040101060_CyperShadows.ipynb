{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **in this notebook I followed an exploratory approach to see which methods work best for anomaly detection**\n",
        "\n",
        "the reason for the weak results is a problem with the dataset temporal relations and generation logic  I plan to fix that for the finals as well as trying a few more methods that might work better"
      ],
      "metadata": {
        "id": "KOmMgTu6z1rI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upload and explore dataset**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "psG9baKaOK15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "MnvurJptN1jK",
        "outputId": "8b7638d5-ea47-4509-e6e7-54b48a943e8b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-40148618-6082-45fe-9d3f-ce6c98e9a948\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-40148618-6082-45fe-9d3f-ce6c98e9a948\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving linux_auth_logs_full(new).csv to linux_auth_logs_full(new).csv\n",
            "csv_name set to: linux_auth_logs_full(new).csv\n"
          ]
        }
      ],
      "source": [
        "#uploading dataset\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    csv_name = next(iter(uploaded))\n",
        "except Exception:\n",
        "    csv_name = \"linux_auth_logs_labeled.csv\"\n",
        "\n",
        "print('csv_name set to:', csv_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import joblib\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-UEHm3FkOQo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read the dataset and pring the head\n",
        "df = pd.read_csv(csv_name)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gQ20dkMrOV53",
        "outputId": "b79b98ef-ceba-43f0-d5c8-937590fc9e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    timestamp       source_ip      server username service  \\\n",
              "0  2023-01-01 00:42:39.936864   175.59.175.26  web-sgp-02  aaron54   login   \n",
              "1  2023-01-01 00:42:44.872570  68.162.133.242   db-ber-01  aaron54     ssh   \n",
              "2  2023-01-01 00:42:45.001529  70.154.139.216  srv-nyc-01  aaron54    cron   \n",
              "3  2023-01-01 00:42:45.096641  40.138.230.209  srv-ldn-02  aaron54   login   \n",
              "4  2023-01-01 00:42:45.122138     9.151.194.9  web-sgp-02  aaron54    cron   \n",
              "\n",
              "   attempts   status  port protocol             comment anomaly_label  \\\n",
              "0         1   Failed  22.0     SSH2  User aaron54 login        normal   \n",
              "1         1   Failed  22.0     SSH2  User aaron54 login        normal   \n",
              "2         1   Failed  22.0     SSH2  User aaron54 login        normal   \n",
              "3         1  Success  22.0     SSH2  User aaron54 login        normal   \n",
              "4         1  Success  22.0     SSH2  User aaron54 login        normal   \n",
              "\n",
              "    delta_t  \n",
              "0       NaN  \n",
              "1  4.935706  \n",
              "2  0.128959  \n",
              "3  0.095112  \n",
              "4  0.025497  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29a327b6-cec5-4bec-a0bb-7d7c83b18e4d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>source_ip</th>\n",
              "      <th>server</th>\n",
              "      <th>username</th>\n",
              "      <th>service</th>\n",
              "      <th>attempts</th>\n",
              "      <th>status</th>\n",
              "      <th>port</th>\n",
              "      <th>protocol</th>\n",
              "      <th>comment</th>\n",
              "      <th>anomaly_label</th>\n",
              "      <th>delta_t</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-01-01 00:42:39.936864</td>\n",
              "      <td>175.59.175.26</td>\n",
              "      <td>web-sgp-02</td>\n",
              "      <td>aaron54</td>\n",
              "      <td>login</td>\n",
              "      <td>1</td>\n",
              "      <td>Failed</td>\n",
              "      <td>22.0</td>\n",
              "      <td>SSH2</td>\n",
              "      <td>User aaron54 login</td>\n",
              "      <td>normal</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-01-01 00:42:44.872570</td>\n",
              "      <td>68.162.133.242</td>\n",
              "      <td>db-ber-01</td>\n",
              "      <td>aaron54</td>\n",
              "      <td>ssh</td>\n",
              "      <td>1</td>\n",
              "      <td>Failed</td>\n",
              "      <td>22.0</td>\n",
              "      <td>SSH2</td>\n",
              "      <td>User aaron54 login</td>\n",
              "      <td>normal</td>\n",
              "      <td>4.935706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-01-01 00:42:45.001529</td>\n",
              "      <td>70.154.139.216</td>\n",
              "      <td>srv-nyc-01</td>\n",
              "      <td>aaron54</td>\n",
              "      <td>cron</td>\n",
              "      <td>1</td>\n",
              "      <td>Failed</td>\n",
              "      <td>22.0</td>\n",
              "      <td>SSH2</td>\n",
              "      <td>User aaron54 login</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.128959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-01-01 00:42:45.096641</td>\n",
              "      <td>40.138.230.209</td>\n",
              "      <td>srv-ldn-02</td>\n",
              "      <td>aaron54</td>\n",
              "      <td>login</td>\n",
              "      <td>1</td>\n",
              "      <td>Success</td>\n",
              "      <td>22.0</td>\n",
              "      <td>SSH2</td>\n",
              "      <td>User aaron54 login</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.095112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-01-01 00:42:45.122138</td>\n",
              "      <td>9.151.194.9</td>\n",
              "      <td>web-sgp-02</td>\n",
              "      <td>aaron54</td>\n",
              "      <td>cron</td>\n",
              "      <td>1</td>\n",
              "      <td>Success</td>\n",
              "      <td>22.0</td>\n",
              "      <td>SSH2</td>\n",
              "      <td>User aaron54 login</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.025497</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29a327b6-cec5-4bec-a0bb-7d7c83b18e4d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29a327b6-cec5-4bec-a0bb-7d7c83b18e4d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29a327b6-cec5-4bec-a0bb-7d7c83b18e4d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-48379eae-cc0a-400d-8a80-9fb81b858303\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-48379eae-cc0a-400d-8a80-9fb81b858303')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-48379eae-cc0a-400d-8a80-9fb81b858303 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset info\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNV-ebXAOXfT",
        "outputId": "a6c50e74-e904-49b6-98c1-6b21dab5eda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 12 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   timestamp      500000 non-null  object \n",
            " 1   source_ip      500000 non-null  object \n",
            " 2   server         500000 non-null  object \n",
            " 3   username       500000 non-null  object \n",
            " 4   service        500000 non-null  object \n",
            " 5   attempts       500000 non-null  int64  \n",
            " 6   status         500000 non-null  object \n",
            " 7   port           475000 non-null  float64\n",
            " 8   protocol       415000 non-null  object \n",
            " 9   comment        500000 non-null  object \n",
            " 10  anomaly_label  500000 non-null  object \n",
            " 11  delta_t        496987 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(9)\n",
            "memory usage: 45.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocessing the Dataset**"
      ],
      "metadata": {
        "id": "iJF_AntBoSvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'timestamp' column in the original df to datetime\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n"
      ],
      "metadata": {
        "id": "UORrSe7qSO_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['anomaly_label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "uqMiUYorJnae",
        "outputId": "b89f013c-7cfb-4b57-81aa-6ec6eb7c027c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "anomaly_label\n",
              "normal                  350000\n",
              "port_scan                60000\n",
              "brute_force              50000\n",
              "privilege_escalation     25000\n",
              "geo_anomaly              15000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anomaly_label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>normal</th>\n",
              "      <td>350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>port_scan</th>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brute_force</th>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>privilege_escalation</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>geo_anomaly</th>\n",
              "      <td>15000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "70I7YI5wJmLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Brute-force Attacks data prep"
      ],
      "metadata": {
        "id": "gDFaGAT7od1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort df_unlabeled by 'username' and 'timestamp'\n",
        "df_unlabeled = df_unlabeled.sort_values(['username', 'timestamp'])\n",
        "\n",
        "print(df_unlabeled.dtypes)\n",
        "df_unlabeled.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "U-1Uu_PHSVo1",
        "outputId": "8745b0c9-7f81-4763-f462-7ed67eae0244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_unlabeled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1676713199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sort df_unlabeled by 'username' and 'timestamp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_unlabeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_unlabeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_unlabeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_unlabeled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User-level aggregated features\n",
        "user_features = df.groupby('username').agg({\n",
        "    'timestamp': ['count', 'min', 'max'],  # Total activities, first & last login\n",
        "    'status': lambda x: (x == 'Failed').sum(),  # Total failures\n",
        "    'attempts': ['sum', 'mean', 'max'],  # Attempt patterns\n",
        "    'source_ip': 'nunique',  # Unique IPs used\n",
        "    'service': 'nunique',  # Unique services accessed\n",
        "    'server': 'nunique',  # Unique servers accessed\n",
        "    'port': 'nunique'  # Unique ports used\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten the column names\n",
        "user_features.columns = ['username', 'total_activities', 'first_activity', 'last_activity',\n",
        "                        'total_failures', 'total_attempts', 'avg_attempts', 'max_attempts',\n",
        "                        'unique_ips', 'unique_services', 'unique_servers', 'unique_ports']\n",
        "\n",
        "# Calculate derived features\n",
        "user_features['failure_rate'] = user_features['total_failures'] / user_features['total_activities']\n",
        "user_features['activity_duration_days'] = (user_features['last_activity'] - user_features['first_activity']).dt.total_seconds() / (24 * 3600)\n",
        "user_features['activities_per_day'] = user_features['total_activities'] / user_features['activity_duration_days'].replace(0, 1)  # Avoid division by zero"
      ],
      "metadata": {
        "id": "4Ng7N4I7SXyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_features.head(5))"
      ],
      "metadata": {
        "id": "rcDi8yfhStbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Geolocation Attacks data prep"
      ],
      "metadata": {
        "id": "YB7SnKDWol-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by username and timestamp\n",
        "df_geo = df_unlabeled.sort_values(['username', 'timestamp']).copy()\n",
        "\n",
        "# Calculate time differences between consecutive logins\n",
        "df_geo['time_since_last_login'] = df_geo.groupby('username')['timestamp'].diff()\n",
        "df_geo['time_since_last_login_hours'] = df_geo['time_since_last_login'].dt.total_seconds() / 3600\n",
        "\n",
        "# Server and IP access patterns (count occurrences)\n",
        "df_geo['server_access_count'] = df_geo.groupby(['username', 'server'])['server'].transform('count')\n",
        "df_geo['ip_access_count'] = df_geo.groupby(['username', 'source_ip'])['source_ip'].transform('count')\n",
        "\n",
        "# Create change flags (these will be boolean -> can be summed)\n",
        "df_geo['server_change'] = (df_geo.groupby('username')['server'].shift() != df_geo['server']).fillna(False)\n",
        "df_geo['ip_change'] = (df_geo.groupby('username')['source_ip'].shift() != df_geo['source_ip']).fillna(False)\n",
        "\n",
        "# Consecutive same location count\n",
        "df_geo['location_group'] = (df_geo['server_change'] | df_geo['ip_change']).cumsum()\n",
        "df_geo['consecutive_same_location'] = df_geo.groupby(['username', 'location_group']).cumcount() + 1\n",
        "\n",
        "print(\"Processed data sample:\")\n",
        "print(df_geo[['username', 'timestamp', 'source_ip', 'server', 'time_since_last_login_hours', 'server_change']].head(10))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bIZQSatxYn1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxI_SUmNoXpA"
      },
      "outputs": [],
      "source": [
        "# Separate aggregations by data type\n",
        "basic_features = df_geo.groupby('username').agg({\n",
        "    'server': 'nunique',\n",
        "    'source_ip': 'nunique',\n",
        "    'service': 'nunique',\n",
        "    'timestamp': ['count', 'min', 'max'],\n",
        "}).reset_index()\n",
        "basic_features.columns = ['username', 'unique_servers', 'unique_ips', 'unique_services', 'total_logins', 'first_seen', 'last_seen']\n",
        "\n",
        "# For numeric features - handle NaN during aggregation\n",
        "numeric_features = df_geo.groupby('username').agg({\n",
        "    'time_since_last_login_hours': ['mean', 'std', 'min', 'max'],\n",
        "    'server_access_count': ['mean', 'max'],\n",
        "    'ip_access_count': ['mean', 'max'],\n",
        "    'attempts': ['mean', 'max', 'sum'],\n",
        "    'port': 'nunique',\n",
        "}).reset_index()\n",
        "numeric_features.columns = ['username', 'avg_time_between_logins', 'std_time_between_logins', 'min_time_between_logins',\n",
        "                           'max_time_between_logins', 'avg_server_access', 'max_server_access', 'avg_ip_access',\n",
        "                           'max_ip_access', 'avg_attempts', 'max_attempts', 'total_attempts', 'unique_ports']\n",
        "\n",
        "# Boolean features\n",
        "boolean_features = df_geo.groupby('username').agg({\n",
        "    'server_change': 'sum',\n",
        "    'ip_change': 'sum',\n",
        "    'consecutive_same_location': 'max',\n",
        "}).reset_index()\n",
        "boolean_features.columns = ['username', 'server_changes', 'ip_changes', 'max_consecutive_same_location']\n",
        "\n",
        "# Status features\n",
        "status_features = df_geo.groupby('username').agg({\n",
        "    'status': lambda x: (x == 'failure').sum(),\n",
        "}).reset_index()\n",
        "status_features.columns = ['username', 'failed_logins']\n",
        "\n",
        "# Merge all features\n",
        "geo_features = basic_features.merge(numeric_features, on='username', how='left')\n",
        "geo_features = geo_features.merge(boolean_features, on='username', how='left')\n",
        "geo_features = geo_features.merge(status_features, on='username', how='left')\n",
        "\n",
        "#handle devision by zero\n",
        "geo_features['active_days'] = (\n",
        "    (geo_features['last_seen'] - geo_features['first_seen']).dt.total_seconds() / (24 * 3600)\n",
        ").replace(0, 1)  # Avoid division by zero\n",
        "\n",
        "# Safe division to prevent NaN\n",
        "geo_features['logins_per_day'] = np.where(\n",
        "    geo_features['active_days'] > 0,\n",
        "    geo_features['total_logins'] / geo_features['active_days'],\n",
        "    0\n",
        ")\n",
        "geo_features['servers_per_login'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    geo_features['unique_servers'] / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "geo_features['ips_per_login'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    geo_features['unique_ips'] / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "geo_features['failure_rate'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    geo_features['failed_logins'] / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "\n",
        "# Handle NaN in std_time_between_logins (occurs when users have only one login)\n",
        "geo_features['std_time_between_logins'] = geo_features['std_time_between_logins'].fillna(0)\n",
        "\n",
        "# Risk scores\n",
        "geo_features['location_volatility'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    (geo_features['server_changes'] + geo_features['ip_changes']) / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "\n",
        "geo_features['geo_risk_score'] = (\n",
        "    geo_features['unique_ips'] * 0.3 +\n",
        "    geo_features['unique_servers'] * 0.3 +\n",
        "    geo_features['location_volatility'] * 0.2 +\n",
        "    geo_features['failure_rate'] * 0.2\n",
        ")\n",
        "\n",
        "# Fill any remaining NaN values with median\n",
        "numeric_cols = geo_features.select_dtypes(include=np.number).columns\n",
        "geo_features[numeric_cols] = geo_features[numeric_cols].fillna(geo_features[numeric_cols].median())\n",
        "\n",
        "print(f\"\\nRemaining NaN values in geo_features: {geo_features.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Port Scanning Attack data prep"
      ],
      "metadata": {
        "id": "skmZa4c5o3Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For port scanning we want the DataFrame sorted by source_ip and timestamp\n",
        "df_by_source = df.sort_values(['source_ip', 'timestamp'])"
      ],
      "metadata": {
        "id": "KewlQMjgY0km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a 15-minute floor time window column on the df_by_source DataFrame\n",
        "df_by_source['time_window'] = df_by_source['timestamp'].dt.floor('15min')"
      ],
      "metadata": {
        "id": "-dJ307MmY8-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate port scanning features per source_ip/time_window using df_by_source\n",
        "port_scan_features = df_by_source.groupby(['source_ip', 'time_window']).agg({\n",
        "    'port': [\n",
        "        'count',  # total_port_attempts\n",
        "        'nunique',  # unique_ports_scanned\n",
        "        lambda x: (x > 10000).sum(),  # high_port_attempts (verify threshold)\n",
        "        'min',  # min_port\n",
        "        'max',  # max_port\n",
        "        lambda x: np.std(x) if len(x) > 1 else 0,  # port_std_dev\n",
        "    ],\n",
        "    'status': [\n",
        "        lambda x: (x == 'Failed').sum(),  # failed_attempts\n",
        "        lambda x: (x == 'Success').sum(),  # success_attempts\n",
        "    ],\n",
        "    'server': 'nunique',  # unique_servers_targeted\n",
        "    'service': 'nunique',  # unique_services_accessed\n",
        "    'username': 'nunique',  # unique_usernames_tried\n",
        "}).reset_index()\n"
      ],
      "metadata": {
        "id": "3UccSSQGY-7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port_scan_features.columns = [\n",
        "    'source_ip', 'time_window',\n",
        "    'total_port_attempts', 'unique_ports_scanned', 'high_port_attempts',\n",
        "    'min_port', 'max_port', 'port_std_dev',\n",
        "    'failed_attempts', 'success_attempts',\n",
        "    'unique_servers', 'unique_services', 'unique_usernames'\n",
        "]"
      ],
      "metadata": {
        "id": "zxs7oP82ZCGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port_scan_features['port_range'] = port_scan_features['max_port'] - port_scan_features['min_port']\n",
        "port_scan_features['high_port_ratio'] = port_scan_features['high_port_attempts'] / port_scan_features['total_port_attempts']\n",
        "port_scan_features['failure_rate'] = port_scan_features['failed_attempts'] / port_scan_features['total_port_attempts']\n",
        "port_scan_features['success_rate'] = port_scan_features['success_attempts'] / port_scan_features['total_port_attempts']\n",
        "port_scan_features['ports_per_minute'] = port_scan_features['unique_ports_scanned'] / 15  # 15-minute windows\n"
      ],
      "metadata": {
        "id": "7He4RigmZGnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port_scan_features = port_scan_features.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "print(\"Port scanning features shape:\", port_scan_features.shape)\n",
        "print(\"\\nFeatures created:\")\n",
        "print(port_scan_features.columns.tolist())\n",
        "print(\"\\nSample of engineered features:\")\n",
        "print(port_scan_features.head())"
      ],
      "metadata": {
        "id": "tDZ6Ig3hZI3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Privilage Escilation data prep"
      ],
      "metadata": {
        "id": "QyDsx95BpKlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_by_user = df.sort_values(['username', 'timestamp'])\n",
        "\n",
        "df_by_user.head(10)"
      ],
      "metadata": {
        "id": "cUhNAN3WZLgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "priv_keywords = [\n",
        "    \"sudo\", \"su -\", \"root\", \"chmod 777\", \"chown\", \"setuid\", \"setgid\",\n",
        "    \"privilege\", \"escalate\", \"elevate\", \"exploit\", \"admin access\",\n",
        "    \"bypass\", \"shell\", \"spawn\", \"superuser\", \"policy\"\n",
        "]\n",
        "\n",
        "def has_priv_keyword(text):\n",
        "    # Handle NaNs and non-string values safely\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return False\n",
        "    text = text.lower()\n",
        "    return any(k in text for k in priv_keywords)\n",
        "\n",
        "\n",
        "priv_user_features = df_by_user.groupby('username').agg({\n",
        "    'timestamp': ['count', 'min', 'max'],\n",
        "    'comment': lambda x: sum(has_priv_keyword(t) for t in x),\n",
        "    'service': 'nunique',\n",
        "    'server': 'nunique',\n",
        "    'attempts': ['sum', 'mean', 'max']}).reset_index()"
      ],
      "metadata": {
        "id": "KtM8F7lXZMV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "priv_user_features.columns = [\n",
        "    'username',\n",
        "    'total_activities', 'first_activity', 'last_activity',\n",
        "    'priv_keyword_count',\n",
        "    'unique_services', 'unique_servers',\n",
        "    'total_attempts', 'avg_attempts', 'max_attempts',\n",
        "]\n",
        "\n",
        "\n",
        "priv_user_features['priv_keyword_rate'] = (\n",
        "    priv_user_features['priv_keyword_count'] / priv_user_features['total_activities']\n",
        ")\n",
        "\n",
        "\n",
        "priv_user_features['activity_duration_days'] = (\n",
        "    (priv_user_features['last_activity'] - priv_user_features['first_activity'])\n",
        "    .dt.total_seconds() / (24 * 3600)\n",
        ")\n",
        "\n",
        "\n",
        "priv_user_features['activities_per_day'] = (\n",
        "    priv_user_features['total_activities'] /\n",
        "    priv_user_features['activity_duration_days'].replace(0, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "TUWMpm3mZRLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(priv_user_features.head(10))\n",
        "print(priv_user_features.describe())\n",
        "\n",
        "\n",
        "print(priv_user_features.sort_values('priv_keyword_count', ascending=False).head(10))"
      ],
      "metadata": {
        "id": "cqyTNgRyZSBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling data for PCA\n",
        "\n"
      ],
      "metadata": {
        "id": "29vot2Q1pSrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical columns to be used for scaling and PCA\n",
        "numerical_cols_bf = user_features.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Exclude 'username' and timestamp columns\n",
        "exclude_cols = ['username', 'first_activity', 'last_activity']\n",
        "bf_features_for_scaling = [col for col in numerical_cols_bf if col not in exclude_cols]\n",
        "\n",
        "# Select only the features for scaling\n",
        "X_bf_numerical = user_features[bf_features_for_scaling]\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the numerical features\n",
        "scaled_bf_features = scaler.fit_transform(X_bf_numerical)\n",
        "\n",
        "print(\"Shape of scaled brute force features:\", scaled_bf_features.shape)\n",
        "print(\"First 5 rows of scaled brute force features:\\n\", scaled_bf_features[:5])\n",
        "\n",
        "# --- PCA for brute-force features ---\n",
        "from sklearn.decomposition import PCA\n",
        "pca_bf = PCA(n_components=0.95)  # retain 95% variance\n",
        "pca_bf_features = pca_bf.fit_transform(scaled_bf_features)\n",
        "pca_bf_df = pd.DataFrame(pca_bf_features, columns=[f'PC_{i+1}' for i in range(pca_bf_features.shape[1])])\n",
        "print(f\"Number of components for Brute-force PCA: {pca_bf.n_components_}\")\n",
        "print(f\"Explained variance ratio for Brute-force PCA: {pca_bf.explained_variance_ratio_.sum():.2f}\")\n",
        "print(\"\\nFirst 5 rows of PCA transformed Brute-force features:\")\n",
        "print(pca_bf_df.head())"
      ],
      "metadata": {
        "id": "6m8Gond2ZbNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare numeric geolocation features and scale them before PCA\n",
        "numerical_cols_geo = geo_features.select_dtypes(include=np.number).columns.tolist()\n",
        "exclude_cols_geo = ['username', 'first_seen', 'last_seen']\n",
        "geo_features_for_scaling = [c for c in numerical_cols_geo if c not in exclude_cols_geo]\n",
        "X_geo_numerical = geo_features[geo_features_for_scaling]\n",
        "scaler_geo = StandardScaler()\n",
        "scaled_geo_features = scaler_geo.fit_transform(X_geo_numerical)\n",
        "\n",
        "# Initialize PCA for geolocation features\n",
        "pca_geo = PCA(n_components=0.95) # Retain 95% of variance\n",
        "\n",
        "# Fit PCA to the scaled data and transform it\n",
        "pca_geo_features = pca_geo.fit_transform(scaled_geo_features)\n",
        "\n",
        "# Create a DataFrame from the PCA transformed features for better readability\n",
        "pca_geo_df = pd.DataFrame(pca_geo_features, columns=[f'PC_{i+1}' for i in range(pca_geo_features.shape[1])])\n",
        "\n",
        "print(f\"Number of components for Geolocation PCA: {pca_geo.n_components_}\")\n",
        "print(f\"Explained variance ratio for Geolocation PCA: {pca_geo.explained_variance_ratio_.sum():.2f}\")\n",
        "print(\"\\nFirst 5 rows of PCA transformed Geolocation features:\")\n",
        "print(pca_geo_df.head())"
      ],
      "metadata": {
        "id": "GrnwN1exaEAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols_ps = port_scan_features.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "\n",
        "exclude_cols_ps = ['source_ip', 'time_window']\n",
        "ps_features_for_scaling = [col for col in numerical_cols_ps if col not in exclude_cols_ps]\n",
        "\n",
        "\n",
        "X_ps_numerical = port_scan_features[ps_features_for_scaling]\n",
        "\n",
        "scaler_ps = StandardScaler()\n",
        "\n",
        "scaled_ps_features = scaler_ps.fit_transform(X_ps_numerical)\n",
        "\n",
        "print(\"Shape of scaled port scanning features:\", scaled_ps_features.shape)\n",
        "print(\"First 5 rows of scaled port scanning features:\\n\", scaled_ps_features[:5])"
      ],
      "metadata": {
        "id": "Nnb1td2-aKG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_ps = PCA(n_components=0.95)\n",
        "\n",
        "\n",
        "pca_ps_features = pca_ps.fit_transform(scaled_ps_features)\n",
        "\n",
        "pca_ps_df = pd.DataFrame(pca_ps_features, columns=[f'PC_{i+1}' for i in range(pca_ps_features.shape[1])])\n",
        "\n",
        "print(f\"Number of components for Port Scanning PCA: {pca_ps.n_components_}\")\n",
        "print(f\"Explained variance ratio for Port Scanning PCA: {pca_ps.explained_variance_ratio_.sum():.2f}\")\n",
        "print(\"\\nFirst 5 rows of PCA transformed Port Scanning features:\")\n",
        "print(pca_ps_df.head())"
      ],
      "metadata": {
        "id": "DiLi_Ij2aMs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols_priv = priv_user_features.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "\n",
        "exclude_cols_priv = ['username', 'first_activity', 'last_activity']\n",
        "priv_features_for_scaling = [col for col in numerical_cols_priv if col not in exclude_cols_priv]\n",
        "\n",
        "\n",
        "X_priv_numerical = priv_user_features[priv_features_for_scaling]\n",
        "\n",
        "\n",
        "scaler_priv = StandardScaler()\n",
        "\n",
        "\n",
        "scaled_priv_features = scaler_priv.fit_transform(X_priv_numerical)\n",
        "\n",
        "print(\"Shape of scaled privilege escalation features:\", scaled_priv_features.shape)\n",
        "print(\"First 5 rows of scaled privilege escalation features:\\n\", scaled_priv_features[:5])"
      ],
      "metadata": {
        "id": "Mvae2yGmaPgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_priv = PCA(n_components=0.95)\n",
        "\n",
        "\n",
        "pca_priv_features = pca_priv.fit_transform(scaled_priv_features)\n",
        "\n",
        "\n",
        "pca_priv_df = pd.DataFrame(pca_priv_features, columns=[f'PC_{i+1}' for i in range(pca_priv_features.shape[1])])\n",
        "\n",
        "print(f\"Number of components for Privilege Escalation PCA: {pca_priv.n_components_}\")\n",
        "print(f\"Explained variance ratio for Privilege Escalation PCA: {pca_priv.explained_variance_ratio_.sum():.2f}\")\n",
        "print(\"\\nFirst 5 rows of PCA transformed Privilege Escalation features:\")\n",
        "print(pca_priv_df.head())"
      ],
      "metadata": {
        "id": "wf2rTwDuaT1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Selection using\n",
        "* heuristic selection\n",
        "* variance based selection\n",
        "* PCA-based selection\n"
      ],
      "metadata": {
        "id": "GgRcNXwjpfGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_features_by_attack_type(attack_type):\n",
        "    \"\"\"Return a heuristic list of columns to keep for the given attack type.\n",
        "    This is based on the engineered feature DataFrames created previously:\n",
        "    - 'brute_force' -> `user_features`\n",
        "    - 'geolocation' -> `geo_features`\n",
        "    - 'port_scan' -> `port_scan_features`\n",
        "    - 'privilege' -> `priv_user_features`\n",
        "    \"\"\"\n",
        "    attack_type = attack_type.lower()\n",
        "    if attack_type == 'brute_force' or attack_type == 'bruteforce':\n",
        "        return [\n",
        "            'total_activities', 'total_failures', 'failure_rate',\n",
        "            'avg_attempts', 'max_attempts', 'activities_per_day',\n",
        "            'unique_ips'\n",
        "        ]\n",
        "    elif attack_type == 'geolocation' or attack_type == 'geo':\n",
        "        return [\n",
        "            'unique_ips', 'unique_servers', 'unique_services',\n",
        "            'logins_per_day', 'active_days', 'geo_risk_score'\n",
        "        ]\n",
        "    elif attack_type == 'port_scan' or attack_type == 'port_scanning':\n",
        "        return [\n",
        "            'total_port_attempts', 'unique_ports_scanned', 'ports_per_minute',\n",
        "            'port_range', 'high_port_ratio', 'unique_services', 'unique_usernames'\n",
        "        ]\n",
        "    elif attack_type == 'privilege' or attack_type == 'priv_escalation':\n",
        "        return [\n",
        "            'priv_keyword_count', 'priv_keyword_rate', 'activities_per_day',\n",
        "            'unique_services', 'unique_servers'\n",
        "        ]\n",
        "    else:\n",
        "\n",
        "        return None\n",
        "\n",
        "def variance_based_selection(df, top_k=10, corr_threshold=0.9):\n",
        "    \"\"\"Select top_k features by variance, then drop highly correlated features.\n",
        "    Returns ordered list of selected columns.\n",
        "    \"\"\"\n",
        "\n",
        "    num = df.select_dtypes(include=[np.number]).copy()\n",
        "    if num.shape[1] == 0:\n",
        "        return []\n",
        "    variances = num.var().sort_values(ascending=False)\n",
        "    top_cols = variances.head(top_k).index.tolist()\n",
        "\n",
        "\n",
        "    keep = []\n",
        "    corr = num[top_cols].corr().abs()\n",
        "    for col in top_cols:\n",
        "        if not any((corr.loc[col, other] > corr_threshold) for other in keep):\n",
        "            keep.append(col)\n",
        "    return keep\n",
        "\n",
        "def pca_loadings_selection(df, n_components=5, top_k=10):\n",
        "    \"\"\"Use PCA loadings to pick features that have large absolute weights\n",
        "    across the first `n_components` principal components.\n",
        "    Returns a list of selected column names.\n",
        "    \"\"\"\n",
        "    num = df.select_dtypes(include=[np.number]).copy()\n",
        "    if num.shape[1] == 0:\n",
        "        return []\n",
        "\n",
        "    scaler_tmp = StandardScaler()\n",
        "    X = scaler_tmp.fit_transform(num.fillna(0))\n",
        "    pca_tmp = PCA(n_components=min(n_components, X.shape[1]))\n",
        "    pca_tmp.fit(X)\n",
        "    loadings = np.abs(pca_tmp.components_).sum(axis=0)\n",
        "    feature_scores = pd.Series(loadings, index=num.columns)\n",
        "    selected = feature_scores.sort_values(ascending=False).head(top_k).index.tolist()\n",
        "    return selected\n",
        "\n",
        "def prepare_selected_features(df, selected_cols, scaler=None):\n",
        "    \"\"\"Given a DataFrame and a list of columns, return scaled numpy array\n",
        "    ready for anomaly models and the fitted scaler.\n",
        "    \"\"\"\n",
        "    if selected_cols is None:\n",
        "\n",
        "        selected_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    X = df[selected_cols].fillna(0).astype(float)\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        Xs = scaler.fit_transform(X)\n",
        "    else:\n",
        "        Xs = scaler.transform(X)\n",
        "    return Xs, scaler, selected_cols"
      ],
      "metadata": {
        "id": "ujToNBfKaUyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear everything and start fresh\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== STARTING FRESH WITH CONSISTENT DATA ===\")\n",
        "\n",
        "# 1. Make sure df is your 500K dataset\n",
        "print(f\"Using df with {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
        "\n",
        "# 2. Reset all feature DataFrames to df\n",
        "user_features = df.copy()\n",
        "geo_features = df.copy()\n",
        "priv_user_features = df.copy()\n",
        "port_scan_features = df.copy()\n",
        "\n",
        "print(f\"\\n All feature DataFrames now have {df.shape[0]:,} rows\")\n",
        "\n",
        "# 3. Recreate ground truth from df\n",
        "print(\"\\nCreating ground truth labels from df...\")\n",
        "user_ground_truth_df = pd.DataFrame({\n",
        "    'username': df['username'],\n",
        "    'ground_truth': df['anomaly_label'].map({\n",
        "        'normal': 0,\n",
        "        'brute_force': 1,\n",
        "        'geo_anomaly': 2,\n",
        "        'privilege_escalation': 3,\n",
        "        'port_scan': 0\n",
        "    })\n",
        "})\n",
        "\n",
        "port_scan_ground_truth_df = pd.DataFrame({\n",
        "    'source_ip': df['source_ip'],\n",
        "    'time_window': df['timestamp'].dt.floor('H'),\n",
        "    'ground_truth': (df['anomaly_label'] == 'port_scan').astype(int)\n",
        "})\n",
        "\n",
        "print(f\"user_ground_truth_df: {user_ground_truth_df.shape}\")\n",
        "print(f\"Brute force anomalies: {(user_ground_truth_df['ground_truth'] == 1).sum():,}\")\n",
        "print(f\"Port scan anomalies: {port_scan_ground_truth_df['ground_truth'].sum():,}\")\n",
        "\n",
        "# 4. Now run your feature selection code (the block you posted)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running feature selection with consistent data...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear old results\n",
        "results = {}\n",
        "\n",
        "# Run your feature selection code (it will now use consistent 500K data)\n",
        "# ... paste your feature selection code here ...\n",
        "\n",
        "# 5. THEN run the stratified split\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running stratified splits...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run the rebuild_stratified_splits() function\n",
        "results = rebuild_stratified_splits()\n",
        "\n",
        "print(\"\\n DONE! You should now have thousands of anomalies per split.\")"
      ],
      "metadata": {
        "id": "ns3ewJZkejKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "persist = {k: {\n",
        "    'heuristic_cols': v['heuristic_cols'],\n",
        "    'variance_cols': v['variance_cols'],\n",
        "    'pca_cols': v['pca_cols']\n",
        "} for k, v in results.items()}\n",
        "with open('selected_feature_sets.json', 'w') as f:\n",
        "    json.dump(persist, f, indent=2)\n",
        "print('Saved selected_feature_sets.json')"
      ],
      "metadata": {
        "id": "7ak6ujxAsR-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting the dataset and training prep"
      ],
      "metadata": {
        "id": "rH5cpZpUqS48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === STRATIFIED SPLIT REBUILD - FIXED VERSION ===\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "def rebuild_stratified_splits():\n",
        "    \"\"\"\n",
        "    CORRECT stratified splitting that preserves rare anomaly samples\n",
        "    \"\"\"\n",
        "    global results\n",
        "\n",
        "    rebuild_count = 0\n",
        "    print(\"Rebuilding splits with PROPER stratified sampling...\")\n",
        "\n",
        "    for attack_type in ['brute_force', 'geolocation', 'privilege', 'port_scan']:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing: {attack_type}\")\n",
        "        print('='*60)\n",
        "\n",
        "        # Get the CORRECT base dataframe and labels\n",
        "        if attack_type == 'brute_force':\n",
        "            base_df = user_features\n",
        "            # Brute force labels: 1 if brute_force, 0 otherwise\n",
        "            y_full = (user_ground_truth_df['ground_truth'] == 1).astype(int).values\n",
        "\n",
        "        elif attack_type == 'geolocation':\n",
        "            base_df = geo_features\n",
        "            # Geo anomaly labels: 1 if geo_anomaly, 0 otherwise\n",
        "            y_full = (user_ground_truth_df['ground_truth'] == 2).astype(int).values\n",
        "\n",
        "        elif attack_type == 'privilege':\n",
        "            base_df = priv_user_features\n",
        "            # Privilege escalation labels: 1 if privilege_escalation, 0 otherwise\n",
        "            y_full = (user_ground_truth_df['ground_truth'] == 3).astype(int).values\n",
        "\n",
        "        elif attack_type == 'port_scan':\n",
        "            base_df = port_scan_features\n",
        "            # Port scan labels: 1 if port_scan, 0 otherwise\n",
        "            y_full = (port_scan_ground_truth_df['ground_truth'] == 1).astype(int).values\n",
        "\n",
        "        # Check anomaly count\n",
        "        n_anomalies = np.sum(y_full == 1)\n",
        "        n_total = len(y_full)\n",
        "        anomaly_rate = n_anomalies / n_total\n",
        "\n",
        "        print(f\"Total samples: {n_total:,}\")\n",
        "        print(f\"Anomalies: {n_anomalies:,} ({anomaly_rate:.4%})\")\n",
        "\n",
        "        # If too few anomalies, use manual split instead of stratified\n",
        "        if n_anomalies < 10:\n",
        "            print(f\"  Too few anomalies ({n_anomalies}) - using MANUAL split\")\n",
        "            # Use manual split function (defined below)\n",
        "            train_idx, val_idx, test_idx = manual_split_for_rare_class(y_full, min_anomalies=5)\n",
        "        else:\n",
        "            # Use STRATIFIED split\n",
        "            print(f\" Using stratified split\")\n",
        "\n",
        "            # Split 1: Train+Val (80%) vs Test (20%)\n",
        "            sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "            train_val_idx, test_idx = next(sss1.split(np.zeros(len(y_full)), y_full))\n",
        "\n",
        "            # Split 2: Train (75% of train_val) vs Val (25% of train_val)\n",
        "            sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
        "            temp_y = y_full[train_val_idx]\n",
        "            train_temp_idx, val_temp_idx = next(sss2.split(np.zeros(len(temp_y)), temp_y))\n",
        "\n",
        "            train_idx = train_val_idx[train_temp_idx]\n",
        "            val_idx = train_val_idx[val_temp_idx]\n",
        "\n",
        "        # Create splits\n",
        "        final_train_idx = train_idx\n",
        "        final_val_idx = val_idx\n",
        "        final_test_idx = test_idx\n",
        "\n",
        "        # Verify anomaly distribution\n",
        "        train_anom = np.sum(y_full[final_train_idx] == 1)\n",
        "        val_anom = np.sum(y_full[final_val_idx] == 1)\n",
        "        test_anom = np.sum(y_full[final_test_idx] == 1)\n",
        "\n",
        "        print(f\"Train: {len(final_train_idx):,} samples, {train_anom} anomalies\")\n",
        "        print(f\"Val:   {len(final_val_idx):,} samples, {val_anom} anomalies\")\n",
        "        print(f\"Test:  {len(final_test_idx):,} samples, {test_anom} anomalies\")\n",
        "\n",
        "        # For each feature selection method\n",
        "        for sel_label, col_key in [('heuristic', 'heuristic_cols'),\n",
        "                                   ('variance', 'variance_cols'),\n",
        "                                   ('pca', 'pca_cols')]:\n",
        "\n",
        "            # Get selected columns\n",
        "            if attack_type not in results or col_key not in results[attack_type]:\n",
        "                print(f\"  Skipping {sel_label}: no columns found\")\n",
        "                continue\n",
        "\n",
        "            sel_cols = results[attack_type][col_key]\n",
        "            if not sel_cols or len(sel_cols) == 0:\n",
        "                print(f\"  Skipping {sel_label}: empty column list\")\n",
        "                continue\n",
        "\n",
        "            # Prepare features for ALL data first\n",
        "            X_full, scaler_sel, used_cols = prepare_selected_features(base_df, sel_cols, scaler=None)\n",
        "\n",
        "            # Create splits by indexing\n",
        "            X_train = X_full[final_train_idx]\n",
        "            X_val = X_full[final_val_idx]\n",
        "            X_test = X_full[final_test_idx]\n",
        "            y_train = y_full[final_train_idx]\n",
        "            y_val = y_full[final_val_idx]\n",
        "            y_test = y_full[final_test_idx]\n",
        "\n",
        "            # Store in results\n",
        "            if attack_type not in results:\n",
        "                results[attack_type] = {}\n",
        "\n",
        "            results[attack_type][f'{sel_label}_X_train'] = X_train\n",
        "            results[attack_type][f'{sel_label}_X_val'] = X_val\n",
        "            results[attack_type][f'{sel_label}_X_test'] = X_test\n",
        "            results[attack_type][f'{sel_label}_y_train'] = y_train\n",
        "            results[attack_type][f'{sel_label}_y_val'] = y_val\n",
        "            results[attack_type][f'{sel_label}_y_test'] = y_test\n",
        "            results[attack_type][f'{sel_label}_scaler'] = scaler_sel\n",
        "\n",
        "            rebuild_count += 1\n",
        "\n",
        "            print(f\"   {sel_label}: Train={X_train.shape}, Anom={y_train.sum()}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" Rebuild complete. Updated {rebuild_count} splits.\")\n",
        "    print('='*60)\n",
        "    return results\n",
        "\n",
        "def manual_split_for_rare_class(y, min_anomalies=5):\n",
        "    \"\"\"\n",
        "    Manual split when stratified can't work (too few anomalies)\n",
        "    \"\"\"\n",
        "    # Find anomaly indices\n",
        "    anomaly_idx = np.where(y == 1)[0]\n",
        "    normal_idx = np.where(y == 0)[0]\n",
        "\n",
        "    # Shuffle\n",
        "    np.random.shuffle(anomaly_idx)\n",
        "    np.random.shuffle(normal_idx)\n",
        "\n",
        "    # Split anomalies: 70% train, 15% val, 15% test\n",
        "    n_anom = len(anomaly_idx)\n",
        "    n_train_anom = int(0.7 * n_anom)\n",
        "    n_val_anom = int(0.15 * n_anom)\n",
        "\n",
        "    # Ensure minimum anomalies\n",
        "    n_train_anom = max(n_train_anom, min_anomalies)\n",
        "    n_val_anom = max(n_val_anom, min(2, n_anom - n_train_anom))\n",
        "    n_test_anom = n_anom - n_train_anom - n_val_anom\n",
        "\n",
        "    # Split normals: same ratios\n",
        "    n_norm = len(normal_idx)\n",
        "    n_train_norm = int(0.7 * n_norm)\n",
        "    n_val_norm = int(0.15 * n_norm)\n",
        "\n",
        "    # Create indices\n",
        "    train_idx = np.concatenate([\n",
        "        anomaly_idx[:n_train_anom],\n",
        "        normal_idx[:n_train_norm]\n",
        "    ])\n",
        "\n",
        "    val_idx = np.concatenate([\n",
        "        anomaly_idx[n_train_anom:n_train_anom + n_val_anom],\n",
        "        normal_idx[n_train_norm:n_train_norm + n_val_norm]\n",
        "    ])\n",
        "\n",
        "    test_idx = np.concatenate([\n",
        "        anomaly_idx[n_train_anom + n_val_anom:],\n",
        "        normal_idx[n_train_norm + n_val_norm:]\n",
        "    ])\n",
        "\n",
        "    # Shuffle each set\n",
        "    np.random.shuffle(train_idx)\n",
        "    np.random.shuffle(val_idx)\n",
        "    np.random.shuffle(test_idx)\n",
        "\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "# Run the fix\n",
        "results = rebuild_stratified_splits()\n",
        "\n",
        "# Verify the splits\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFICATION - Anomaly counts per split:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary_data = []\n",
        "for attack_type in results:\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        y_train = results[attack_type].get(f'{prefix}_y_train')\n",
        "        y_val = results[attack_type].get(f'{prefix}_y_val')\n",
        "        y_test = results[attack_type].get(f'{prefix}_y_test')\n",
        "\n",
        "        if y_train is not None:\n",
        "            summary_data.append({\n",
        "                'attack_type': attack_type,\n",
        "                'prefix': prefix,\n",
        "                'n_train': len(y_train),\n",
        "                'n_val': len(y_val) if y_val is not None else 0,\n",
        "                'n_test': len(y_test) if y_test is not None else 0,\n",
        "                'anoms_train': int(y_train.sum()),\n",
        "                'anoms_val': int(y_val.sum()) if y_val is not None else 0,\n",
        "                'anoms_test': int(y_test.sum()) if y_test is not None else 0\n",
        "            })\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string())"
      ],
      "metadata": {
        "id": "BV1nSCJbHrHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your 500,000 row generated dataset\n",
        "print(f\"Total dataset shape: {df.shape}\")\n",
        "print(f\"\\nAnomaly distribution:\")\n",
        "print(df['anomaly_label'].value_counts())\n",
        "print(df['anomaly_label'].value_counts(normalize=True))\n",
        "\n",
        "# You should see something like:\n",
        "# normal                  350000  (70%)\n",
        "# port_scan                60000  (12%)\n",
        "# brute_force              50000  (10%)\n",
        "# privilege_escalation     25000  (5%)\n",
        "# geo_anomaly              15000  (3%)"
      ],
      "metadata": {
        "id": "peWsYZllbF-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DEBUGGING THE DATA PIPELINE ===\")\n",
        "\n",
        "# 1. Check your feature DataFrames\n",
        "print(\"\\n1. Feature DataFrame sizes:\")\n",
        "print(f\"user_features shape: {user_features.shape if 'user_features' in globals() else 'NOT FOUND'}\")\n",
        "print(f\"geo_features shape: {geo_features.shape if 'geo_features' in globals() else 'NOT FOUND'}\")\n",
        "print(f\"priv_user_features shape: {priv_user_features.shape if 'priv_user_features' in globals() else 'NOT FOUND'}\")\n",
        "print(f\"port_scan_features shape: {port_scan_features.shape if 'port_scan_features' in globals() else 'NOT FOUND'}\")\n",
        "\n",
        "# 2. Check ground truth DataFrames\n",
        "print(\"\\n2. Ground truth DataFrame sizes:\")\n",
        "print(f\"user_ground_truth_df shape: {user_ground_truth_df.shape if 'user_ground_truth_df' in globals() else 'NOT FOUND'}\")\n",
        "print(f\"port_scan_ground_truth_df shape: {port_scan_ground_truth_df.shape if 'port_scan_ground_truth_df' in globals() else 'NOT FOUND'}\")\n",
        "\n",
        "# 3. Check if they match df\n",
        "print(\"\\n3. Comparing to df (500K):\")\n",
        "if 'user_ground_truth_df' in globals():\n",
        "    print(f\"user_ground_truth_df rows: {user_ground_truth_df.shape[0]:,}\")\n",
        "    print(f\"df rows: {df.shape[0]:,}\")\n",
        "    if user_ground_truth_df.shape[0] == df.shape[0]:\n",
        "        print(\" MATCH!\")\n",
        "    else:\n",
        "        print(f\" MISMATCH! Difference: {df.shape[0] - user_ground_truth_df.shape[0]:,} rows\")"
      ],
      "metadata": {
        "id": "tfuxi_NPdpN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'anomaly_label' column to create df_unlabeled\n",
        "df_unlabeled = df.drop('anomaly_label', axis=1)"
      ],
      "metadata": {
        "id": "RR31bPcCbtv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEBUG: Check what's actually in results\n",
        "print(\"DEBUG: Checking results structure...\")\n",
        "for attack_type in results.keys():\n",
        "    print(f\"\\n{attack_type}:\")\n",
        "    print(f\"  Keys available: {list(results[attack_type].keys())}\")\n",
        "\n",
        "    # Count how many splits we have\n",
        "    split_count = sum(1 for k in results[attack_type].keys() if k.endswith('_X_train'))\n",
        "    print(f\"  Number of splits found: {split_count}\")\n",
        "\n",
        "    # Show sample sizes for each split\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        train_key = f'{prefix}_X_train'\n",
        "        if train_key in results[attack_type]:\n",
        "            X_train = results[attack_type][train_key]\n",
        "            y_train = results[attack_type].get(f'{prefix}_y_train', None)\n",
        "            anomalies = int(y_train.sum()) if y_train is not None else 'N/A'\n",
        "            print(f\"  {prefix}: {X_train.shape} samples, {anomalies} anomalies\")"
      ],
      "metadata": {
        "id": "shdIH945gfOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Isolation forest Model"
      ],
      "metadata": {
        "id": "teDOPzDXqhFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "\n",
        "print(\"Training Isolation Forest models with PROPER contamination settings...\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing {attack_type}...\")\n",
        "    print('='*60)\n",
        "\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if selection_method_key.endswith('_X_train'):\n",
        "            # Extract data\n",
        "            X_train = methods[selection_method_key]\n",
        "            X_val = methods[selection_method_key.replace('_X_train', '_X_val')]\n",
        "            X_test = methods[selection_method_key.replace('_X_train', '_X_test')]\n",
        "            y_train = methods[selection_method_key.replace('_X_train', '_y_train')]\n",
        "\n",
        "            # Get prefix (heuristic/variance/pca)\n",
        "            prefix = selection_method_key.replace('_X_train', '')\n",
        "\n",
        "            # Calculate ACTUAL anomaly rate in training data\n",
        "            train_anomaly_rate = np.mean(y_train == 1)\n",
        "\n",
        "            # Set contamination based on ACTUAL rate\n",
        "            # For Isolation Forest, contamination should be slightly higher than actual rate\n",
        "            # because it's estimating the proportion of outliers\n",
        "            contamination = min(0.1, max(0.001, train_anomaly_rate * 1.5))\n",
        "\n",
        "            print(f\"  {prefix}: {X_train.shape[0]:,} train samples, {int(y_train.sum())} anomalies ({train_anomaly_rate:.4%})\")\n",
        "            print(f\"    Using contamination: {contamination:.4f}\")\n",
        "\n",
        "            # Initialize Isolation Forest with PROPER settings\n",
        "            isolation_forest_model = IsolationForest(\n",
        "                n_estimators=100,           # More trees for better stability\n",
        "                max_samples='auto',         # Use all samples for each tree\n",
        "                contamination=contamination, # Match actual anomaly rate\n",
        "                max_features=1.0,           # Use all features\n",
        "                bootstrap=False,            # Don't bootstrap for anomaly detection\n",
        "                random_state=42,\n",
        "                n_jobs=-1,                  # Use all cores\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            isolation_forest_model.fit(X_train)\n",
        "\n",
        "            # Calculate anomaly scores for ALL splits\n",
        "            train_scores = isolation_forest_model.score_samples(X_train)\n",
        "            val_scores = isolation_forest_model.score_samples(X_val)\n",
        "            test_scores = isolation_forest_model.score_samples(X_test)\n",
        "\n",
        "            # For IF, LOWER scores = more anomalous\n",
        "            # Store as negative scores so higher = more anomalous (consistent with other models)\n",
        "            train_scores_neg = -train_scores\n",
        "            val_scores_neg = -val_scores\n",
        "            test_scores_neg = -test_scores\n",
        "\n",
        "            # Store everything\n",
        "            model_key = f'{prefix}_isolation_forest_model'\n",
        "            train_scores_key = f'{prefix}_train_scores_if'\n",
        "            val_scores_key = f'{prefix}_val_scores_if'\n",
        "            test_scores_key = f'{prefix}_test_scores_if'\n",
        "\n",
        "            results[attack_type][model_key] = isolation_forest_model\n",
        "            results[attack_type][train_scores_key] = train_scores_neg\n",
        "            results[attack_type][val_scores_key] = val_scores_neg\n",
        "            results[attack_type][test_scores_key] = test_scores_neg\n",
        "\n",
        "            # Also store the contamination used\n",
        "            results[attack_type][f'{prefix}_if_contamination'] = contamination\n",
        "\n",
        "            print(f\"     Trained, scores calculated\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Isolation Forest training complete!\")\n",
        "print('='*60)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED VERIFICATION of Isolation Forest Models\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "found_models = 0\n",
        "for attack_type in ['brute_force', 'geolocation', 'port_scan', 'privilege']:\n",
        "    if attack_type in results:\n",
        "        print(f\"\\n{attack_type.upper()}:\")\n",
        "        for prefix in ['heuristic', 'variance', 'pca']:\n",
        "            model_key = f'{prefix}_isolation_forest_model'\n",
        "            train_scores_key = f'{prefix}_train_scores_if'\n",
        "\n",
        "            if model_key in results[attack_type]:\n",
        "                found_models += 1\n",
        "                model = results[attack_type][model_key]\n",
        "                contamination = results[attack_type].get(f'{prefix}_if_contamination', 'N/A')\n",
        "\n",
        "                # Get sample counts\n",
        "                y_train = results[attack_type].get(f'{prefix}_y_train')\n",
        "                train_anomalies = int(y_train.sum()) if y_train is not None else 'N/A'\n",
        "\n",
        "                print(f\"  {prefix}:\")\n",
        "                print(f\"    Model type: {type(model).__name__}\")\n",
        "                print(f\"    Contamination: {contamination}\")\n",
        "                print(f\"    Train samples: {results[attack_type][train_scores_key].shape[0]:,}\")\n",
        "                print(f\"    Train anomalies: {train_anomalies}\")\n",
        "\n",
        "                # Check score statistics\n",
        "                scores = results[attack_type][train_scores_key]\n",
        "                print(f\"    Score range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
        "                print(f\"    Mean score: {scores.mean():.3f}\")\n",
        "                print(f\"    Std score: {scores.std():.3f}\")\n",
        "\n",
        "                # Check if model is predicting anything\n",
        "                if hasattr(model, 'predict'):\n",
        "                    # Sample predictions\n",
        "                    X_sample = results[attack_type].get(f'{prefix}_X_train')\n",
        "                    if X_sample is not None and len(X_sample) > 0:\n",
        "                        sample_pred = model.predict(X_sample[:10])\n",
        "                        anomalies_pred = sum(sample_pred == -1)\n",
        "                        print(f\"    Sample predictions: {anomalies_pred}/10 flagged as anomalies\")\n",
        "            else:\n",
        "                print(f\"  {prefix}: NO MODEL FOUND (key: {model_key})\")\n",
        "    else:\n",
        "        print(f\"\\n{attack_type.upper()}: NOT IN RESULTS DICTIONARY\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"SUMMARY: Found {found_models} Isolation Forest models\")\n",
        "print('='*80)\n",
        "\n",
        "# Quick check for common issues\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMMON ISSUES CHECKLIST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "issues = []\n",
        "for attack_type in results.keys():\n",
        "    # Check if splits exist\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        train_key = f'{prefix}_X_train'\n",
        "        if train_key in results[attack_type]:\n",
        "            X_train = results[attack_type][train_key]\n",
        "            y_train = results[attack_type].get(f'{prefix}_y_train')\n",
        "\n",
        "            if y_train is not None:\n",
        "                anomaly_rate = np.mean(y_train == 1) if len(y_train) > 0 else 0\n",
        "                print(f\"{attack_type} - {prefix}:\")\n",
        "                print(f\"  Samples: {X_train.shape[0]:,}\")\n",
        "                print(f\"  Anomalies: {int(y_train.sum()):,}\")\n",
        "                print(f\"  Anomaly rate: {anomaly_rate:.4%}\")\n",
        "\n",
        "                if anomaly_rate == 0:\n",
        "                    issues.append(f\"{attack_type}-{prefix}: NO ANOMALIES in training!\")\n",
        "                elif anomaly_rate < 0.001:\n",
        "                    issues.append(f\"{attack_type}-{prefix}: VERY FEW anomalies ({anomaly_rate:.4%})\")\n",
        "\n",
        "if issues:\n",
        "    print(f\"\\n  ISSUES FOUND:\")\n",
        "    for issue in issues:\n",
        "        print(f\"  - {issue}\")\n",
        "else:\n",
        "    print(\"\\n No major issues found in training data\")"
      ],
      "metadata": {
        "id": "eA9QCpl_HtYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoencoders Model training"
      ],
      "metadata": {
        "id": "AGbR3QFaqnh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Check GPU availability and configure TensorFlow for better performance\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Configure TensorFlow for better performance\n",
        "tf.config.optimizer.set_jit(True)  # Enable XLA compilation\n",
        "\n",
        "# Pre-compile a model template to avoid recompiling in loop\n",
        "def create_autoencoder(input_dim):\n",
        "    encoding_dim = max(1, input_dim // 4)  # Smaller encoding for faster training\n",
        "\n",
        "    autoencoder = Sequential([\n",
        "        Dense(encoding_dim, activation='relu', input_shape=(input_dim,)),\n",
        "        Dense(input_dim, activation='linear')\n",
        "    ])\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "# Collect all training data first, then train in batch\n",
        "training_tasks = []\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if selection_method_key.endswith('_X_train'):\n",
        "            training_tasks.append({\n",
        "                'attack_type': attack_type,\n",
        "                'selection_method': selection_method_key,\n",
        "                'X_train': methods[selection_method_key],\n",
        "                'X_val': methods[selection_method_key.replace('_X_train', '_X_val')]\n",
        "            })\n",
        "\n",
        "print(f\"Found {len(training_tasks)} models to train\")\n",
        "\n",
        "# Train models sequentially with progress tracking\n",
        "for i, task in enumerate(training_tasks):\n",
        "    print(f\"Training model {i+1}/{len(training_tasks)}: {task['attack_type']} - {task['selection_method']}\")\n",
        "\n",
        "    X_train = task['X_train']\n",
        "    X_val = task['X_val']\n",
        "\n",
        "    # Skip if data is too small\n",
        "    if X_train.shape[0] < 10 or X_val.shape[0] < 5:\n",
        "        print(f\"  Skipping - insufficient data: train={X_train.shape[0]}, val={X_val.shape[0]}\")\n",
        "        continue\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    # Create and train model\n",
        "    autoencoder = create_autoencoder(input_dim)\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,  # Reduced patience for faster training\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train with smaller epochs and larger batch size\n",
        "    history = autoencoder.fit(\n",
        "        X_train, X_train,\n",
        "        epochs=30,  # Reduced epochs\n",
        "        batch_size=min(64, X_train.shape[0]),  # Adaptive batch size\n",
        "        shuffle=True,\n",
        "        validation_data=(X_val, X_val),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Calculate reconstruction errors with batching\n",
        "    train_predictions = autoencoder.predict(X_train, batch_size=256, verbose=0)\n",
        "    val_predictions = autoencoder.predict(X_val, batch_size=256, verbose=0)\n",
        "\n",
        "    train_reconstruction_errors = np.mean(np.square(X_train - train_predictions), axis=1)\n",
        "    val_reconstruction_errors = np.mean(np.square(X_val - val_predictions), axis=1)\n",
        "\n",
        "    # Store results\n",
        "    model_key = task['selection_method'].replace('_X_train', '_autoencoder_model')\n",
        "    train_errors_key = task['selection_method'].replace('_X_train', '_train_reconstruction_errors')\n",
        "    val_errors_key = task['selection_method'].replace('_X_train', '_val_reconstruction_errors')\n",
        "\n",
        "    results[task['attack_type']][model_key] = autoencoder\n",
        "    results[task['attack_type']][train_errors_key] = train_reconstruction_errors\n",
        "    results[task['attack_type']][val_errors_key] = val_reconstruction_errors\n",
        "\n",
        "    print(f\"  Completed - Final loss: {history.history['loss'][-1]:.4f}\")\n",
        "\n",
        "print(\"All models trained successfully!\")"
      ],
      "metadata": {
        "id": "O6vTd4o_Hv0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the models"
      ],
      "metadata": {
        "id": "6m4v-AZxy5P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()): # Iterate over a copy of keys\n",
        "        if selection_method_key.endswith('_isolation_forest_model'):\n",
        "            # Extract method prefix (e.g., 'heuristic', 'variance', 'pca')\n",
        "            method_prefix = selection_method_key.replace('_isolation_forest_model', '')\n",
        "\n",
        "            # Retrieve the trained Isolation Forest model\n",
        "            isolation_forest_model = methods[selection_method_key]\n",
        "\n",
        "            # Construct the key for the corresponding X_test data\n",
        "            X_test_key = f'{method_prefix}_X_test'\n",
        "            X_test = methods[X_test_key]\n",
        "\n",
        "            # Calculate anomaly scores for X_test\n",
        "            test_scores = isolation_forest_model.decision_function(X_test)\n",
        "\n",
        "            # Store the test scores back into the results dictionary\n",
        "            test_scores_key = f'{method_prefix}_test_scores'\n",
        "            results[attack_type][test_scores_key] = test_scores\n",
        "\n",
        "            print(f\"Anomaly scores calculated for {attack_type} - {method_prefix}. Shape: {test_scores.shape}\")\n",
        "\n",
        "# Verify one example to ensure test scores are stored\n",
        "print(\"\\nBrute-force (heuristic) test scores shape:\")\n",
        "print(f\"Test scores shape: {results['brute_force']['heuristic_test_scores'].shape}\")"
      ],
      "metadata": {
        "id": "MafgrH2pHxxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluating Autoencoder Models on test data...\")\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if selection_method_key.endswith('_autoencoder_model'):\n",
        "            # Extract the selection method name (e.g., 'heuristic', 'variance', 'pca')\n",
        "            method_prefix = selection_method_key.replace('_autoencoder_model', '')\n",
        "\n",
        "            # Get the trained Autoencoder model\n",
        "            autoencoder_model = methods[selection_method_key]\n",
        "\n",
        "            # Get the corresponding X_test data\n",
        "            X_test_key = f'{method_prefix}_X_test'\n",
        "            X_test = methods[X_test_key]\n",
        "\n",
        "            # Predict reconstruction on the test set\n",
        "            # Ensure X_test has data before predicting\n",
        "            if X_test.shape[0] > 0:\n",
        "                test_predictions = autoencoder_model.predict(X_test, batch_size=256, verbose=0)\n",
        "                # Calculate reconstruction errors\n",
        "                test_reconstruction_errors = np.mean(np.square(X_test - test_predictions), axis=1)\n",
        "            else:\n",
        "                test_reconstruction_errors = np.array([]) # Empty array if no test data\n",
        "\n",
        "            # Store the test reconstruction errors in the results dictionary\n",
        "            test_errors_key = f'{method_prefix}_test_reconstruction_errors'\n",
        "            results[attack_type][test_errors_key] = test_reconstruction_errors\n",
        "\n",
        "            print(f\"  {attack_type} - {method_prefix} Autoencoder test reconstruction errors shape: {test_reconstruction_errors.shape}\")\n",
        "\n",
        "print(\"\\nAll models evaluated on test data. Scores are stored in the 'results' dictionary.\")\n",
        "print(\"Accessing Isolation Forest test scores for brute_force, heuristic method: results['brute_force']['heuristic_test_scores']\")\n",
        "print(\"Accessing Autoencoder test reconstruction errors for port_scan, pca method: results['port_scan']['pca_test_reconstruction_errors']\")"
      ],
      "metadata": {
        "id": "L4UooJ68IJ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_anomaly'] = df['anomaly_label'].map({'anomaly': 1, 'normal': 0})\n",
        "print(\"Created 'is_anomaly' column:\")\n",
        "print(df[['anomaly_label', 'is_anomaly']].head())"
      ],
      "metadata": {
        "id": "nfvaiFuNk_Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. For user-based features (brute_force, geolocation, privilege)\n",
        "user_ground_truth_df = df.groupby('username')['is_anomaly'].max().reset_index()\n",
        "user_ground_truth_df.rename(columns={'is_anomaly': 'ground_truth'}, inplace=True)\n",
        "print(\"\\nUser-based ground truth labels (user_ground_truth_df):\")\n",
        "print(user_ground_truth_df.head())\n",
        "\n",
        "# 3. For IP/time-window-based features (port_scan)\n",
        "# Add a 'time_window' column to the original df DataFrame\n",
        "df['time_window'] = df['timestamp'].dt.floor('15min')\n",
        "\n",
        "port_scan_ground_truth_df = df.groupby(['source_ip', 'time_window'])['is_anomaly'].max().reset_index()\n",
        "port_scan_ground_truth_df.rename(columns={'is_anomaly': 'ground_truth'}, inplace=True)\n",
        "print(\"\\nIP/time-window-based ground truth labels (port_scan_ground_truth_df):\")\n",
        "print(port_scan_ground_truth_df.head())"
      ],
      "metadata": {
        "id": "kkj1slmok_wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Helper function to get the base DataFrame for an attack type\n",
        "def get_base_dataframe(attack_type):\n",
        "    if attack_type == 'brute_force':\n",
        "        return user_features\n",
        "    elif attack_type == 'geolocation':\n",
        "        return geo_features\n",
        "    elif attack_type == 'privilege':\n",
        "        return priv_user_features\n",
        "    elif attack_type == 'port_scan':\n",
        "        return port_scan_features\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown attack type: {attack_type}\")\n",
        "\n",
        "# Iterate through each attack type and feature selection method\n",
        "for attack_type, methods in results.items():\n",
        "    print(f\"Processing ground truth for {attack_type}...\")\n",
        "\n",
        "    # Get the base DataFrame for the current attack type\n",
        "    base_df = get_base_dataframe(attack_type)\n",
        "\n",
        "    if attack_type in ['brute_force', 'geolocation', 'privilege']:\n",
        "        # Merge with user_ground_truth_df\n",
        "        merged_df = base_df.merge(user_ground_truth_df, on='username', how='left')\n",
        "        y_full = merged_df['ground_truth'].values\n",
        "    elif attack_type == 'port_scan':\n",
        "        # Merge with port_scan_ground_truth_df\n",
        "        merged_df = base_df.merge(port_scan_ground_truth_df, on=['source_ip', 'time_window'], how='left')\n",
        "        y_full = merged_df['ground_truth'].values\n",
        "    else:\n",
        "        raise ValueError(f\"Unhandled attack type for ground truth merging: {attack_type}\")\n",
        "\n",
        "    # Fill any NaN values in y_full (if any arise from merges) with 0 (normal)\n",
        "    y_full = np.nan_to_num(y_full, nan=0.0)\n",
        "\n",
        "\n",
        "    sample_X_key = next((k for k in methods if k.endswith('_X_train')), None)\n",
        "    if sample_X_key is None:\n",
        "        print(f\"  Warning: Could not find any _X_train for {attack_type}, skipping Y splits.\")\n",
        "        continue\n",
        "\n",
        "    sample_X_train = methods[sample_X_key]\n",
        "\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if selection_method_key.endswith('_X_train'):\n",
        "            method_prefix = selection_method_key.replace('_X_train', '')\n",
        "            X_train = methods[selection_method_key]\n",
        "            X_val = methods[f'{method_prefix}_X_val']\n",
        "            X_test = methods[f'{method_prefix}_X_test']\n",
        "\n",
        "\n",
        "            # First, get the full aligned y for the current feature set. This handles potential filtering/sorting.\n",
        "            if attack_type in ['brute_force', 'geolocation', 'privilege']:\n",
        "                current_feature_df = get_base_dataframe(attack_type)\n",
        "                full_feature_and_y_df = current_feature_df.merge(user_ground_truth_df, on='username', how='left')\n",
        "\n",
        "\n",
        "            # Align y_full to the *original* (pre-split) feature set it corresponds to (e.g., user_features)\n",
        "            if attack_type == 'brute_force':\n",
        "                y_full_aligned = user_features.merge(user_ground_truth_df, on='username', how='left')['ground_truth'].values\n",
        "            elif attack_type == 'geolocation':\n",
        "                y_full_aligned = geo_features.merge(user_ground_truth_df, on='username', how='left')['ground_truth'].values\n",
        "            elif attack_type == 'privilege':\n",
        "                y_full_aligned = priv_user_features.merge(user_ground_truth_df, on='username', how='left')['ground_truth'].values\n",
        "            elif attack_type == 'port_scan':\n",
        "                y_full_aligned = port_scan_features.merge(port_scan_ground_truth_df, on=['source_ip', 'time_window'], how='left')['ground_truth'].values\n",
        "            y_full_aligned = np.nan_to_num(y_full_aligned, nan=0.0)\n",
        "\n",
        "\n",
        "            # Use the exact same split parameters as for X to ensure alignment\n",
        "            y_temp_train, y_test = train_test_split(y_full_aligned, test_size=0.2, random_state=42)\n",
        "            y_train, y_val = train_test_split(y_temp_train, test_size=0.25, random_state=42)\n",
        "\n",
        "            # Store the y splits\n",
        "            results[attack_type][f'{method_prefix}_y_train'] = y_train\n",
        "            results[attack_type][f'{method_prefix}_y_val'] = y_val\n",
        "            results[attack_type][f'{method_prefix}_y_test'] = y_test\n",
        "\n",
        "            print(f\"  {attack_type} - {method_prefix}: y_train shape={y_train.shape}, y_val shape={y_val.shape}, y_test shape={y_test.shape}\")\n",
        "\n",
        "\n",
        "print(\"\\nGround truth labels splitting complete.\")\n",
        "\n",
        "# Verify one example\n",
        "print(\"\\nBrute-force (heuristic) ground truth splits shapes:\")\n",
        "print(f\"y_train shape: {results['brute_force']['heuristic_y_train'].shape}\")\n",
        "print(f\"y_val shape: {results['brute_force']['heuristic_y_val'].shape}\")\n",
        "print(f\"y_test shape: {results['brute_force']['heuristic_y_test'].shape}\")"
      ],
      "metadata": {
        "id": "63ytbEGXINbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "print(\"Evaluating Isolation Forest models performance...\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()): # Iterate over a copy of keys\n",
        "        # Check if the key corresponds to the test scores of an Isolation Forest model\n",
        "        if selection_method_key.endswith('_test_scores') and 'isolation_forest' not in selection_method_key: # Ensure it's IF scores, not AE errors\n",
        "            method_prefix = selection_method_key.replace('_test_scores', '')\n",
        "\n",
        "            # Retrieve Isolation Forest test scores\n",
        "            test_scores = methods[selection_method_key]\n",
        "\n",
        "            # Retrieve the corresponding y_test ground truth labels\n",
        "            y_test_key = f'{method_prefix}_y_test'\n",
        "            y_test = methods[y_test_key]\n",
        "\n",
        "            # Only calculate if there are actual anomalies in y_test (or normal instances for AP)\n",
        "            if len(np.unique(y_test)) > 1:\n",
        "                # Calculate ROC AUC score\n",
        "                roc_auc = roc_auc_score(y_test, test_scores)\n",
        "\n",
        "                # Calculate Average Precision score\n",
        "                avg_precision = average_precision_score(y_test, test_scores)\n",
        "\n",
        "                # Store these scores in the results dictionary\n",
        "                results[attack_type][f'{method_prefix}_if_roc_auc'] = roc_auc\n",
        "                results[attack_type][f'{method_prefix}_if_avg_precision'] = avg_precision\n",
        "\n",
        "                print(f\"  {attack_type} - {method_prefix} IF: ROC AUC = {roc_auc:.4f}, Avg Precision = {avg_precision:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {attack_type} - {method_prefix} IF: Skipping metrics calculation due to single class in y_test (all normal or all anomaly).\")\n",
        "\n",
        "print(\"\\nIsolation Forest model performance evaluation complete.\")\n",
        "\n",
        "# Verify one example to ensure metrics are stored, only if they exist\n",
        "print(\"\\nBrute-force (heuristic) Isolation Forest metrics:\")\n",
        "if 'heuristic_if_roc_auc' in results['brute_force']:\n",
        "    print(f\"ROC AUC: {results['brute_force']['heuristic_if_roc_auc']:.4f}\")\n",
        "    print(f\"Average Precision: {results['brute_force']['heuristic_if_avg_precision']:.4f}\")\n",
        "else:\n",
        "    print(\"  Metrics for brute_force - heuristic IF were skipped (single class in y_test).\")"
      ],
      "metadata": {
        "id": "E9EDr8yQIWlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Distribution of 'anomaly_label' in original df:\")\n",
        "print(df['anomaly_label'].value_counts())\n",
        "\n",
        "print(\"\\nDistribution of 'is_anomaly' in original df (after mapping):\")\n",
        "print(df['is_anomaly'].value_counts())\n",
        "\n",
        "print(\"\\nDistribution of 'ground_truth' in user_ground_truth_df:\")\n",
        "print(user_ground_truth_df['ground_truth'].value_counts())\n",
        "\n",
        "print(\"\\nDistribution of 'ground_truth' in port_scan_ground_truth_df:\")\n",
        "print(port_scan_ground_truth_df['ground_truth'].value_counts())"
      ],
      "metadata": {
        "id": "iuMA7fR3IbQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_anomaly'] = df['anomaly_label'].apply(lambda x: 1 if x != 'normal' else 0)\n",
        "print(\"Corrected 'is_anomaly' column creation:\")\n",
        "print(df[['anomaly_label', 'is_anomaly']].head())\n",
        "print(df['is_anomaly'].value_counts())\n",
        "\n",
        "user_ground_truth_df = df.groupby('username')['is_anomaly'].max().reset_index()\n",
        "user_ground_truth_df.rename(columns={'is_anomaly': 'ground_truth'}, inplace=True)\n",
        "print(\"\\nRe-generated User-based ground truth labels (user_ground_truth_df):\")\n",
        "print(user_ground_truth_df.head())\n",
        "print(user_ground_truth_df['ground_truth'].value_counts())\n",
        "\n",
        "\n",
        "df['time_window'] = df['timestamp'].dt.floor('15min')\n",
        "\n",
        "port_scan_ground_truth_df = df.groupby(['source_ip', 'time_window'])['is_anomaly'].max().reset_index()\n",
        "port_scan_ground_truth_df.rename(columns={'is_anomaly': 'ground_truth'}, inplace=True)\n",
        "print(\"\\nRe-generated IP/time-window-based ground truth labels (port_scan_ground_truth_df):\")\n",
        "print(port_scan_ground_truth_df.head())\n",
        "print(port_scan_ground_truth_df['ground_truth'].value_counts())"
      ],
      "metadata": {
        "id": "r6vK8FaCIdJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Helper function to get the base DataFrame for an attack type\n",
        "def get_base_dataframe(attack_type):\n",
        "    if attack_type == 'brute_force':\n",
        "        return user_features\n",
        "    elif attack_type == 'geolocation':\n",
        "        return geo_features\n",
        "    elif attack_type == 'privilege':\n",
        "        return priv_user_features\n",
        "    elif attack_type == 'port_scan':\n",
        "        return port_scan_features\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown attack type: {attack_type}\")\n",
        "\n",
        "# Iterate through each attack type and feature selection method\n",
        "for attack_type, methods in results.items():\n",
        "    print(f\"Processing ground truth for {attack_type}...\")\n",
        "\n",
        "    # Align y_full to the *original* (pre-split) feature set it corresponds to (e.g., user_features)\n",
        "    if attack_type == 'brute_force':\n",
        "        y_full_aligned = user_features.merge(user_ground_truth_df, on='username', how='left')['ground_truth'].values\n",
        "    elif attack_type == 'geolocation':\n",
        "        y_full_aligned = geo_features.merge(user_ground_truth_df, on='username', how='left')['ground_truth'].values\n",
        "    elif attack_type == 'privilege':\n",
        "        y_full_aligned = priv_user_features.merge(user_ground_truth_df, on='username', how='left')['ground_truth'].values\n",
        "    elif attack_type == 'port_scan':\n",
        "        y_full_aligned = port_scan_features.merge(port_scan_ground_truth_df, on=['source_ip', 'time_window'], how='left')['ground_truth'].values\n",
        "    y_full_aligned = np.nan_to_num(y_full_aligned, nan=0.0)\n",
        "\n",
        "    # Find a selection method's X_train to get its size, to ensure consistent splitting\n",
        "    sample_X_key = next((k for k in methods if k.endswith('_X_train')), None)\n",
        "    if sample_X_key is None:\n",
        "        print(f\"  Warning: Could not find any _X_train for {attack_type}, skipping Y splits.\")\n",
        "        continue\n",
        "\n",
        "    # Use the exact same split parameters as for X to ensure alignment\n",
        "    y_temp_train, y_test = train_test_split(y_full_aligned, test_size=0.2, random_state=42)\n",
        "    y_train, y_val = train_test_split(y_temp_train, test_size=0.25, random_state=42)\n",
        "\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if selection_method_key.endswith('_X_train'):\n",
        "            method_prefix = selection_method_key.replace('_X_train', '')\n",
        "\n",
        "            # Store the y splits\n",
        "            results[attack_type][f'{method_prefix}_y_train'] = y_train\n",
        "            results[attack_type][f'{method_prefix}_y_val'] = y_val\n",
        "            results[attack_type][f'{method_prefix}_y_test'] = y_test\n",
        "\n",
        "            print(f\"  {attack_type} - {method_prefix}: y_train shape={{y_train.shape}}, y_val shape={{y_val.shape}}, y_test shape={{y_test.shape}}\")\n",
        "\n",
        "\n",
        "print(\"\\nGround truth labels splitting complete.\")\n",
        "\n",
        "# Verify one example\n",
        "print(\"\\nBrute-force (heuristic) ground truth splits shapes:\")\n",
        "print(f\"y_train shape: {results['brute_force']['heuristic_y_train'].shape}\")\n",
        "print(f\"y_val shape: {results['brute_force']['heuristic_y_val'].shape}\")\n",
        "print(f\"y_test shape: {results['brute_force']['heuristic_y_test'].shape}\")"
      ],
      "metadata": {
        "id": "s2c4u3fVIiZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Evaluating Isolation Forest models performance...\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()): # Iterate over a copy of keys\n",
        "\n",
        "        if selection_method_key.endswith('_test_scores') and 'if' in selection_method_key:\n",
        "            method_prefix = selection_method_key.replace('_test_scores', '')\n",
        "\n",
        "\n",
        "            if_test_scores_key = f'{method_prefix}' # This is incorrect, it should be the actual key with scores\n",
        "            if_test_scores = methods[selection_method_key]\n",
        "\n",
        "            # Retrieve the corresponding y_test ground truth labels\n",
        "            y_test_key = f'{method_prefix.replace(\"_if\", \"\")}_y_test'\n",
        "            y_test = methods[y_test_key]\n",
        "\n",
        "            # Only calculate if there are actual anomalies in y_test (or normal instances for AP)\n",
        "            if len(np.unique(y_test)) > 1:\n",
        "                # Calculate ROC AUC score\n",
        "                roc_auc = roc_auc_score(y_test, if_test_scores)\n",
        "\n",
        "                # Calculate Average Precision score\n",
        "                avg_precision = average_precision_score(y_test, if_test_scores)\n",
        "\n",
        "                # Store these scores in the results dictionary\n",
        "                results[attack_type][f'{method_prefix}_roc_auc'] = roc_auc\n",
        "                results[attack_type][f'{method_prefix}_avg_precision'] = avg_precision\n",
        "\n",
        "                print(f\"  {attack_type} - {method_prefix} IF: ROC AUC = {roc_auc:.4f}, Avg Precision = {avg_precision:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {attack_type} - {method_prefix} IF: Skipping metrics calculation due to single class in y_test (all normal or all anomaly).\")\n",
        "\n",
        "print(\"\\nIsolation Forest model performance evaluation complete.\")\n",
        "\n",
        "# Verify one example to ensure metrics are stored, only if they exist\n",
        "print(\"\\nBrute-force (heuristic) Isolation Forest metrics:\")\n",
        "if 'heuristic_if_test_scores_roc_auc' in results['brute_force']:\n",
        "    print(f\"ROC AUC: {results['brute_force']['heuristic_if_test_scores_roc_auc']:.4f}\")\n",
        "    print(f\"Average Precision: {results['brute_force']['heuristic_if_test_scores_avg_precision']:.4f}\")\n",
        "else:\n",
        "\n",
        "    if 'heuristic_test_scores_roc_auc' in results['brute_force']:\n",
        "        print(f\"ROC AUC: {results['brute_force']['heuristic_test_scores_roc_auc']:.4f}\")\n",
        "        print(f\"Average Precision: {results['brute_force']['heuristic_test_scores_avg_precision']:.4f}\")\n",
        "    else:\n",
        "        print(\"  Metrics for brute_force - heuristic IF were skipped (single class in y_test) or keys not found.\")\n"
      ],
      "metadata": {
        "id": "hN_I-SA_IkY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Verifying unique classes in y_test arrays:\")\n",
        "for attack_type, methods in results.items():\n",
        "    print(f\"\\nAttack Type: {attack_type}\")\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if selection_method_key.endswith('_y_test'):\n",
        "            method_prefix = selection_method_key.replace('_y_test', '')\n",
        "            y_test = methods[selection_method_key]\n",
        "            unique_classes = np.unique(y_test)\n",
        "            class_counts = {cls: np.sum(y_test == cls) for cls in unique_classes}\n",
        "            print(f\"  {method_prefix}_y_test: Unique classes = {unique_classes}, Counts = {class_counts}\")\n"
      ],
      "metadata": {
        "id": "9U2kvrJHInFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "print(\"Evaluating Isolation Forest models performance with corrected ground truth...\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        # Identify the Isolation Forest test scores keys (e.g., 'heuristic_test_scores', 'variance_test_scores')\n",
        "        if selection_method_key.endswith('_test_scores') and 'isolation_forest_model' not in selection_method_key:\n",
        "            method_prefix = selection_method_key.replace('_test_scores', '')\n",
        "\n",
        "            # Retrieve Isolation Forest test scores\n",
        "            test_scores = methods[selection_method_key]\n",
        "\n",
        "            # Retrieve the corresponding y_test ground truth labels\n",
        "            y_test_key = f'{method_prefix}_y_test'\n",
        "            y_test = methods[y_test_key]\n",
        "\n",
        "            # Since the previous step verified that all y_test arrays now contain both classes,\n",
        "            # this check should now pass for all relevant cases.\n",
        "            if len(np.unique(y_test)) > 1:\n",
        "                # Calculate ROC AUC score\n",
        "                roc_auc = roc_auc_score(y_test, test_scores)\n",
        "\n",
        "                # Calculate Average Precision score\n",
        "                avg_precision = average_precision_score(y_test, test_scores)\n",
        "\n",
        "                # Store these scores in the results dictionary under clear keys\n",
        "                results[attack_type][f'{method_prefix}_if_roc_auc'] = roc_auc\n",
        "                results[attack_type][f'{method_prefix}_if_avg_precision'] = avg_precision\n",
        "\n",
        "                print(f\"  {attack_type} - {method_prefix} IF: ROC AUC = {roc_auc:.4f}, Avg Precision = {avg_precision:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {attack_type} - {method_prefix} IF: Still skipping metrics calculation (unexpected single class in y_test).\")\n",
        "\n",
        "print(\"\\nIsolation Forest model performance evaluation complete.\")\n",
        "\n",
        "# Verify one example to ensure metrics are stored, now expecting them to exist\n",
        "print(\"\\nBrute-force (heuristic) Isolation Forest metrics (re-evaluated):\")\n",
        "if 'heuristic_if_roc_auc' in results['brute_force']:\n",
        "    print(f\"ROC AUC: {results['brute_force']['heuristic_if_roc_auc']:.4f}\")\n",
        "    print(f\"Average Precision: {results['brute_force']['heuristic_if_avg_precision']:.4f}\")\n",
        "else:\n",
        "    print(\"  Metrics for brute_force - heuristic IF were still not calculated, please investigate further.\")\n"
      ],
      "metadata": {
        "id": "t1tr6RHGIp9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "print(\"Evaluating Autoencoder models performance...\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        # Identify the Autoencoder test reconstruction errors keys\n",
        "        if selection_method_key.endswith('_test_reconstruction_errors') and 'autoencoder' not in selection_method_key:\n",
        "            method_prefix = selection_method_key.replace('_test_reconstruction_errors', '')\n",
        "\n",
        "            # Retrieve Autoencoder test reconstruction errors\n",
        "            test_reconstruction_errors = methods[selection_method_key]\n",
        "\n",
        "            # Retrieve the corresponding y_test ground truth labels\n",
        "            y_test_key = f'{method_prefix}_y_test'\n",
        "            y_test = methods[y_test_key]\n",
        "\n",
        "            # Only calculate if there are actual anomalies in y_test (or normal instances for AP)\n",
        "            if len(np.unique(y_test)) > 1:\n",
        "                # Calculate ROC AUC score\n",
        "                roc_auc = roc_auc_score(y_test, test_reconstruction_errors)\n",
        "\n",
        "                # Calculate Average Precision score\n",
        "                avg_precision = average_precision_score(y_test, test_reconstruction_errors)\n",
        "\n",
        "                # Store these scores in the results dictionary under clear keys\n",
        "                results[attack_type][f'{method_prefix}_ae_roc_auc'] = roc_auc\n",
        "                results[attack_type][f'{method_prefix}_ae_avg_precision'] = avg_precision\n",
        "\n",
        "                print(f\"  {attack_type} - {method_prefix} AE: ROC AUC = {roc_auc:.4f}, Avg Precision = {avg_precision:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {attack_type} - {method_prefix} AE: Skipping metrics calculation due to single class in y_test (all normal or all anomaly).\")\n",
        "\n",
        "print(\"\\nAutoencoder model performance evaluation complete.\")\n",
        "\n",
        "# Verify one example to ensure metrics are stored\n",
        "print(\"\\nBrute-force (heuristic) Autoencoder metrics:\")\n",
        "if 'heuristic_ae_roc_auc' in results['brute_force']:\n",
        "    print(f\"ROC AUC: {results['brute_force']['heuristic_ae_roc_auc']:.4f}\")\n",
        "    print(f\"Average Precision: {results['brute_force']['heuristic_ae_avg_precision']:.4f}\")\n",
        "else:\n",
        "    print(\"  Metrics for brute_force - heuristic AE were skipped (single class in y_test).\")"
      ],
      "metadata": {
        "id": "hZ_ZYrWoIrkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Performance Metrics Summary ---\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    print(f\"\\nAttack Type: {attack_type.replace('_', ' ').title()}\")\n",
        "    print(\"--------------------------------\")\n",
        "\n",
        "    # Iterate through each selection method (heuristic, variance, pca)\n",
        "    # We'll use a fixed list of prefixes to ensure consistent iteration order\n",
        "    selection_method_prefixes = ['heuristic', 'variance', 'pca']\n",
        "\n",
        "    for prefix in selection_method_prefixes:\n",
        "        print(f\"  Feature Selection Method: {prefix.title()}\")\n",
        "\n",
        "        # Isolation Forest Metrics\n",
        "        if_roc_auc_key = f'{prefix}_if_roc_auc'\n",
        "        if_avg_precision_key = f'{prefix}_if_avg_precision'\n",
        "\n",
        "        if if_roc_auc_key in methods and if_avg_precision_key in methods:\n",
        "            print(f\"    Isolation Forest - ROC AUC: {methods[if_roc_auc_key]:.4f}, Avg Precision: {methods[if_avg_precision_key]:.4f}\")\n",
        "        else:\n",
        "            print(f\"    Isolation Forest - Metrics N/A (likely skipped due to single class in y_test).\")\n",
        "\n",
        "        # Autoencoder Metrics\n",
        "        ae_roc_auc_key = f'{prefix}_ae_roc_auc'\n",
        "        ae_avg_precision_key = f'{prefix}_ae_avg_precision'\n",
        "\n",
        "        if ae_roc_auc_key in methods and ae_avg_precision_key in methods:\n",
        "            print(f\"    Autoencoder      - ROC AUC: {methods[ae_roc_auc_key]:.4f}, Avg Precision: {methods[ae_avg_precision_key]:.4f}\")\n",
        "        else:\n",
        "            print(f\"    Autoencoder      - Metrics N/A (likely skipped due to single class in y_test).\")\n",
        "\n",
        "print(\"\\n--- Summary Complete ---\")"
      ],
      "metadata": {
        "id": "8hXmOOAZItPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#saving the models"
      ],
      "metadata": {
        "id": "oaSMLCoT2TkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(isolation_forest_model, f'isolation_forest_{attack_type}_{method_prefix}.pkl')\n",
        "joblib.dump(scaler, f'scaler_{attack_type}_{method_prefix}.pkl')\n",
        "\n",
        "# For Keras autoencoder:\n",
        "autoencoder.save(f'autoencoder_{attack_type}_{method_prefix}.h5')"
      ],
      "metadata": {
        "id": "2fSUfQY42Xcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62b68a2c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Retrieve the Isolation Forest test scores and corresponding ground truth labels\n",
        "attack_type = 'brute_force'\n",
        "selection_method = 'pca'\n",
        "\n",
        "if_test_scores = results[attack_type][f'{selection_method}_test_scores']\n",
        "y_test = results[attack_type][f'{selection_method}_y_test']\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({\n",
        "    'Anomaly Score': if_test_scores,\n",
        "    'Ground Truth': y_test\n",
        "})\n",
        "\n",
        "# Map ground truth labels to more descriptive names for the legend\n",
        "plot_df['Ground Truth Label'] = plot_df['Ground Truth'].map({0: 'Normal', 1: 'Anomaly'})\n",
        "\n",
        "# 2. Create a histogram or a kernel density estimate (KDE) plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=plot_df, x='Anomaly Score', hue='Ground Truth Label', kde=True, bins=50, palette='viridis', common_norm=False)\n",
        "\n",
        "# 4. Add appropriate labels and a title\n",
        "plt.xlabel('Isolation Forest Anomaly Score')\n",
        "plt.ylabel('Density / Count')\n",
        "plt.title(f'Isolation Forest Anomaly Score Distribution for {attack_type.replace(\"_\", \" \").title()} ({selection_method.upper()})')\n",
        "plt.legend(title='Ground Truth')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# 5. Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64338217"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Retrieve the Isolation Forest test scores and corresponding ground truth labels\n",
        "attack_type = 'brute_force'\n",
        "selection_method = 'pca'\n",
        "\n",
        "if_test_scores = results[attack_type][f'{selection_method}_test_scores']\n",
        "y_test = results[attack_type][f'{selection_method}_y_test']\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({\n",
        "    'Anomaly Score': if_test_scores,\n",
        "    'Ground Truth': y_test\n",
        "})\n",
        "\n",
        "# Map ground truth labels to more descriptive names for the legend\n",
        "plot_df['Ground Truth Label'] = plot_df['Ground Truth'].map({0: 'Normal', 1: 'Anomaly'})\n",
        "\n",
        "# 2. Create a histogram or a kernel density estimate (KDE) plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=plot_df, x='Anomaly Score', hue='Ground Truth Label', kde=True, bins=50, palette='viridis', common_norm=False)\n",
        "\n",
        "# 4. Add appropriate labels and a title\n",
        "plt.xlabel('Isolation Forest Anomaly Score')\n",
        "plt.ylabel('Density / Count')\n",
        "plt.title(f'Isolation Forest Anomaly Score Distribution for {attack_type.replace(\"_\", \" \").title()} ({selection_method.upper()})')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# 5. Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a255fef0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Retrieve the Autoencoder test reconstruction errors and corresponding ground truth labels\n",
        "attack_type = 'brute_force'\n",
        "selection_method = 'pca'\n",
        "\n",
        "ae_test_errors = results[attack_type][f'{selection_method}_test_reconstruction_errors']\n",
        "y_test = results[attack_type][f'{selection_method}_y_test']\n",
        "\n",
        "# 2. Create a Pandas DataFrame for plotting\n",
        "plot_df = pd.DataFrame({\n",
        "    'Reconstruction Error': ae_test_errors,\n",
        "    'Ground Truth': y_test\n",
        "})\n",
        "\n",
        "# 3. Add a new column to plot_df called 'Ground Truth Label'\n",
        "plot_df['Ground Truth Label'] = plot_df['Ground Truth'].map({0: 'Normal', 1: 'Anomaly'})\n",
        "\n",
        "# 4. Create a histogram or KDE plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=plot_df, x='Reconstruction Error', hue='Ground Truth Label', kde=True, bins=50, palette='viridis', common_norm=False)\n",
        "\n",
        "# 5. Add appropriate labels for the x and y axes\n",
        "plt.xlabel('Autoencoder Reconstruction Error')\n",
        "plt.ylabel('Density / Count')\n",
        "\n",
        "# 6. Add a title to the plot\n",
        "plt.title(f'Autoencoder Reconstruction Error Distribution for {attack_type.replace(\"_\", \" \").title()} ({selection_method.upper()})')\n",
        "\n",
        "# 7. Add a grid to the plot for better readability\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# 8. Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d313b4df"
      },
      "source": [
        "## Compare Model Performance (ROC AUC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8228ea6c"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Initialize an empty list to store performance data\n",
        "performance_data = []\n",
        "\n",
        "# Define selection method prefixes for iteration consistency\n",
        "selection_method_prefixes = ['heuristic', 'variance', 'pca']\n",
        "\n",
        "# 2. Iterate through each attack_type and its methods in the results dictionary\n",
        "for attack_type, methods in results.items():\n",
        "    # 3. For each attack_type, iterate through the selection_method_prefixes\n",
        "    for prefix in selection_method_prefixes:\n",
        "        # 4. Check for Isolation Forest ROC AUC scores\n",
        "        if_roc_auc_key = f'{prefix}_if_roc_auc'\n",
        "        if if_roc_auc_key in methods:\n",
        "            performance_data.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Isolation Forest',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[if_roc_auc_key]\n",
        "            })\n",
        "\n",
        "        # 5. Check for Autoencoder ROC AUC scores\n",
        "        ae_roc_auc_key = f'{prefix}_ae_roc_auc'\n",
        "        if ae_roc_auc_key in methods:\n",
        "            performance_data.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Autoencoder',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[ae_roc_auc_key]\n",
        "            })\n",
        "\n",
        "# 6. Convert the list of performance data into a Pandas DataFrame\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "\n",
        "# 7. Filter the DataFrame to include only 'ROC AUC' metrics (redundant here, but good practice)\n",
        "roc_auc_df = performance_df[performance_df['Metric'] == 'ROC AUC']\n",
        "\n",
        "# 8. Create a grouped bar chart using seaborn.catplot\n",
        "g = sns.catplot(data=roc_auc_df, x='Attack Type', y='Score', hue='Model', col='Feature Selection',\n",
        "                kind='bar', palette='Dark2', dodge=True, col_wrap=3, height=5, aspect=1.2)\n",
        "\n",
        "# 9. Add appropriate labels and a title\n",
        "g.set_axis_labels('Attack Type', 'ROC AUC Score')\n",
        "g.set_titles('Feature Selection: {col_name}')\n",
        "g.set_xticklabels(rotation=45, ha='right')\n",
        "plt.suptitle('ROC AUC Score Comparison Across Attack Types, Feature Selection, and Models', y=1.02, fontsize=16)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c216f94"
      },
      "source": [
        "## Compare Model Performance (Average Precision)\n",
        "\n",
        "### Subtask:\n",
        "Create a bar chart to compare the Average Precision scores across all attack types, feature selection methods, and models (Isolation Forest vs. Autoencoder). Ensure the plot is clearly labeled with legends to distinguish between models and methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaa71b13"
      },
      "source": [
        "**Reasoning**:\n",
        "I will extract the Average Precision scores from the `results` dictionary, organize them into a Pandas DataFrame, and then generate a faceted bar chart using `seaborn.catplot` to visualize the comparison across different attack types, feature selection methods, and models as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a854e258"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Initialize an empty list to store Average Precision scores\n",
        "performance_data_ap = []\n",
        "\n",
        "# 2. Define selection method prefixes for iteration consistency\n",
        "selection_method_prefixes = ['heuristic', 'variance', 'pca']\n",
        "\n",
        "# 3. Iterate through each attack_type and its methods in the results dictionary\n",
        "for attack_type, methods in results.items():\n",
        "    # a. For each attack_type, iterate through the selection_method_prefixes\n",
        "    for prefix in selection_method_prefixes:\n",
        "        # i. Check for Isolation Forest Average Precision scores\n",
        "        if_avg_precision_key = f'{prefix}_if_avg_precision'\n",
        "        if if_avg_precision_key in methods:\n",
        "            performance_data_ap.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Isolation Forest',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[if_avg_precision_key]\n",
        "            })\n",
        "\n",
        "        # ii. Check for Autoencoder Average Precision scores\n",
        "        ae_avg_precision_key = f'{prefix}_ae_avg_precision'\n",
        "        if ae_avg_precision_key in methods:\n",
        "            performance_data_ap.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Autoencoder',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[ae_avg_precision_key]\n",
        "            })\n",
        "\n",
        "# 4. Convert the list of performance data into a Pandas DataFrame\n",
        "avg_precision_df = pd.DataFrame(performance_data_ap)\n",
        "\n",
        "# 5. Create a grouped bar chart using seaborn.catplot\n",
        "g = sns.catplot(data=avg_precision_df, x='Attack Type', y='Score', hue='Model', col='Feature Selection',\n",
        "                kind='bar', palette='Dark2', dodge=True, col_wrap=3, height=5, aspect=1.2)\n",
        "\n",
        "# 6. Set the x-axis label to 'Attack Type' and the y-axis label to 'Average Precision Score'\n",
        "g.set_axis_labels('Attack Type', 'Average Precision Score')\n",
        "# 7. Set the titles for each facet\n",
        "g.set_titles('Feature Selection: {col_name}')\n",
        "# 8. Rotate the x-tick labels by 45 degrees and align them to the right\n",
        "g.set_xticklabels(rotation=45, ha='right')\n",
        "# 9. Add a main title to the plot\n",
        "plt.suptitle('Average Precision Score Comparison Across Attack Types, Feature Selection, and Models', y=1.02, fontsize=16)\n",
        "# 10. Add a grid to the y-axis for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "# 11. Adjust the plot layout to prevent the super title from overlapping\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "\n",
        "# 12. Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dbc358c"
      },
      "source": [
        "## Visualize Best Performing Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "099686d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Initialize variables to store the best ROC AUC score and its details\n",
        "best_roc_auc = -1\n",
        "best_attack_type = None\n",
        "best_selection_method = None\n",
        "best_model_type = None\n",
        "\n",
        "# 2. Iterate through the results dictionary to find the combination with the highest ROC AUC score\n",
        "for attack_type, methods in results.items():\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        # Check Isolation Forest ROC AUC\n",
        "        if_roc_auc_key = f'{prefix}_if_roc_auc'\n",
        "        if if_roc_auc_key in methods:\n",
        "            score = methods[if_roc_auc_key]\n",
        "            if score > best_roc_auc:\n",
        "                best_roc_auc = score\n",
        "                best_attack_type = attack_type\n",
        "                best_selection_method = prefix\n",
        "                best_model_type = 'Isolation Forest'\n",
        "\n",
        "        # Check Autoencoder ROC AUC\n",
        "        ae_roc_auc_key = f'{prefix}_ae_roc_auc'\n",
        "        if ae_roc_auc_key in methods:\n",
        "            score = methods[ae_roc_auc_key]\n",
        "            if score > best_roc_auc:\n",
        "                best_roc_auc = score\n",
        "                best_attack_type = attack_type\n",
        "                best_selection_method = prefix\n",
        "                best_model_type = 'Autoencoder'\n",
        "\n",
        "print(f\"Best Model Found: {best_model_type} for {best_attack_type.replace('_', ' ').title()} with {best_selection_method.title()} features (ROC AUC: {best_roc_auc:.4f})\")\n",
        "\n",
        "# 3. Retrieve the test anomaly scores/reconstruction errors and y_test labels for the best combination\n",
        "if best_model_type == 'Isolation Forest':\n",
        "    test_scores_key = f'{best_selection_method}_test_scores'\n",
        "    anomaly_scores = results[best_attack_type][test_scores_key]\n",
        "    metric_label = 'Anomaly Score'\n",
        "else: # Autoencoder\n",
        "    test_errors_key = f'{best_selection_method}_test_reconstruction_errors'\n",
        "    anomaly_scores = results[best_attack_type][test_errors_key]\n",
        "    metric_label = 'Reconstruction Error'\n",
        "\n",
        "y_test_key = f'{best_selection_method}_y_test'\n",
        "y_test = results[best_attack_type][y_test_key]\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({\n",
        "    metric_label: anomaly_scores,\n",
        "    'Ground Truth': y_test\n",
        "})\n",
        "\n",
        "# Map ground truth labels to more descriptive names for the legend\n",
        "plot_df['Ground Truth Label'] = plot_df['Ground Truth'].map({0: 'Normal', 1: 'Anomaly'})\n",
        "\n",
        "# 4. Create a scatter plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(\n",
        "    data=plot_df,\n",
        "    x=np.random.rand(len(plot_df)) * (plot_df.index.max() - plot_df.index.min()) + plot_df.index.min(), # Jittered x-axis\n",
        "    y=metric_label,\n",
        "    hue='Ground Truth Label',\n",
        "    palette={'Normal': 'skyblue', 'Anomaly': 'red'},\n",
        "    alpha=0.6,\n",
        "    s=10 # Smaller point size for large datasets\n",
        ")\n",
        "\n",
        "# 5. Add appropriate labels and a title\n",
        "plt.xlabel('Data Point Index (Jittered)')\n",
        "plt.ylabel(metric_label)\n",
        "plt.title(f'{best_model_type} Anomaly Detection for {best_attack_type.replace(\"_\", \" \").title()} (Features: {best_selection_method.title()})\\nROC AUC: {best_roc_auc:.4f}')\n",
        "plt.legend(title='Ground Truth')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d12c93f1"
      },
      "source": [
        "# Task\n",
        "The last step successfully created visualizations for Isolation Forest and Autoencoder models, comparing their performance using ROC AUC and Average Precision metrics, and identifying the overall best performing model.\n",
        "\n",
        "The next steps will focus on enhancing the anomaly detection system by introducing two new models, One-Class SVM (OCSVM) and Local Outlier Factor (LOF), and refining our approach to data imbalance and hyperparameter tuning.\n",
        "\n",
        "I will now proceed with the following actions:\n",
        "\n",
        "1.  **Address Data Imbalance (Conceptual)**: I will add a text cell discussing the challenges of data imbalance in anomaly detection, why traditional oversampling methods like SMOTE are unsuitable for high-dimensional anomaly detection, and how model-specific parameters and robust evaluation metrics (like ROC AUC and Average Precision) are more appropriate ways to handle this.\n",
        "2.  **Implement OCSVM Model Training and Scoring**: I will integrate One-Class SVM models into our pipeline. For each attack type and feature selection method, OCSVM models will be trained on the training data. Anomaly scores will then be generated for the training, validation, and test sets and stored in the `results` dictionary.\n",
        "3.  **Implement LOF Model Training and Scoring**: Similarly, Local Outlier Factor models will be added. For each attack type and feature selection method, LOF models will be trained, and anomaly scores will be computed for the training, validation, and test sets, also stored in the `results` dictionary.\n",
        "4.  **Discuss Hyperparameter Tuning**: I will add a text cell to outline the importance of hyperparameter tuning for all models, specifically `contamination` for Isolation Forest and OCSVM, and `n_neighbors` for LOF. I'll explain how validation sets can be used to optimize thresholds and parameters to maximize Average Precision.\n",
        "5.  **Evaluate New Models**: The existing evaluation logic will be extended to calculate ROC AUC and Average Precision scores for the newly added OCSVM and LOF models across all attack types and feature selection methods, ensuring comprehensive performance comparison.\n",
        "6.  **Update Performance Visualizations**: I will generate updated grouped bar charts to include OCSVM and LOF, comparing ROC AUC and Average Precision scores across all models, attack types, and feature selection methods.\n",
        "7.  **Update Best Performing Model Visualization**: The visualization for the best performing model will be updated to include OCSVM and LOF in the comparison, ensuring the overall top model is visualized.\n",
        "8.  **Final Summary**: Finally, I will provide a summary of the performance of all models, discuss the effectiveness of different approaches, and suggest future steps.\n",
        "\n",
        "Starting with the conceptual discussion on data imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f530f830"
      },
      "source": [
        "## Understand and Address Data Imbalance\n",
        "\n",
        "### Subtask:\n",
        "Discuss and implement strategies to address data imbalance suitable for high-dimensional anomaly detection. Explain why traditional oversampling (like SMOTE) is unsuitable and focus on methods such as model-specific parameters (e.g., `contamination` in Isolation Forest/OCSVM), weighted loss functions for Autoencoders (if applicable), and robust evaluation metrics (ROC AUC, Average Precision) as primary means of handling imbalance, rather than modifying the raw dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ee3e32a"
      },
      "source": [
        "## Understanding and Addressing Data Imbalance in Anomaly Detection\n",
        "\n",
        "Data imbalance is a pervasive challenge in anomaly detection, where anomalies (the target class) are by definition rare compared to normal instances. This inherent rarity leads to several difficulties, especially in high-dimensional datasets:\n",
        "\n",
        "### Challenges of Data Imbalance in High-Dimensional Anomaly Detection:\n",
        "1.  **Model Bias**: Most machine learning algorithms are designed to maximize overall accuracy. With imbalanced data, they tend to become biased towards the majority (normal) class, often classifying anomalous instances as normal, simply because there are so few of them.\n",
        "2.  **Poor Anomaly Detection**: Models might achieve high accuracy simply by predicting the majority class for all instances, rendering them useless for identifying the rare anomalies we are interested in.\n",
        "3.  **Increased Noise in High Dimensions**: In high-dimensional spaces, the data becomes sparse, and the concept of 'distance' can become less meaningful. Imbalance exacerbates this, as the few anomalous samples are widely scattered, making it harder for models to learn their distinct patterns.\n",
        "4.  **Computational Cost**: Modifying high-dimensional datasets can be computationally intensive and might lead to the curse of dimensionality issues during data generation or transformation.\n",
        "\n",
        "### Why Traditional Oversampling (e.g., SMOTE) is Unsuitable:\n",
        "Traditional oversampling techniques like SMOTE (Synthetic Minority Oversampling Technique) work by creating synthetic samples of the minority class. While effective for classification tasks with balanced classes, they are generally **unsuitable for anomaly detection** for several critical reasons:\n",
        "1.  **Definition of Anomaly**: Anomalies are rare, distinct, and often extreme observations. Generating synthetic anomalies by interpolating between existing anomalies fundamentally contradicts this definition. It assumes that anomalies exist on a manifold that can be linearly (or non-linearly, with variants) interpolated, which is often not true. Anomalies are unique, not 'average' versions of other anomalies.\n",
        "2.  **Risk of Smearing Anomaly Boundaries**: Synthetic anomaly generation can create 'normal-looking' anomalies or 'anomaly-looking' normal points, effectively smearing the boundary between normal and anomalous regions. This can make it harder for the model to distinguish true anomalies.\n",
        "3.  **Introduction of Noise**: In high-dimensional spaces, SMOTE can easily generate synthetic samples that are noisy or not representative of actual anomalies, further obscuring the true anomaly patterns.\n",
        "4.  **No New Information**: Synthetic samples don't introduce new information about the underlying data distribution; they merely replicate existing patterns (or variations thereof). True anomalies are, by nature, novel.\n",
        "\n",
        "### Alternative Strategies for Addressing Data Imbalance:\n",
        "Instead of modifying the dataset itself, the focus in anomaly detection shifts to using techniques that are inherently designed to handle imbalance or are robust to it. These include:\n",
        "\n",
        "1.  **Model-Specific Parameters:**\n",
        "    *   **`contamination` Parameter (Isolation Forest, One-Class SVM):** Algorithms like Isolation Forest and One-Class SVM (OCSVM) often include a `contamination` parameter. This parameter explicitly tells the model the expected proportion of anomalies in the dataset. By setting this parameter, the model can adjust its decision boundary to identify a certain percentage of the most extreme observations as anomalies. This directly addresses the imbalance by acknowledging the rarity of anomalies during model training and prediction.\n",
        "    *   **Thresholding**: Many anomaly detection models output an anomaly score. Instead of a fixed classification, a threshold can be set on these scores to determine what constitutes an anomaly. This threshold can be chosen based on domain knowledge or desired recall/precision trade-offs, regardless of the inherent class imbalance.\n",
        "\n",
        "2.  **Weighted Loss Functions (for Autoencoders/Neural Networks):**\n",
        "    *   For reconstruction-based models like Autoencoders, if you were to train them in a supervised manner (e.g., for classification between normal/anomaly), you could use weighted loss functions. This involves assigning a higher penalty to misclassifications of the minority (anomaly) class during training. However, autoencoders are primarily unsupervised for anomaly detection, learning to reconstruct normal data, and anomalies result in high reconstruction errors. The 'imbalance' is handled by the unsupervised nature of learning *only* the normal class.\n",
        "\n",
        "3.  **Robust Evaluation Metrics:**\n",
        "    Traditional accuracy is a misleading metric for imbalanced datasets. Instead, robust metrics that focus on the performance of the minority class are crucial:\n",
        "    *   **ROC AUC (Receiver Operating Characteristic Area Under the Curve):** This metric evaluates a model's ability to discriminate between classes across various threshold settings. It is less sensitive to class imbalance than accuracy because it considers both true positive rate and false positive rate.\n",
        "    *   **Average Precision (AP) / PR AUC (Precision-Recall Area Under the Curve):** AP is particularly well-suited for highly imbalanced datasets. It focuses on the model's performance on the positive class (anomalies) and is more sensitive to the trade-off between precision and recall for anomalies. A high AP score indicates that a model is good at finding anomalies without generating too many false positives.\n",
        "    *   **F1-Score, Recall, Precision**: These metrics can also be useful, but they depend on a specific threshold. Reporting them for the minority class helps understand model performance.\n",
        "\n",
        "By focusing on these model-centric and evaluation-centric strategies, we can effectively build and assess anomaly detection systems in the presence of severe data imbalance, without resorting to data modifications that could undermine the integrity of anomaly patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e653af42"
      },
      "source": [
        "## Add OCSVM Model Training and Scoring\n",
        "\n",
        "### Subtask:\n",
        "Implement One-Class SVM (OCSVM) models for each attack type and feature selection method. Train the OCSVM models on the training data and calculate anomaly scores for the training, validation, and test sets. These scores will be stored in the 'results' dictionary. Highlight how OCSVM's nature is designed for outlier detection in imbalanced datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27259868"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing One-Class SVM (OCSVM) models. First, I will import the necessary `OneClassSVM` class from `sklearn.svm`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60bbb76a"
      },
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "print(\"OneClassSVM imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da38e593"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `OneClassSVM` class is imported, I will proceed to implement the OCSVM model training and scoring as per the subtask instructions. This involves iterating through the `results` dictionary, initializing an OCSVM model for each attack type and feature selection method, training it on the training data, and then calculating and storing anomaly scores for the training, validation, and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aec14c18"
      },
      "source": [
        "print(\"Training One-Class SVM models...\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if selection_method_key.endswith('_X_train'):\n",
        "            # Extract training, validation, and test data\n",
        "            X_train = methods[selection_method_key]\n",
        "            X_val = methods[selection_method_key.replace('_X_train', '_X_val')]\n",
        "            X_test = methods[selection_method_key.replace('_X_train', '_X_test')]\n",
        "\n",
        "            # Initialize One-Class SVM model\n",
        "            # nu is an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors\n",
        "            # It is analogous to contamination in IsolationForest\n",
        "            ocsvm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto') # Using 'auto' for gamma to simplify for now\n",
        "\n",
        "            # Train the model (OCSVM is unsupervised, only uses X_train)\n",
        "            # Ensure X_train has sufficient samples (at least 2 for OCSVM)\n",
        "            if X_train.shape[0] > 1:\n",
        "                ocsvm_model.fit(X_train)\n",
        "\n",
        "                # Calculate anomaly scores using decision_function\n",
        "                # Lower values are more anomalous, higher are more normal\n",
        "                train_scores_ocsvm = ocsvm_model.decision_function(X_train)\n",
        "                val_scores_ocsvm = ocsvm_model.decision_function(X_val)\n",
        "                test_scores_ocsvm = ocsvm_model.decision_function(X_test)\n",
        "\n",
        "                # Store the model and scores back into the results dictionary\n",
        "                model_key = selection_method_key.replace('_X_train', '_ocsvm_model')\n",
        "                train_scores_key = selection_method_key.replace('_X_train', '_train_scores_ocsvm')\n",
        "                val_scores_key = selection_method_key.replace('_X_train', '_val_scores_ocsvm')\n",
        "                test_scores_key = selection_method_key.replace('_X_train', '_test_scores_ocsvm')\n",
        "\n",
        "                results[attack_type][model_key] = ocsvm_model\n",
        "                results[attack_type][train_scores_key] = train_scores_ocsvm\n",
        "                results[attack_type][val_scores_key] = val_scores_ocsvm\n",
        "                results[attack_type][test_scores_key] = test_scores_ocsvm\n",
        "\n",
        "                print(f\"  Trained OCSVM for {attack_type} - {selection_method_key.replace('_X_train', '')}\")\n",
        "            else:\n",
        "                print(f\"  Skipping OCSVM for {attack_type} - {selection_method_key.replace('_X_train', '')}: Insufficient training data ({X_train.shape[0]} samples).\")\n",
        "\n",
        "\n",
        "# Verify one example to ensure models and scores are stored\n",
        "print(\"\\nVerification: Brute-force (heuristic) OCSVM model and scores shapes:\")\n",
        "if 'brute_force' in results and 'heuristic_ocsvm_model' in results['brute_force']:\n",
        "    print(f\"Model type: {type(results['brute_force']['heuristic_ocsvm_model'])}\")\n",
        "    print(f\"Train scores shape: {results['brute_force']['heuristic_train_scores_ocsvm'].shape}\")\n",
        "    print(f\"Validation scores shape: {results['brute_force']['heuristic_val_scores_ocsvm'].shape}\")\n",
        "    print(f\"Test scores shape: {results['brute_force']['heuristic_test_scores_ocsvm'].shape}\")\n",
        "else:\n",
        "    print(\"  OCSVM model for brute_force - heuristic not found or skipped.\")\n",
        "\n",
        "print(\"\\nOne-Class SVM model training and scoring complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b2ac4f8"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now implement the Local Outlier Factor (LOF) models. This involves importing the `LocalOutlierFactor` class, iterating through the `results` dictionary, and for each attack type and feature selection method, initializing, training, and scoring an LOF model. The model and its scores will then be stored in the `results` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"Training Local Outlier Factor models (OPTIMIZED)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# GLOBAL OPTIMIZATIONS\n",
        "SCALING_ENABLED = True  # Scaling can help LOF convergence\n",
        "MAX_SAMPLES = 3000      # Maximum samples for training\n",
        "N_NEIGHBORS = 15        # Fixed for consistency\n",
        "BATCH_SIZE = 2000       # For scoring\n",
        "\n",
        "lof_cache = {}\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        if not selection_method_key.endswith('_X_train'):\n",
        "            continue\n",
        "\n",
        "        # Quick check for data existence\n",
        "        X_train = methods.get(selection_method_key)\n",
        "        if X_train is None or len(X_train) <= N_NEIGHBORS:\n",
        "            continue\n",
        "\n",
        "        # Get corresponding validation and test data\n",
        "        base_key = selection_method_key.replace('_X_train', '')\n",
        "        X_val = methods.get(f'{base_key}_X_val')\n",
        "        X_test = methods.get(f'{base_key}_X_test')\n",
        "\n",
        "        if X_val is None or X_test is None:\n",
        "            continue\n",
        "\n",
        "        print(f\"  Processing {attack_type} - {base_key}...\", end=\"\")\n",
        "\n",
        "        try:\n",
        "            # OPTIMIZATION: Scale data for better LOF performance\n",
        "            if SCALING_ENABLED:\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_val_scaled = scaler.transform(X_val)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "            else:\n",
        "                X_train_scaled = X_train\n",
        "                X_val_scaled = X_val\n",
        "                X_test_scaled = X_test\n",
        "\n",
        "            # OPTIMIZATION: Use subset for training\n",
        "            if len(X_train_scaled) > MAX_SAMPLES:\n",
        "                idx = np.random.RandomState(42).choice(\n",
        "                    len(X_train_scaled),\n",
        "                    MAX_SAMPLES,\n",
        "                    replace=False\n",
        "                )\n",
        "                X_train_fit = X_train_scaled[idx]\n",
        "            else:\n",
        "                X_train_fit = X_train_scaled\n",
        "\n",
        "            # Check cache based on data characteristics\n",
        "            cache_key = f\"{X_train_fit.shape}_{hash(tuple(X_train_fit.mean(axis=0).tobytes()))}\"\n",
        "\n",
        "            if cache_key in lof_cache:\n",
        "                lof_model = lof_cache[cache_key]\n",
        "                model_trained = False\n",
        "            else:\n",
        "                # Train LOF with memory-efficient parameters\n",
        "                lof_model = LocalOutlierFactor(\n",
        "                    n_neighbors=N_NEIGHBORS,\n",
        "                    algorithm='kd_tree',  # Faster for moderate dimensions\n",
        "                    leaf_size=20,  # Smaller leaf = faster queries\n",
        "                    metric='minkowski',\n",
        "                    p=2,\n",
        "                    contamination='auto',\n",
        "                    novelty=True,\n",
        "                    n_jobs=1  # Single job for stability\n",
        "                )\n",
        "\n",
        "                # Train\n",
        "                lof_model.fit(X_train_fit)\n",
        "                lof_cache[cache_key] = lof_model\n",
        "                model_trained = True\n",
        "\n",
        "            # Efficient scoring with progress bar for large datasets\n",
        "            def score_dataset(X, dataset_name=\"\"):\n",
        "                if len(X) == 0:\n",
        "                    return np.array([])\n",
        "\n",
        "                # Score in batches\n",
        "                scores = []\n",
        "                n_batches = max(1, len(X) // BATCH_SIZE)\n",
        "\n",
        "                for i in range(0, len(X), BATCH_SIZE):\n",
        "                    batch = X[i:i + BATCH_SIZE]\n",
        "                    batch_scores = lof_model.decision_function(batch)\n",
        "                    scores.append(batch_scores)\n",
        "\n",
        "                return np.concatenate(scores) if scores else np.array([])\n",
        "\n",
        "            # Get scores\n",
        "            train_scores = score_dataset(X_train_scaled, \"train\")\n",
        "            val_scores = score_dataset(X_val_scaled, \"val\")\n",
        "            test_scores = score_dataset(X_test_scaled, \"test\")\n",
        "\n",
        "            # Store results\n",
        "            results[attack_type][f'{base_key}_lof_model'] = lof_model\n",
        "            results[attack_type][f'{base_key}_train_scores_lof'] = train_scores\n",
        "            results[attack_type][f'{base_key}_val_scores_lof'] = val_scores\n",
        "            results[attack_type][f'{base_key}_test_scores_lof'] = test_scores\n",
        "\n",
        "            # Store scaler if used\n",
        "            if SCALING_ENABLED:\n",
        "                results[attack_type][f'{base_key}_lof_scaler'] = scaler\n",
        "\n",
        "            print(f\"  ({'trained' if model_trained else 'cached'})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {str(e)[:50]}...\")\n",
        "            continue\n",
        "\n",
        "print(f\"\\nCompleted in {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"Models trained/cached: {len(lof_cache)}\")"
      ],
      "metadata": {
        "id": "nt7CdZ40-ehX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97b0237e"
      },
      "source": [
        "# Task\n",
        "## Hyperparameter Tuning and Threshold Optimization\n",
        "\n",
        "### Subtask:\n",
        "Discuss the importance of hyperparameter tuning for all models, especially the `contamination` parameter for Isolation Forest and OCSVM, and `n_neighbors` for LOF. Explain how to optimize anomaly detection thresholds, potentially using validation sets, to maximize metrics like Average Precision, which are crucial for imbalanced datasets, rather than relying solely on fixed contamination values.\n",
        "\n",
        "### Reasoning:\n",
        "I will add a text cell to outline the importance of hyperparameter tuning for all models, specifically `contamination` for Isolation Forest and OCSVM, and `n_neighbors` for LOF. I'll explain how validation sets can be used to optimize thresholds and parameters to maximize Average Precision. This addresses the hyperparameter tuning and threshold optimization part of the plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1eee403"
      },
      "source": [
        "## Add LOF Model Training and Scoring\n",
        "\n",
        "### Subtask:\n",
        "Implement Local Outlier Factor (LOF) models for each attack type and feature selection method. Train the LOF models on the training data and calculate anomaly scores for the training, validation, and test sets. These scores will be stored in the 'results' dictionary. Emphasize LOF's density-based approach, which naturally adapts to varying densities of normal and anomalous data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d60c5dc1"
      },
      "source": [
        "## Hyperparameter Tuning and Threshold Optimization\n",
        "\n",
        "### Subtask:\n",
        "Discuss the importance of hyperparameter tuning for all models, especially the `contamination` parameter for Isolation Forest and OCSVM, and `n_neighbors` for LOF. Explain how to optimize anomaly detection thresholds, potentially using validation sets, to maximize metrics like Average Precision, which are crucial for imbalanced datasets, rather than relying solely on fixed contamination values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12374a9b"
      },
      "source": [
        "## Hyperparameter Tuning and Threshold Optimization\n",
        "\n",
        "Anomaly detection models, while powerful, often rely heavily on carefully selected hyperparameters to perform optimally. Unlike supervised learning where a clear target class guides optimization, unsupervised anomaly detection models require a nuanced approach to tuning, especially given the inherent imbalance of anomaly datasets.\n",
        "\n",
        "### Importance of Hyperparameter Tuning\n",
        "\n",
        "Hyperparameters dictate the learning process and structure of a model, significantly influencing its ability to distinguish between normal and anomalous instances. Incorrectly chosen hyperparameters can lead to models that either miss many anomalies (high false negatives) or flag too many normal instances as anomalous (high false positives).\n",
        "\n",
        "*   **Isolation Forest:** The `contamination` parameter is crucial. It represents the proportion of outliers in the data. While often set to an estimated value, tuning it allows the model to adjust its internal thresholds and effectively isolate a more accurate number of anomalies. The `n_estimators` (number of trees) and `max_features` (features to sample for each tree) also impact the robustness and diversity of the forest.\n",
        "*   **One-Class SVM (OCSVM):** Similar to Isolation Forest, `nu` (an upper bound on the fraction of training errors and a lower bound on the fraction of support vectors) is analogous to `contamination`. The `kernel` type (e.g., 'rbf', 'linear') and its associated parameters like `gamma` (for 'rbf' kernel) define the shape of the decision boundary. Tuning `nu` and `gamma` is vital for OCSVM to accurately delineate the normal class boundary.\n",
        "*   **Local Outlier Factor (LOF):** The `n_neighbors` parameter is central to LOF. It defines the number of neighbors considered to compute the local density of a sample. The choice of `n_neighbors` directly impacts the 'locality' of the outlier definition; a small `n_neighbors` might make the model sensitive to very local fluctuations, while a large `n_neighbors` might smooth out local structures, potentially missing subtle anomalies. The `contamination` parameter in LOF also helps set a threshold for classification.\n",
        "\n",
        "### Optimizing Anomaly Detection Thresholds using Validation Sets\n",
        "\n",
        "Many anomaly detection models output an 'anomaly score' rather than a direct binary classification. A threshold must be applied to these scores to classify instances as normal or anomalous. Relying solely on a fixed `contamination` value (e.g., 0.01) might not be optimal, as the true proportion of anomalies can vary, and what constitutes an 'anomaly' might need to be contextually defined based on business needs (e.g., prioritizing recall over precision).\n",
        "\n",
        "**Process for Threshold Optimization:**\n",
        "1.  **Generate Scores:** Train models on the training set and generate anomaly scores for the validation set.\n",
        "2.  **Iterate Thresholds:** Systematically evaluate a range of potential thresholds on the validation set's anomaly scores.\n",
        "3.  **Evaluate Metrics:** For each threshold, calculate relevant evaluation metrics (e.g., ROC AUC, Average Precision, F1-score).\n",
        "4.  **Select Best Threshold:** Choose the threshold that maximizes the desired metric on the validation set. For example, if the goal is to identify as many true anomalies as possible without an excessive number of false alarms, a threshold maximizing Average Precision would be preferred.\n",
        "\n",
        "### Why Average Precision is Crucial for Imbalanced Datasets\n",
        "\n",
        "In highly imbalanced anomaly detection scenarios, metrics like standard accuracy are misleading. A model predicting everything as 'normal' might achieve 99% accuracy but fail to detect any anomalies. Metrics like ROC AUC are better, as they are less sensitive to class imbalance, evaluating the model's overall discriminative power.\n",
        "\n",
        "However, **Average Precision (AP) or PR AUC (Precision-Recall Area Under the Curve) is particularly critical** for threshold optimization in imbalanced anomaly detection for the following reasons:\n",
        "\n",
        "*   **Focus on the Minority Class:** AP focuses directly on the performance of the positive (anomaly) class. It measures the area under the Precision-Recall curve, which plots precision against recall for various thresholds.\n",
        "*   **Sensitive to Positive Predictive Value:** Precision (`TP / (TP + FP)`) is a crucial measure of how many of the detected anomalies are actual anomalies. In fraud detection or security breach detection, a high precision is often desired to minimize investigation costs associated with false alarms.\n",
        "*   **Addresses Imbalance Directly:** Unlike ROC AUC, which can look optimistic with severe class imbalance (due to the True Negative rate dominating the False Positive rate), AP provides a more realistic and often lower score, reflecting the true challenge of detecting rare events. A high AP score indicates that the model can find anomalies with high confidence and relatively few false positives.\n",
        "\n",
        "By optimizing hyperparameters and thresholds against Average Precision on a validation set, we ensure that our models are tuned to effectively identify the rare anomalous events, which is the primary goal of anomaly detection in imbalanced datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93a481e4"
      },
      "source": [
        "## Evaluate New Models\n",
        "\n",
        "### Subtask:\n",
        "Extend the existing evaluation logic to calculate ROC AUC and Average Precision scores for the newly added OCSVM and LOF models across all attack types and feature selection methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5383f7be"
      },
      "source": [
        "**Reasoning**:\n",
        "I will calculate ROC AUC and Average Precision scores for the newly added OCSVM and LOF models across all attack types and feature selection methods, and store them in the `results` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d877023d"
      },
      "source": [
        "print(\"Evaluating OCSVM and LOF models performance...\")\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for selection_method_key in list(methods.keys()):\n",
        "        # Evaluate OCSVM models\n",
        "        if selection_method_key.endswith('_test_scores_ocsvm'):\n",
        "            method_prefix = selection_method_key.replace('_test_scores_ocsvm', '')\n",
        "\n",
        "            ocsvm_test_scores = methods[selection_method_key]\n",
        "            y_test_key = f'{method_prefix}_y_test'\n",
        "            y_test = methods[y_test_key]\n",
        "\n",
        "            if len(np.unique(y_test)) > 1:\n",
        "                ocsvm_roc_auc = roc_auc_score(y_test, ocsvm_test_scores)\n",
        "                ocsvm_avg_precision = average_precision_score(y_test, ocsvm_test_scores)\n",
        "\n",
        "                results[attack_type][f'{method_prefix}_ocsvm_roc_auc'] = ocsvm_roc_auc\n",
        "                results[attack_type][f'{method_prefix}_ocsvm_avg_precision'] = ocsvm_avg_precision\n",
        "\n",
        "                print(f\"  {attack_type} - {method_prefix} OCSVM: ROC AUC = {ocsvm_roc_auc:.4f}, Avg Precision = {ocsvm_avg_precision:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {attack_type} - {method_prefix} OCSVM: Skipping metrics calculation due to single class in y_test.\")\n",
        "\n",
        "        # Evaluate LOF models\n",
        "        elif selection_method_key.endswith('_test_scores_lof'):\n",
        "            method_prefix = selection_method_key.replace('_test_scores_lof', '')\n",
        "\n",
        "            lof_test_scores = methods[selection_method_key]\n",
        "            y_test_key = f'{method_prefix}_y_test'\n",
        "            y_test = methods[y_test_key]\n",
        "\n",
        "            if len(np.unique(y_test)) > 1:\n",
        "                lof_roc_auc = roc_auc_score(y_test, lof_test_scores)\n",
        "                lof_avg_precision = average_precision_score(y_test, lof_test_scores)\n",
        "\n",
        "                results[attack_type][f'{method_prefix}_lof_roc_auc'] = lof_roc_auc\n",
        "                results[attack_type][f'{method_prefix}_lof_avg_precision'] = lof_avg_precision\n",
        "\n",
        "                print(f\"  {attack_type} - {method_prefix} LOF: ROC AUC = {lof_roc_auc:.4f}, Avg Precision = {lof_avg_precision:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {attack_type} - {method_prefix} LOF: Skipping metrics calculation due to single class in y_test.\")\n",
        "\n",
        "print(\"\\nOCSVM and LOF model performance evaluation complete.\")\n",
        "\n",
        "# Verify one example to ensure metrics are stored\n",
        "print(\"\\nBrute-force (heuristic) OCSVM and LOF metrics:\")\n",
        "if 'heuristic_ocsvm_roc_auc' in results['brute_force']:\n",
        "    print(f\"OCSVM ROC AUC: {results['brute_force']['heuristic_ocsvm_roc_auc']:.4f}\")\n",
        "    print(f\"OCSVM Average Precision: {results['brute_force']['heuristic_ocsvm_avg_precision']:.4f}\")\n",
        "else:\n",
        "    print(\"  OCSVM metrics for brute_force - heuristic were skipped.\")\n",
        "\n",
        "if 'heuristic_lof_roc_auc' in results['brute_force']:\n",
        "    print(f\"LOF ROC AUC: {results['brute_force']['heuristic_lof_roc_auc']:.4f}\")\n",
        "    print(f\"LOF Average Precision: {results['brute_force']['heuristic_lof_avg_precision']:.4f}\")\n",
        "else:\n",
        "    print(\"  LOF metrics for brute_force - heuristic were skipped.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === PORT_SCAN quick tuning + threshold selection (run AFTER splits & training) ===\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "atk = 'port_scan'\n",
        "best_summary = []\n",
        "\n",
        "for prefix in ['heuristic','variance','pca']:\n",
        "    X_train = results[atk].get(f'{prefix}_X_train')\n",
        "    X_val = results[atk].get(f'{prefix}_X_val')\n",
        "    X_test = results[atk].get(f'{prefix}_X_test')\n",
        "    y_val = results[atk].get(f'{prefix}_y_val')\n",
        "    y_test = results[atk].get(f'{prefix}_y_test')\n",
        "    scaler = results[atk].get(f'{prefix}_scaler')\n",
        "\n",
        "    if X_train is None or X_val is None or X_test is None:\n",
        "        print(f\"{atk}-{prefix}: missing data, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Standardize (fit on train)\n",
        "    scaler_local = StandardScaler().fit(X_train) if scaler is None else scaler\n",
        "    Xtr = scaler_local.transform(X_train)\n",
        "    Xv = scaler_local.transform(X_val)\n",
        "    Xt = scaler_local.transform(X_test)\n",
        "    results[atk][f'{prefix}_scaler'] = scaler_local  # save back if absent\n",
        "\n",
        "    # 1) IsolationForest quick grid\n",
        "    best_if = {'ap': -1}\n",
        "    for contam in [0.005, 0.01, 0.02]:\n",
        "        try:\n",
        "            m = IsolationForest(contamination=contam, n_estimators=100, max_samples=min(512, len(Xtr)), random_state=42, n_jobs=1)\n",
        "            m.fit(Xtr)\n",
        "            val_scores = -m.decision_function(Xv)  # higher = anomalous\n",
        "            ap = average_precision_score(y_val, val_scores)\n",
        "            if ap > best_if['ap']:\n",
        "                best_if = {'ap':ap,'model':m,'contamination':contam,'val_scores':val_scores}\n",
        "        except Exception:\n",
        "            continue\n",
        "    # pick threshold maximizing F1 on validation\n",
        "    p, r, t = precision_recall_curve(y_val, best_if['val_scores'])\n",
        "    f1 = 2*p*r/(p+r+1e-12)\n",
        "    best_idx = np.nanargmax(f1)\n",
        "    best_if_threshold = t[best_idx] if best_idx < len(t) else t[-1]\n",
        "    # evaluate on test\n",
        "    test_scores_if = -best_if['model'].decision_function(Xt)\n",
        "    test_ap_if = average_precision_score(y_test, test_scores_if >= best_if_threshold)\n",
        "    test_roc_if = roc_auc_score(y_test, test_scores_if)\n",
        "    results[atk][f'{prefix}_if_tuned_model'] = best_if['model']\n",
        "    results[atk][f'{prefix}_if_tuned_threshold'] = best_if_threshold\n",
        "    results[atk][f'{prefix}_if_tuned_ap_test'] = test_ap_if\n",
        "    results[atk][f'{prefix}_if_tuned_roc_test'] = test_roc_if\n",
        "\n",
        "    # 2) OCSVM quick grid\n",
        "    best_ocsvm = {'ap': -1}\n",
        "    # train on subset if large\n",
        "    tr_idx = np.arange(len(Xtr))\n",
        "    if len(Xtr) > 5000:\n",
        "        rng = np.random.RandomState(42)\n",
        "        tr_idx = rng.choice(len(Xtr), 5000, replace=False)\n",
        "    Xtr_ocsvm = Xtr[tr_idx]\n",
        "    for nu in [0.005, 0.01, 0.02]:\n",
        "        try:\n",
        "            m = OneClassSVM(nu=nu, kernel='rbf', gamma='scale')\n",
        "            m.fit(Xtr_ocsvm)\n",
        "            val_scores = -m.decision_function(Xv)\n",
        "            ap = average_precision_score(y_val, val_scores)\n",
        "            if ap > best_ocsvm['ap']:\n",
        "                best_ocsvm = {'ap':ap,'model':m,'nu':nu,'val_scores':val_scores}\n",
        "        except Exception:\n",
        "            continue\n",
        "    if best_ocsvm['ap'] >= 0:\n",
        "        p, r, t = precision_recall_curve(y_val, best_ocsvm['val_scores'])\n",
        "        f1 = 2*p*r/(p+r+1e-12)\n",
        "        best_idx = np.nanargmax(f1)\n",
        "        best_ocsvm_threshold = t[best_idx] if best_idx < len(t) else t[-1]\n",
        "        test_scores_ocsvm = -best_ocsvm['model'].decision_function(Xt)\n",
        "        test_ap_ocsvm = average_precision_score(y_test, test_scores_ocsvm >= best_ocsvm_threshold)\n",
        "        test_roc_ocsvm = roc_auc_score(y_test, test_scores_ocsvm)\n",
        "        results[atk][f'{prefix}_ocsvm_tuned_model'] = best_ocsvm['model']\n",
        "        results[atk][f'{prefix}_ocsvm_tuned_threshold'] = best_ocsvm_threshold\n",
        "        results[atk][f'{prefix}_ocsvm_tuned_ap_test'] = test_ap_ocsvm\n",
        "        results[atk][f'{prefix}_ocsvm_tuned_roc_test'] = test_roc_ocsvm\n",
        "\n",
        "    # 3) LOF quick grid (novelty=True so we can call decision_function)\n",
        "    best_lof = {'ap': -1}\n",
        "    for n_neighbors in [10, 15, 30]:\n",
        "        try:\n",
        "            m = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination='auto')\n",
        "            # fit on subset if needed\n",
        "            m.fit(Xtr if len(Xtr) <= 20000 else Xtr[np.random.RandomState(42).choice(len(Xtr), 10000, replace=False)])\n",
        "            val_scores = -m.decision_function(Xv)\n",
        "            ap = average_precision_score(y_val, val_scores)\n",
        "            if ap > best_lof['ap']:\n",
        "                best_lof = {'ap':ap,'model':m,'n_neighbors':n_neighbors,'val_scores':val_scores}\n",
        "        except Exception:\n",
        "            continue\n",
        "    if best_lof['ap'] >= 0:\n",
        "        p, r, t = precision_recall_curve(y_val, best_lof['val_scores'])\n",
        "        f1 = 2*p*r/(p+r+1e-12)\n",
        "        best_idx = np.nanargmax(f1)\n",
        "        best_lof_threshold = t[best_idx] if best_idx < len(t) else t[-1]\n",
        "        test_scores_lof = -best_lof['model'].decision_function(Xt)\n",
        "        test_ap_lof = average_precision_score(y_test, test_scores_lof >= best_lof_threshold)\n",
        "        test_roc_lof = roc_auc_score(y_test, test_scores_lof)\n",
        "        results[atk][f'{prefix}_lof_tuned_model'] = best_lof['model']\n",
        "        results[atk][f'{prefix}_lof_tuned_threshold'] = best_lof_threshold\n",
        "        results[atk][f'{prefix}_lof_tuned_ap_test'] = test_ap_lof\n",
        "        results[atk][f'{prefix}_lof_tuned_roc_test'] = test_roc_lof\n",
        "\n",
        "    best_summary.append((prefix,\n",
        "                         results[atk].get(f'{prefix}_if_tuned_ap_test'),\n",
        "                         results[atk].get(f'{prefix}_ocsvm_tuned_ap_test'),\n",
        "                         results[atk].get(f'{prefix}_lof_tuned_ap_test')))\n",
        "\n",
        "# Print small table of test AP after tuning\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(best_summary, columns=['prefix','IF_test_AP','OCSVM_test_AP','LOF_test_AP'])\n",
        "print('Port_scan tuned test AP (by prefix):')\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "id": "sjFwmGQcHh0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eda7656"
      },
      "source": [
        "# Task\n",
        "Implement hyperparameter tuning for Isolation Forest, OCSVM, and LOF models by defining parameter grids for each model. For each attack type and feature selection method, iterate through these grids, train the models on the training data, and evaluate their performance using Average Precision on the validation sets. Store the best models and their optimized hyperparameters in the `results` dictionary. After tuning, re-evaluate the performance of all models (Isolation Forest, Autoencoder, OCSVM, and LOF) on their respective test sets, using the newly optimized parameters for Isolation Forest, OCSVM, and LOF. Calculate and store the ROC AUC and Average Precision scores for all models on the test sets. Finally, update the grouped bar charts to visualize the performance of all models, including OCSVM and LOF, and update the visualization for the overall best-performing model based on the tuned parameters. Summarize the performance of all anomaly detection models, highlighting the impact of hyperparameter tuning and proposing future steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "168c2fec"
      },
      "source": [
        "## Implement Hyperparameter Tuning\n",
        "\n",
        "### Subtask:\n",
        "Implement hyperparameter tuning for Isolation Forest, One-Class SVM (OCSVM), and Local Outlier Factor (LOF) models using Average Precision as the scoring metric on validation sets. Store the best models and their optimized hyperparameters in the `results` dictionary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30c41db0"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define parameter grids for Isolation Forest, One-Class SVM, and Local Outlier Factor (LOF) models. Then, I will iterate through each attack type and feature selection method, retrieving the data splits. For each combination, I will check for sufficient anomalies in the validation set. Subsequently, I will perform a manual grid search for each model, fit them on the training data, calculate anomaly scores on the validation set, and evaluate performance using Average Precision. Finally, I will store the best-performing models and their hyperparameters in the `results` dictionary and print the tuning results. I will explicitly handle the `contamination` parameter constraint for LOF with `novelty=True`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import average_precision_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optimized parameter grids (reduced for speed)\n",
        "param_grids = {\n",
        "    'Isolation Forest': {\n",
        "        'n_estimators': [100],  # Reduced: Single value for speed\n",
        "        'max_samples': [256, 512],  # Added: Controls tree size\n",
        "        'contamination': [0.005, 0.01]  # Reduced: Focus on realistic values\n",
        "    },\n",
        "    'One-Class SVM': {\n",
        "        'nu': [0.005, 0.01],  # Reduced: Most common values\n",
        "        'kernel': ['rbf'],  # Single best kernel\n",
        "        'gamma': ['scale']  # Single best gamma\n",
        "    },\n",
        "    'Local Outlier Factor': {\n",
        "        'n_neighbors': [10, 20],  # Reduced\n",
        "        'algorithm': ['auto']  # Added for speed\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Starting OPTIMIZED hyperparameter tuning...\")\n",
        "\n",
        "# Configuration\n",
        "MAX_SAMPLES = 2000  # Reduced from 5000\n",
        "MAX_VAL_SAMPLES = 1000  # Score on subset\n",
        "USE_PARALLEL = True  # Parallelize across attack types\n",
        "N_JOBS = 2  # Conservative for Colab CPU\n",
        "\n",
        "# Store results\n",
        "tuned_models_results = {}\n",
        "\n",
        "def tune_model_for_config(args):\n",
        "    \"\"\"Parallel worker function\"\"\"\n",
        "    attack_type, prefix, X_train, X_val, y_val = args\n",
        "\n",
        "    results = {\n",
        "        'if_tuned_model': None,\n",
        "        'if_tuned_params': None,\n",
        "        'ocsvm_tuned_model': None,\n",
        "        'ocsvm_tuned_params': None,\n",
        "        'lof_tuned_model': None,\n",
        "        'lof_tuned_params': None\n",
        "    }\n",
        "\n",
        "    # Skip if insufficient anomalies\n",
        "    num_anomalies = np.sum(y_val == 1)\n",
        "    if num_anomalies < 1:\n",
        "        return (attack_type, prefix), results\n",
        "\n",
        "    print(f\"  Tuning {attack_type} - {prefix}...\")\n",
        "\n",
        "    # --- 1. OPTIMIZATION: Subsample validation for scoring ---\n",
        "    if len(X_val) > MAX_VAL_SAMPLES:\n",
        "        idx = np.random.RandomState(42).choice(len(X_val), MAX_VAL_SAMPLES, replace=False)\n",
        "        X_val_scoring = X_val[idx]\n",
        "        y_val_scoring = y_val[idx]\n",
        "    else:\n",
        "        X_val_scoring = X_val\n",
        "        y_val_scoring = y_val\n",
        "\n",
        "    # --- 2. OPTIMIZATION: Standardize data once for all models ---\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val_scoring)\n",
        "\n",
        "    # --- Isolation Forest Tuning ---\n",
        "    print(f\"    IF for {attack_type} - {prefix}...\", end=\"\")\n",
        "    best_if_ap = -1\n",
        "\n",
        "    # OPTIMIZATION: Pre-compute sample size\n",
        "    n_samples = min(256, len(X_train_scaled))\n",
        "\n",
        "    for n_est in param_grids['Isolation Forest']['n_estimators']:\n",
        "        for max_samp in param_grids['Isolation Forest']['max_samples']:\n",
        "            for contam in param_grids['Isolation Forest']['contamination']:\n",
        "                try:\n",
        "                    # OPTIMIZATION: Use faster parameters\n",
        "                    if_model = IsolationForest(\n",
        "                        n_estimators=n_est,\n",
        "                        max_samples=max_samp,\n",
        "                        contamination=contam,\n",
        "                        max_features=0.5,  # Reduced for speed\n",
        "                        bootstrap=False,  # Faster training\n",
        "                        random_state=42,\n",
        "                        n_jobs=1  # Single job for stability\n",
        "                    )\n",
        "                    # OPTIMIZATION: Train on subset for speed\n",
        "                    if len(X_train_scaled) > MAX_SAMPLES:\n",
        "                        idx = np.random.RandomState(42).choice(\n",
        "                            len(X_train_scaled),\n",
        "                            MAX_SAMPLES,\n",
        "                            replace=False\n",
        "                        )\n",
        "                        if_model.fit(X_train_scaled[idx])\n",
        "                    else:\n",
        "                        if_model.fit(X_train_scaled)\n",
        "\n",
        "                    # OPTIMIZATION: Fast scoring\n",
        "                    val_scores_if = if_model.score_samples(X_val_scaled)  # Faster than decision_function\n",
        "                    ap_score_if = average_precision_score(y_val_scoring, -val_scores_if)\n",
        "\n",
        "                    if ap_score_if > best_if_ap:\n",
        "                        best_if_ap = ap_score_if\n",
        "                        results['if_tuned_model'] = if_model\n",
        "                        results['if_tuned_params'] = {\n",
        "                            'n_estimators': n_est,\n",
        "                            'max_samples': max_samp,\n",
        "                            'contamination': contam\n",
        "                        }\n",
        "                except Exception:\n",
        "                    continue\n",
        "    print(f\" AP: {best_if_ap:.4f}\")\n",
        "\n",
        "    # --- One-Class SVM Tuning ---\n",
        "    print(f\"    OCSVM for {attack_type} - {prefix}...\", end=\"\")\n",
        "    best_ocsvm_ap = -1\n",
        "\n",
        "    # OPTIMIZATION: Train OCSVM on subset\n",
        "    if len(X_train_scaled) > MAX_SAMPLES:\n",
        "        idx = np.random.RandomState(42).choice(\n",
        "            len(X_train_scaled),\n",
        "            min(MAX_SAMPLES, len(X_train_scaled)),\n",
        "            replace=False\n",
        "        )\n",
        "        X_train_ocsvm = X_train_scaled[idx]\n",
        "    else:\n",
        "        X_train_ocsvm = X_train_scaled\n",
        "\n",
        "    for nu_val in param_grids['One-Class SVM']['nu']:\n",
        "        for kernel_val in param_grids['One-Class SVM']['kernel']:\n",
        "            for gamma_val in param_grids['One-Class SVM']['gamma']:\n",
        "                try:\n",
        "                    # OPTIMIZATION: Smaller cache size\n",
        "                    ocsvm_model = OneClassSVM(\n",
        "                        nu=nu_val,\n",
        "                        kernel=kernel_val,\n",
        "                        gamma=gamma_val,\n",
        "                        cache_size=200,  # Reduced for memory\n",
        "                        max_iter=1000,  # Limit iterations\n",
        "                        verbose=False\n",
        "                    )\n",
        "                    ocsvm_model.fit(X_train_ocsvm)\n",
        "\n",
        "                    # OPTIMIZATION: Score in batches\n",
        "                    val_scores_ocsvm = ocsvm_model.decision_function(X_val_scaled)\n",
        "                    ap_score_ocsvm = average_precision_score(y_val_scoring, -val_scores_ocsvm)\n",
        "\n",
        "                    if ap_score_ocsvm > best_ocsvm_ap:\n",
        "                        best_ocsvm_ap = ap_score_ocsvm\n",
        "                        results['ocsvm_tuned_model'] = ocsvm_model\n",
        "                        results['ocsvm_tuned_params'] = {\n",
        "                            'nu': nu_val,\n",
        "                            'kernel': kernel_val,\n",
        "                            'gamma': gamma_val\n",
        "                        }\n",
        "                except Exception:\n",
        "                    continue\n",
        "    print(f\" AP: {best_ocsvm_ap:.4f}\")\n",
        "\n",
        "    # --- Local Outlier Factor Tuning ---\n",
        "    print(f\"    LOF for {attack_type} - {prefix}...\", end=\"\")\n",
        "    best_lof_ap = -1\n",
        "\n",
        "    # OPTIMIZATION: Further reduce LOF training samples\n",
        "    lof_max_samples = min(1000, len(X_train_scaled))\n",
        "    if len(X_train_scaled) > lof_max_samples:\n",
        "        idx = np.random.RandomState(42).choice(\n",
        "            len(X_train_scaled),\n",
        "            lof_max_samples,\n",
        "            replace=False\n",
        "        )\n",
        "        X_train_lof = X_train_scaled[idx]\n",
        "    else:\n",
        "        X_train_lof = X_train_scaled\n",
        "\n",
        "    for n_neigh in param_grids['Local Outlier Factor']['n_neighbors']:\n",
        "        for algo in param_grids['Local Outlier Factor']['algorithm']:\n",
        "            if n_neigh >= len(X_train_lof):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # OPTIMIZATION: Optimized LOF parameters\n",
        "                lof_model = LocalOutlierFactor(\n",
        "                    n_neighbors=n_neigh,\n",
        "                    algorithm=algo,\n",
        "                    leaf_size=20,  # Smaller for faster queries\n",
        "                    novelty=True,\n",
        "                    contamination='auto',\n",
        "                    n_jobs=1  # Single job\n",
        "                )\n",
        "                lof_model.fit(X_train_lof)\n",
        "\n",
        "                # OPTIMIZATION: Score validation subset\n",
        "                val_scores_lof = lof_model.decision_function(X_val_scaled)\n",
        "                ap_score_lof = average_precision_score(y_val_scoring, -val_scores_lof)\n",
        "\n",
        "                if ap_score_lof > best_lof_ap:\n",
        "                    best_lof_ap = ap_score_lof\n",
        "                    results['lof_tuned_model'] = lof_model\n",
        "                    results['lof_tuned_params'] = {\n",
        "                        'n_neighbors': n_neigh,\n",
        "                        'algorithm': algo,\n",
        "                        'contamination': 'auto'\n",
        "                    }\n",
        "            except Exception:\n",
        "                continue\n",
        "    print(f\" AP: {best_lof_ap:.4f}\")\n",
        "\n",
        "    # Store scaler for later use\n",
        "    results['scaler'] = scaler\n",
        "\n",
        "    return (attack_type, prefix), results\n",
        "\n",
        "# Prepare all configurations\n",
        "configs = []\n",
        "for attack_type, methods in results.items():\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        X_train = methods.get(f'{prefix}_X_train')\n",
        "        X_val = methods.get(f'{prefix}_X_val')\n",
        "        y_val = methods.get(f'{prefix}_y_val')\n",
        "\n",
        "        if (X_train is not None and len(X_train) > 10 and\n",
        "            X_val is not None and len(X_val) > 10 and\n",
        "            y_val is not None and np.sum(y_val == 1) > 0):\n",
        "            configs.append((attack_type, prefix, X_train, X_val, y_val))\n",
        "\n",
        "# Run tuning (parallel or sequential)\n",
        "print(f\"\\nFound {len(configs)} configurations to tune\")\n",
        "tuned_results = {}\n",
        "\n",
        "if USE_PARALLEL and len(configs) > 1:\n",
        "    print(\"Running parallel tuning...\")\n",
        "    with ThreadPoolExecutor(max_workers=N_JOBS) as executor:\n",
        "        future_to_config = {\n",
        "            executor.submit(tune_model_for_config, config): config\n",
        "            for config in configs\n",
        "        }\n",
        "\n",
        "        for future in as_completed(future_to_config):\n",
        "            config_key, config_results = future.result()\n",
        "            tuned_results[config_key] = config_results\n",
        "            attack_type, prefix = config_key\n",
        "            print(f\"  Completed {attack_type} - {prefix}\")\n",
        "else:\n",
        "    print(\"Running sequential tuning...\")\n",
        "    for config in configs:\n",
        "        config_key, config_results = tune_model_for_config(config)\n",
        "        tuned_results[config_key] = config_results\n",
        "\n",
        "# Organize results back into main structure\n",
        "for (attack_type, prefix), model_results in tuned_results.items():\n",
        "    if attack_type not in tuned_models_results:\n",
        "        tuned_models_results[attack_type] = {}\n",
        "    tuned_models_results[attack_type][prefix] = model_results\n",
        "\n",
        "# Update main results dictionary\n",
        "update_count = 0\n",
        "for attack_type in tuned_models_results:\n",
        "    for prefix in tuned_models_results[attack_type]:\n",
        "        for key, value in tuned_models_results[attack_type][prefix].items():\n",
        "            if key != 'scaler':  # Store scaler separately\n",
        "                results[attack_type][f'{prefix}_{key}'] = value\n",
        "                update_count += 1\n",
        "            else:\n",
        "                results[attack_type][f'{prefix}_scaler'] = value\n",
        "\n",
        "print(f\"\\nHyperparameter tuning complete!\")\n",
        "print(f\"Updated {update_count} model entries in results dictionary\")\n",
        "print(f\"Total configurations tuned: {len(tuned_results)}\")"
      ],
      "metadata": {
        "id": "TEPbpuxMApEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fb9cb12"
      },
      "source": [
        "# Task\n",
        "Evaluate all models with their tuned parameters (where applicable) on their respective test sets, calculating and storing ROC AUC and Average Precision scores for Isolation Forest, Autoencoder, OCSVM, and LOF models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2da3b513"
      },
      "source": [
        "## Evaluate All Models with Tuned Parameters\n",
        "\n",
        "### Subtask:\n",
        "Re-evaluate the performance of all models (Isolation Forest, Autoencoder, OCSVM, and LOF) on their respective test sets, using the parameters optimized during the hyperparameter tuning step for Isolation Forest, OCSVM, and LOF. Calculate and store ROC AUC and Average Precision scores for all models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bd0a145"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-evaluate all models (Isolation Forest, Autoencoder, OCSVM, and LOF) on their respective test sets. For Isolation Forest, OCSVM, and LOF, I will prioritize using the tuned models and their corresponding scalers. For Autoencoders, I will use their previously calculated test reconstruction errors. I will then calculate and store the ROC AUC and Average Precision scores for all models in the `results` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f1abdb2"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Re-evaluating all models with (tuned) parameters on test data...\")\n",
        "\n",
        "# Helper function to get model and scale X_test\n",
        "def get_model_and_scaled_data(attack_type, prefix, model_type, X_test, results):\n",
        "    model = None\n",
        "    scaler = results[attack_type].get(f'{prefix}_scaler')\n",
        "\n",
        "    # Prioritize tuned model\n",
        "    tuned_model_key = f'{prefix}_{model_type}_tuned_model'\n",
        "    if tuned_model_key in results[attack_type]:\n",
        "        model = results[attack_type][tuned_model_key]\n",
        "    else:\n",
        "        # Fallback to untuned model\n",
        "        untuned_model_key = f'{prefix}_{model_type}_model'\n",
        "        if untuned_model_key in results[attack_type]:\n",
        "            model = results[attack_type][untuned_model_key]\n",
        "\n",
        "    scaled_X_test = X_test\n",
        "    if scaler is not None:\n",
        "        scaled_X_test = scaler.transform(X_test)\n",
        "\n",
        "    return model, scaled_X_test\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        print(f\"  Processing {attack_type.replace('_', ' ').title()} - {prefix.title()}...\")\n",
        "\n",
        "        X_test = methods.get(f'{prefix}_X_test')\n",
        "        y_test = methods.get(f'{prefix}_y_test')\n",
        "\n",
        "        if X_test is None or y_test is None or len(X_test) == 0:\n",
        "            print(f\"    Skipping: Missing test data for {attack_type} - {prefix}\")\n",
        "            continue\n",
        "\n",
        "        # Ensure y_test has both classes for metric calculation\n",
        "        if len(np.unique(y_test)) < 2:\n",
        "            print(f\"    Skipping metrics: Single class in y_test for {attack_type} - {prefix}\")\n",
        "            continue\n",
        "\n",
        "        # --- Isolation Forest ---\n",
        "        if_model, scaled_X_test_if = get_model_and_scaled_data(attack_type, prefix, 'if', X_test, results)\n",
        "        if if_model:\n",
        "            if_test_scores = if_model.decision_function(scaled_X_test_if)\n",
        "            results[attack_type][f'{prefix}_if_roc_auc_tuned'] = roc_auc_score(y_test, if_test_scores)\n",
        "            results[attack_type][f'{prefix}_if_avg_precision_tuned'] = average_precision_score(y_test, if_test_scores)\n",
        "            print(f\"      IF ROC AUC (Tuned): {results[attack_type][f'{prefix}_if_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No IF model found for {attack_type} - {prefix}\")\n",
        "\n",
        "        # --- Autoencoder --- (scores are already computed, just re-evaluate metrics consistently)\n",
        "        ae_test_errors_key = f'{prefix}_test_reconstruction_errors'\n",
        "        if ae_test_errors_key in methods:\n",
        "            ae_test_errors = methods[ae_test_errors_key]\n",
        "            results[attack_type][f'{prefix}_ae_roc_auc_tuned'] = roc_auc_score(y_test, ae_test_errors)\n",
        "            results[attack_type][f'{prefix}_ae_avg_precision_tuned'] = average_precision_score(y_test, ae_test_errors)\n",
        "            print(f\"      AE ROC AUC (Tuned): {results[attack_type][f'{prefix}_ae_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No AE errors found for {attack_type} - {prefix}\")\n",
        "\n",
        "        # --- One-Class SVM (OCSVM) ---\n",
        "        ocsvm_model, scaled_X_test_ocsvm = get_model_and_scaled_data(attack_type, prefix, 'ocsvm', X_test, results)\n",
        "        if ocsvm_model:\n",
        "            ocsvm_test_scores = ocsvm_model.decision_function(scaled_X_test_ocsvm)\n",
        "            results[attack_type][f'{prefix}_ocsvm_roc_auc_tuned'] = roc_auc_score(y_test, ocsvm_test_scores)\n",
        "            results[attack_type][f'{prefix}_ocsvm_avg_precision_tuned'] = average_precision_score(y_test, ocsvm_test_scores)\n",
        "            print(f\"      OCSVM ROC AUC (Tuned): {results[attack_type][f'{prefix}_ocsvm_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No OCSVM model found for {attack_type} - {prefix}\")\n",
        "\n",
        "        # --- Local Outlier Factor (LOF) ---\n",
        "        # Note: For LOF with novelty=True, fit on X_train, then decision_function on new data\n",
        "        # If novelty=False, fit_predict on the whole dataset. Here we assume novelty=True from tuning.\n",
        "        lof_model, scaled_X_test_lof = get_model_and_scaled_data(attack_type, prefix, 'lof', X_test, results)\n",
        "        if lof_model:\n",
        "            lof_test_scores = lof_model.decision_function(scaled_X_test_lof)\n",
        "            results[attack_type][f'{prefix}_lof_roc_auc_tuned'] = roc_auc_score(y_test, lof_test_scores)\n",
        "            results[attack_type][f'{prefix}_lof_avg_precision_tuned'] = average_precision_score(y_test, lof_test_scores)\n",
        "            print(f\"      LOF ROC AUC (Tuned): {results[attack_type][f'{prefix}_lof_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No LOF model found for {attack_type} - {prefix}\")\n",
        "\n",
        "print(\"\\nAll models re-evaluated. Tuned metrics stored in results dictionary.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a861d93"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the grouped bar charts to include the performance metrics (ROC AUC) for the newly evaluated OCSVM and LOF models, alongside Isolation Forest and Autoencoder. This will provide a comprehensive visualization of all models' performance after tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9307c98e"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize an empty list to store performance data for ROC AUC\n",
        "performance_data_roc_auc = []\n",
        "\n",
        "# Define selection method prefixes for iteration consistency\n",
        "selection_method_prefixes = ['heuristic', 'variance', 'pca']\n",
        "\n",
        "# Iterate through each attack_type and its methods in the results dictionary\n",
        "for attack_type, methods in results.items():\n",
        "    # For each attack_type, iterate through the selection_method_prefixes\n",
        "    for prefix in selection_method_prefixes:\n",
        "        # Check for Isolation Forest ROC AUC scores (tuned)\n",
        "        if_roc_auc_key = f'{prefix}_if_roc_auc_tuned'\n",
        "        if if_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Isolation Forest',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[if_roc_auc_key]\n",
        "            })\n",
        "\n",
        "        # Check for Autoencoder ROC AUC scores (tuned - using previous values)\n",
        "        ae_roc_auc_key = f'{prefix}_ae_roc_auc_tuned'\n",
        "        if ae_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Autoencoder',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[ae_roc_auc_key]\n",
        "            })\n",
        "\n",
        "        # Check for OCSVM ROC AUC scores (tuned)\n",
        "        ocsvm_roc_auc_key = f'{prefix}_ocsvm_roc_auc_tuned'\n",
        "        if ocsvm_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'OCSVM',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[ocsvm_roc_auc_key]\n",
        "            })\n",
        "\n",
        "        # Check for LOF ROC AUC scores (tuned)\n",
        "        lof_roc_auc_key = f'{prefix}_lof_roc_auc_tuned'\n",
        "        if lof_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'LOF',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[lof_roc_auc_key]\n",
        "            })\n",
        "\n",
        "# Convert the list of performance data into a Pandas DataFrame\n",
        "roc_auc_df_tuned = pd.DataFrame(performance_data_roc_auc)\n",
        "\n",
        "# Create a grouped bar chart using seaborn.catplot for ROC AUC\n",
        "g_roc_auc = sns.catplot(data=roc_auc_df_tuned, x='Attack Type', y='Score', hue='Model', col='Feature Selection',\n",
        "                kind='bar', palette='viridis', dodge=True, col_wrap=3, height=5, aspect=1.2)\n",
        "\n",
        "# Add appropriate labels and a title\n",
        "g_roc_auc.set_axis_labels('Attack Type', 'ROC AUC Score')\n",
        "g_roc_auc.set_titles('Feature Selection: {col_name}')\n",
        "g_roc_auc.set_xticklabels(rotation=45, ha='right')\n",
        "plt.suptitle('Tuned Model ROC AUC Score Comparison Across Attack Types, Feature Selection, and Models', y=1.02, fontsize=16)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c57fac7a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now update the grouped bar charts to include the Average Precision (AP) for the newly evaluated OCSVM and LOF models, alongside Isolation Forest and Autoencoder. This will provide a comprehensive visualization of all models' performance after tuning, addressing the second part of the visualization subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a438c33b"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize an empty list to store performance data for Average Precision\n",
        "performance_data_avg_precision = []\n",
        "\n",
        "# Define selection method prefixes for iteration consistency\n",
        "selection_method_prefixes = ['heuristic', 'variance', 'pca']\n",
        "\n",
        "# Iterate through each attack_type and its methods in the results dictionary\n",
        "for attack_type, methods in results.items():\n",
        "    # For each attack_type, iterate through the selection_method_prefixes\n",
        "    for prefix in selection_method_prefixes:\n",
        "        # Check for Isolation Forest Average Precision scores (tuned)\n",
        "        if_avg_precision_key = f'{prefix}_if_avg_precision_tuned'\n",
        "        if if_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Isolation Forest',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[if_avg_precision_key]\n",
        "            })\n",
        "\n",
        "        # Check for Autoencoder Average Precision scores (tuned - using previous values)\n",
        "        ae_avg_precision_key = f'{prefix}_ae_avg_precision_tuned'\n",
        "        if ae_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Autoencoder',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[ae_avg_precision_key]\n",
        "            })\n",
        "\n",
        "        # Check for OCSVM Average Precision scores (tuned)\n",
        "        ocsvm_avg_precision_key = f'{prefix}_ocsvm_avg_precision_tuned'\n",
        "        if ocsvm_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'OCSVM',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[ocsvm_avg_precision_key]\n",
        "            })\n",
        "\n",
        "        # Check for LOF Average Precision scores (tuned)\n",
        "        lof_avg_precision_key = f'{prefix}_lof_avg_precision_tuned'\n",
        "        if lof_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'LOF',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[lof_avg_precision_key]\n",
        "            })\n",
        "\n",
        "# Convert the list of performance data into a Pandas DataFrame\n",
        "avg_precision_df_tuned = pd.DataFrame(performance_data_avg_precision)\n",
        "\n",
        "# Create a grouped bar chart using seaborn.catplot for Average Precision\n",
        "g_avg_precision = sns.catplot(data=avg_precision_df_tuned, x='Attack Type', y='Score', hue='Model', col='Feature Selection',\n",
        "                kind='bar', palette='viridis', dodge=True, col_wrap=3, height=5, aspect=1.2)\n",
        "\n",
        "# Add appropriate labels and a title\n",
        "g_avg_precision.set_axis_labels('Attack Type', 'Average Precision Score')\n",
        "g_avg_precision.set_titles('Feature Selection: {col_name}')\n",
        "g_avg_precision.set_xticklabels(rotation=45, ha='right')\n",
        "plt.suptitle('Tuned Model Average Precision Score Comparison Across Attack Types, Feature Selection, and Models', y=1.02, fontsize=16)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c6b7a7d"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the visualization for the best performing model. First, I need to iterate through the `results` dictionary to find the model (Isolation Forest, Autoencoder, OCSVM, or LOF) that achieved the highest ROC AUC score after tuning across all attack types and feature selection methods. Then, I will retrieve the relevant test scores (anomaly scores or reconstruction errors) and the corresponding ground truth labels for this best model and create a scatter plot to visualize its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bfd2253"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Initialize variables to store the best ROC AUC score and its details\n",
        "best_roc_auc_tuned = -1\n",
        "best_attack_type = None\n",
        "best_selection_method = None\n",
        "best_model_type = None\n",
        "\n",
        "# 2. Iterate through the results dictionary to find the combination with the highest ROC AUC score (tuned)\n",
        "for attack_type, methods in results.items():\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        # Check Isolation Forest ROC AUC (tuned)\n",
        "        if_roc_auc_key = f'{prefix}_if_roc_auc_tuned'\n",
        "        if if_roc_auc_key in methods:\n",
        "            score = methods[if_roc_auc_key]\n",
        "            if score > best_roc_auc_tuned:\n",
        "                best_roc_auc_tuned = score\n",
        "                best_attack_type = attack_type\n",
        "                best_selection_method = prefix\n",
        "                best_model_type = 'Isolation Forest'\n",
        "\n",
        "        # Check Autoencoder ROC AUC (tuned)\n",
        "        ae_roc_auc_key = f'{prefix}_ae_roc_auc_tuned'\n",
        "        if ae_roc_auc_key in methods:\n",
        "            score = methods[ae_roc_auc_key]\n",
        "            if score > best_roc_auc_tuned:\n",
        "                best_roc_auc_tuned = score\n",
        "                best_attack_type = attack_type\n",
        "                best_selection_method = prefix\n",
        "                best_model_type = 'Autoencoder'\n",
        "\n",
        "        # Check OCSVM ROC AUC (tuned)\n",
        "        ocsvm_roc_auc_key = f'{prefix}_ocsvm_roc_auc_tuned'\n",
        "        if ocsvm_roc_auc_key in methods:\n",
        "            score = methods[ocsvm_roc_auc_key]\n",
        "            if score > best_roc_auc_tuned:\n",
        "                best_roc_auc_tuned = score\n",
        "                best_attack_type = attack_type\n",
        "                best_selection_method = prefix\n",
        "                best_model_type = 'OCSVM'\n",
        "\n",
        "        # Check LOF ROC AUC (tuned)\n",
        "        lof_roc_auc_key = f'{prefix}_lof_roc_auc_tuned'\n",
        "        if lof_roc_auc_key in methods:\n",
        "            score = methods[lof_roc_auc_key]\n",
        "            if score > best_roc_auc_tuned:\n",
        "                best_roc_auc_tuned = score\n",
        "                best_attack_type = attack_type\n",
        "                best_selection_method = prefix\n",
        "                best_model_type = 'LOF'\n",
        "\n",
        "print(f\"Best Tuned Model Found: {best_model_type} for {best_attack_type.replace('_', ' ').title()} with {best_selection_method.title()} features (ROC AUC: {best_roc_auc_tuned:.4f})\")\n",
        "\n",
        "# 3. Retrieve the test anomaly scores/reconstruction errors and y_test labels for the best combination\n",
        "if best_model_type == 'Isolation Forest':\n",
        "    # Use the tuned model to get decision function if it exists, otherwise fall back to untuned\n",
        "    if f'{best_selection_method}_if_tuned_model' in results[best_attack_type]:\n",
        "        model = results[best_attack_type][f'{best_selection_method}_if_tuned_model']\n",
        "    else:\n",
        "        model = results[best_attack_type][f'{best_selection_method}_isolation_forest_model']\n",
        "\n",
        "    # Scale X_test if scaler exists\n",
        "    X_test_original = results[best_attack_type][f'{best_selection_method}_X_test']\n",
        "    scaler = results[best_attack_type].get(f'{best_selection_method}_scaler')\n",
        "    if scaler is not None:\n",
        "        X_test_scaled = scaler.transform(X_test_original)\n",
        "    else:\n",
        "        X_test_scaled = X_test_original\n",
        "\n",
        "    anomaly_scores = model.decision_function(X_test_scaled)\n",
        "    metric_label = 'Anomaly Score'\n",
        "elif best_model_type == 'Autoencoder':\n",
        "    anomaly_scores = results[best_attack_type][f'{best_selection_method}_test_reconstruction_errors']\n",
        "    metric_label = 'Reconstruction Error'\n",
        "elif best_model_type == 'OCSVM':\n",
        "    if f'{best_selection_method}_ocsvm_tuned_model' in results[best_attack_type]:\n",
        "        model = results[best_attack_type][f'{best_selection_method}_ocsvm_tuned_model']\n",
        "    else:\n",
        "        model = results[best_attack_type][f'{best_selection_method}_ocsvm_model']\n",
        "\n",
        "    X_test_original = results[best_attack_type][f'{best_selection_method}_X_test']\n",
        "    scaler = results[best_attack_type].get(f'{best_selection_method}_scaler')\n",
        "    if scaler is not None:\n",
        "        X_test_scaled = scaler.transform(X_test_original)\n",
        "    else:\n",
        "        X_test_scaled = X_test_original\n",
        "    anomaly_scores = model.decision_function(X_test_scaled)\n",
        "    metric_label = 'Anomaly Score'\n",
        "elif best_model_type == 'LOF':\n",
        "    if f'{best_selection_method}_lof_tuned_model' in results[best_attack_type]:\n",
        "        model = results[best_attack_type][f'{best_selection_method}_lof_tuned_model']\n",
        "    else:\n",
        "        model = results[best_attack_type][f'{best_selection_method}_lof_model']\n",
        "\n",
        "    X_test_original = results[best_attack_type][f'{best_selection_method}_X_test']\n",
        "    scaler = results[best_attack_type].get(f'{best_selection_method}_scaler')\n",
        "    if scaler is not None:\n",
        "        X_test_scaled = scaler.transform(X_test_original)\n",
        "    else:\n",
        "        X_test_scaled = X_test_original\n",
        "    anomaly_scores = model.decision_function(X_test_scaled)\n",
        "    metric_label = 'Anomaly Score'\n",
        "\n",
        "y_test_key = f'{best_selection_method}_y_test'\n",
        "y_test = results[best_attack_type][y_test_key]\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({\n",
        "    metric_label: anomaly_scores,\n",
        "    'Ground Truth': y_test\n",
        "})\n",
        "\n",
        "# Map ground truth labels to more descriptive names for the legend\n",
        "plot_df['Ground Truth Label'] = plot_df['Ground Truth'].map({0: 'Normal', 1: 'Anomaly'})\n",
        "\n",
        "# 4. Create a scatter plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(\n",
        "    data=plot_df,\n",
        "    x=np.random.rand(len(plot_df)) * (plot_df.index.max() - plot_df.index.min()) + plot_df.index.min(), # Jittered x-axis\n",
        "    y=metric_label,\n",
        "    hue='Ground Truth Label',\n",
        "    palette={'Normal': 'skyblue', 'Anomaly': 'red'},\n",
        "    alpha=0.6,\n",
        "    s=10 # Smaller point size for large datasets\n",
        ")\n",
        "\n",
        "# 5. Add appropriate labels and a title\n",
        "plt.xlabel('Data Point Index (Jittered)')\n",
        "plt.ylabel(metric_label)\n",
        "plt.title(f'{best_model_type} Anomaly Detection for {best_attack_type.replace('_', ' ').title()} (Features: {best_selection_method.title()})\\nROC AUC: {best_roc_auc_tuned:.4f}')\n",
        "plt.legend(title='Ground Truth')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b654be53"
      },
      "source": [
        "## Evaluate All Models with Tuned Parameters\n",
        "\n",
        "### Subtask:\n",
        "Re-evaluate the performance of all models (Isolation Forest, Autoencoder, OCSVM, and LOF) on their respective test sets, using the parameters optimized during the hyperparameter tuning step for Isolation Forest, OCSVM, and LOF. Calculate and store ROC AUC and Average Precision scores for all models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "218543b3"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-evaluate all models (Isolation Forest, Autoencoder, OCSVM, and LOF) on their respective test sets. For Isolation Forest, OCSVM, and LOF, I will prioritize using the tuned models and their corresponding scalers. For Autoencoders, I will use their previously calculated test reconstruction errors. I will then calculate and store the ROC AUC and Average Precision scores for all models in the `results` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35dc545c"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Re-evaluating all models with (tuned) parameters on test data...\")\n",
        "\n",
        "# Helper function to get model and scale X_test\n",
        "def get_model_and_scaled_data(attack_type, prefix, model_type, X_test, results):\n",
        "    model = None\n",
        "    scaler = results[attack_type].get(f'{prefix}_scaler')\n",
        "\n",
        "    # Prioritize tuned model\n",
        "    tuned_model_key = f'{prefix}_{model_type}_tuned_model'\n",
        "    if tuned_model_key in results[attack_type]:\n",
        "        model = results[attack_type][tuned_model_key]\n",
        "    else:\n",
        "        # Fallback to untuned model\n",
        "        untuned_model_key = f'{prefix}_{model_type}_model'\n",
        "        if untuned_model_key in results[attack_type]:\n",
        "            model = results[attack_type][untuned_model_key]\n",
        "\n",
        "    scaled_X_test = X_test\n",
        "    if scaler is not None:\n",
        "        scaled_X_test = scaler.transform(X_test)\n",
        "\n",
        "    return model, scaled_X_test\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        print(f\"  Processing {attack_type.replace('_', ' ').title()} - {prefix.title()}...\")\n",
        "\n",
        "        X_test = methods.get(f'{prefix}_X_test')\n",
        "        y_test = methods.get(f'{prefix}_y_test')\n",
        "\n",
        "        if X_test is None or y_test is None or len(X_test) == 0:\n",
        "            print(f\"    Skipping: Missing test data for {attack_type} - {prefix}\")\n",
        "            continue\n",
        "\n",
        "        # Ensure y_test has both classes for metric calculation\n",
        "        if len(np.unique(y_test)) < 2:\n",
        "            print(f\"    Skipping metrics: Single class in y_test for {attack_type} - {prefix}\")\n",
        "            continue\n",
        "\n",
        "        # --- Isolation Forest ---\n",
        "        if_model, scaled_X_test_if = get_model_and_scaled_data(attack_type, prefix, 'if', X_test, results)\n",
        "        if if_model:\n",
        "            if_test_scores = if_model.decision_function(scaled_X_test_if)\n",
        "            results[attack_type][f'{prefix}_if_roc_auc_tuned'] = roc_auc_score(y_test, if_test_scores)\n",
        "            results[attack_type][f'{prefix}_if_avg_precision_tuned'] = average_precision_score(y_test, if_test_scores)\n",
        "            print(f\"      IF ROC AUC (Tuned): {results[attack_type][f'{prefix}_if_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No IF model found for {attack_type} - {prefix}\")\n",
        "\n",
        "        # --- Autoencoder --- (scores are already computed, just re-evaluate metrics consistently)\n",
        "        ae_test_errors_key = f'{prefix}_test_reconstruction_errors'\n",
        "        if ae_test_errors_key in methods:\n",
        "            ae_test_errors = methods[ae_test_errors_key]\n",
        "            results[attack_type][f'{prefix}_ae_roc_auc_tuned'] = roc_auc_score(y_test, ae_test_errors)\n",
        "            results[attack_type][f'{prefix}_ae_avg_precision_tuned'] = average_precision_score(y_test, ae_test_errors)\n",
        "            print(f\"      AE ROC AUC (Tuned): {results[attack_type][f'{prefix}_ae_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No AE errors found for {attack_type} - {prefix}\")\n",
        "\n",
        "        # --- One-Class SVM (OCSVM) ---\n",
        "        ocsvm_model, scaled_X_test_ocsvm = get_model_and_scaled_data(attack_type, prefix, 'ocsvm', X_test, results)\n",
        "        if ocsvm_model:\n",
        "            ocsvm_test_scores = ocsvm_model.decision_function(scaled_X_test_ocsvm)\n",
        "            results[attack_type][f'{prefix}_ocsvm_roc_auc_tuned'] = roc_auc_score(y_test, ocsvm_test_scores)\n",
        "            results[attack_type][f'{prefix}_ocsvm_avg_precision_tuned'] = average_precision_score(y_test, ocsvm_test_scores)\n",
        "            print(f\"      OCSVM ROC AUC (Tuned): {results[attack_type][f'{prefix}_ocsvm_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No OCSVM model found for {attack_type} - {prefix}\")\n",
        "\n",
        "        # --- Local Outlier Factor (LOF) ---\n",
        "        # Note: For LOF with novelty=True, fit on X_train, then decision_function on new data\n",
        "        # If novelty=False, fit_predict on the whole dataset. Here we assume novelty=True from tuning.\n",
        "        lof_model, scaled_X_test_lof = get_model_and_scaled_data(attack_type, prefix, 'lof', X_test, results)\n",
        "        if lof_model:\n",
        "            lof_test_scores = lof_model.decision_function(scaled_X_test_lof)\n",
        "            results[attack_type][f'{prefix}_lof_roc_auc_tuned'] = roc_auc_score(y_test, lof_test_scores)\n",
        "            results[attack_type][f'{prefix}_lof_avg_precision_tuned'] = average_precision_score(y_test, lof_test_scores)\n",
        "            print(f\"      LOF ROC AUC (Tuned): {results[attack_type][f'{prefix}_lof_roc_auc_tuned']:.4f}\")\n",
        "        else:\n",
        "            print(f\"      No LOF model found for {attack_type} - {prefix}\")\n",
        "\n",
        "print(\"\\nAll models re-evaluated. Tuned metrics stored in results dictionary.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be1f2f2a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now update the grouped bar charts to include the performance metrics (ROC AUC) for the newly evaluated OCSVM and LOF models, alongside Isolation Forest and Autoencoder. This will provide a comprehensive visualization of all models' performance after tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50cf17bc"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize an empty list to store performance data for ROC AUC\n",
        "performance_data_roc_auc = []\n",
        "\n",
        "# Define selection method prefixes for iteration consistency\n",
        "selection_method_prefixes = ['heuristic', 'variance', 'pca']\n",
        "\n",
        "# Iterate through each attack_type and its methods in the results dictionary\n",
        "for attack_type, methods in results.items():\n",
        "    # For each attack_type, iterate through the selection_method_prefixes\n",
        "    for prefix in selection_method_prefixes:\n",
        "        # Check for Isolation Forest ROC AUC scores (tuned)\n",
        "        if_roc_auc_key = f'{prefix}_if_roc_auc_tuned'\n",
        "        if if_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Isolation Forest',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[if_roc_auc_key]\n",
        "            })\n",
        "\n",
        "        # Check for Autoencoder ROC AUC scores (tuned - using previous values)\n",
        "        ae_roc_auc_key = f'{prefix}_ae_roc_auc_tuned'\n",
        "        if ae_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Autoencoder',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[ae_roc_auc_key]\n",
        "            })\n",
        "\n",
        "        # Check for OCSVM ROC AUC scores (tuned)\n",
        "        ocsvm_roc_auc_key = f'{prefix}_ocsvm_roc_auc_tuned'\n",
        "        if ocsvm_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'OCSVM',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[ocsvm_roc_auc_key]\n",
        "            })\n",
        "\n",
        "        # Check for LOF ROC AUC scores (tuned)\n",
        "        lof_roc_auc_key = f'{prefix}_lof_roc_auc_tuned'\n",
        "        if lof_roc_auc_key in methods:\n",
        "            performance_data_roc_auc.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'LOF',\n",
        "                'Metric': 'ROC AUC',\n",
        "                'Score': methods[lof_roc_auc_key]\n",
        "            })\n",
        "\n",
        "# Convert the list of performance data into a Pandas DataFrame\n",
        "roc_auc_df_tuned = pd.DataFrame(performance_data_roc_auc)\n",
        "\n",
        "# Create a grouped bar chart using seaborn.catplot for ROC AUC\n",
        "g_roc_auc = sns.catplot(data=roc_auc_df_tuned, x='Attack Type', y='Score', hue='Model', col='Feature Selection',\n",
        "                kind='bar', palette='viridis', dodge=True, col_wrap=3, height=5, aspect=1.2)\n",
        "\n",
        "# Add appropriate labels and a title\n",
        "g_roc_auc.set_axis_labels('Attack Type', 'ROC AUC Score')\n",
        "g_roc_auc.set_titles('Feature Selection: {col_name}')\n",
        "g_roc_auc.set_xticklabels(rotation=45, ha='right')\n",
        "plt.suptitle('Tuned Model ROC AUC Score Comparison Across Attack Types, Feature Selection, and Models', y=1.02, fontsize=16)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab173ec9"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now update the grouped bar charts to include the Average Precision (AP) for the newly evaluated OCSVM and LOF models, alongside Isolation Forest and Autoencoder. This will provide a comprehensive visualization of all models' performance after tuning, addressing the second part of the visualization subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b954d20b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because of a typo in the seaborn function call. I will correct `sns.catcatplot` to `sns.catplot` to resolve the `AttributeError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897332a6"
      },
      "source": [
        "print('--- All Model Performance Metrics (ROC AUC and Average Precision) ---')\n",
        "display(df_fixed.sort_values(by=['attack_type', 'model', 'feature_selection']).reset_index(drop=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "637be082"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize an empty list to store performance data for Average Precision\n",
        "performance_data_avg_precision = []\n",
        "\n",
        "# Define selection method prefixes for iteration consistency\n",
        "selection_method_prefixes = ['heuristic', 'variance', 'pca']\n",
        "\n",
        "# Iterate through each attack_type and its methods in the results dictionary\n",
        "for attack_type, methods in results.items():\n",
        "    # For each attack_type, iterate through the selection_method_prefixes\n",
        "    for prefix in selection_method_prefixes:\n",
        "        # Check for Isolation Forest Average Precision scores (tuned)\n",
        "        if_avg_precision_key = f'{prefix}_if_avg_precision_tuned'\n",
        "        if if_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Isolation Forest',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[if_avg_precision_key]\n",
        "            })\n",
        "\n",
        "        # Check for Autoencoder Average Precision scores (tuned - using previous values)\n",
        "        ae_avg_precision_key = f'{prefix}_ae_avg_precision_tuned'\n",
        "        if ae_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'Autoencoder',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[ae_avg_precision_key]\n",
        "            })\n",
        "\n",
        "        # Check for OCSVM Average Precision scores (tuned)\n",
        "        ocsvm_avg_precision_key = f'{prefix}_ocsvm_avg_precision_tuned'\n",
        "        if ocsvm_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'OCSVM',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[ocsvm_avg_precision_key]\n",
        "            })\n",
        "\n",
        "        # Check for LOF Average Precision scores (tuned)\n",
        "        lof_avg_precision_key = f'{prefix}_lof_avg_precision_tuned'\n",
        "        if lof_avg_precision_key in methods:\n",
        "            performance_data_avg_precision.append({\n",
        "                'Attack Type': attack_type.replace('_', ' ').title(),\n",
        "                'Feature Selection': prefix.title(),\n",
        "                'Model': 'LOF',\n",
        "                'Metric': 'Average Precision',\n",
        "                'Score': methods[lof_avg_precision_key]\n",
        "            })\n",
        "\n",
        "# Convert the list of performance data into a Pandas DataFrame\n",
        "avg_precision_df_tuned = pd.DataFrame(performance_data_avg_precision)\n",
        "\n",
        "# Create a grouped bar chart using seaborn.catplot for Average Precision\n",
        "g_avg_precision = sns.catplot(data=avg_precision_df_tuned, x='Attack Type', y='Score', hue='Model', col='Feature Selection',\n",
        "                kind='bar', palette='viridis', dodge=True, col_wrap=3, height=5, aspect=1.2)\n",
        "\n",
        "# Add appropriate labels and a title\n",
        "g_avg_precision.set_axis_labels('Attack Type', 'Average Precision Score')\n",
        "g_avg_precision.set_titles('Feature Selection: {col_name}')\n",
        "g_avg_precision.set_xticklabels(rotation=45, ha='right')\n",
        "plt.suptitle('Tuned Model Average Precision Score Comparison Across Attack Types, Feature Selection, and Models', y=1.02, fontsize=16)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f52648a0"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the visualization for the best performing model. First, I need to iterate through the `results` dictionary to find the model (Isolation Forest, Autoencoder, OCSVM, or LOF) that achieved the highest ROC AUC score after tuning across all attack types and feature selection methods. Then, I will retrieve the relevant test scores (anomaly scores or reconstruction errors) and the corresponding ground truth labels for this best model and create a scatter plot to visualize its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute and fix score orientation + evaluate consistently\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import traceback\n",
        "\n",
        "fixed_summary = []\n",
        "\n",
        "def safe_transform(scaler, X):\n",
        "    try:\n",
        "        return scaler.transform(X)\n",
        "    except Exception:\n",
        "        return X\n",
        "\n",
        "for attack_type, methods in results.items():\n",
        "    for prefix in ['heuristic', 'variance', 'pca']:\n",
        "        try:\n",
        "            X_test = methods.get(f'{prefix}_X_test')\n",
        "            y_test = methods.get(f'{prefix}_y_test')\n",
        "            if X_test is None or y_test is None or len(X_test) == 0:\n",
        "                continue\n",
        "            if len(np.unique(y_test)) < 2:\n",
        "                # Can't compute ROC/AP with single class\n",
        "                continue\n",
        "\n",
        "            # Prefer prefix-level scaler if stored, fallback to several possible keys\n",
        "            scaler = methods.get(f'{prefix}_scaler') or methods.get(f'{prefix}_lof_scaler') or methods.get('scaler') or None\n",
        "            Xs = safe_transform(scaler, X_test) if scaler is not None else X_test\n",
        "\n",
        "            # Helper to evaluate model whose decision_function/score_samples returns higher for inliers\n",
        "            def eval_model_score(model, Xs, invert=True):\n",
        "                if model is None:\n",
        "                    return None, None\n",
        "                # get score array (higher = inlier by convention for these APIs)\n",
        "                if hasattr(model, 'decision_function'):\n",
        "                    scores = model.decision_function(Xs)\n",
        "                elif hasattr(model, 'score_samples'):\n",
        "                    scores = model.score_samples(Xs)\n",
        "                else:\n",
        "                    return None, None\n",
        "                # Make anomaly_score such that higher = more anomalous\n",
        "                anomaly_score = -scores if invert else scores\n",
        "                roc = roc_auc_score(y_test, anomaly_score)\n",
        "                ap = average_precision_score(y_test, anomaly_score)\n",
        "                return roc, ap\n",
        "\n",
        "            # 1) Isolation Forest (use tuned if available)\n",
        "            if_model = methods.get(f'{prefix}_if_tuned_model') or methods.get(f'{prefix}_isolation_forest_model') or methods.get('isolation_forest_model')\n",
        "            if roc_ap := eval_model_score(if_model, Xs, invert=True):\n",
        "                roc, ap = roc_ap\n",
        "                results[attack_type][f'{prefix}_if_roc_auc_fixed'] = roc\n",
        "                results[attack_type][f'{prefix}_if_ap_fixed'] = ap\n",
        "                fixed_summary.append((attack_type, prefix, 'IsolationForest', roc, ap))\n",
        "\n",
        "            # 2) OCSVM\n",
        "            ocsvm = methods.get(f'{prefix}_ocsvm_tuned_model') or methods.get(f'{prefix}_ocsvm_model') or methods.get('ocsvm_model')\n",
        "            if roc_ap := eval_model_score(ocsvm, Xs, invert=True):\n",
        "                roc, ap = roc_ap\n",
        "                results[attack_type][f'{prefix}_ocsvm_roc_auc_fixed'] = roc\n",
        "                results[attack_type][f'{prefix}_ocsvm_ap_fixed'] = ap\n",
        "                fixed_summary.append((attack_type, prefix, 'OCSVM', roc, ap))\n",
        "\n",
        "            # 3) LOF\n",
        "            lof = methods.get(f'{prefix}_lof_tuned_model') or methods.get(f'{prefix}_lof_model') or methods.get('lof_model')\n",
        "            if roc_ap := eval_model_score(lof, Xs, invert=True):\n",
        "                roc, ap = roc_ap\n",
        "                results[attack_type][f'{prefix}_lof_roc_auc_fixed'] = roc\n",
        "                results[attack_type][f'{prefix}_lof_ap_fixed'] = ap\n",
        "                fixed_summary.append((attack_type, prefix, 'LOF', roc, ap))\n",
        "\n",
        "            # 4) Autoencoder (reconstruction error: higher = more anomalous)\n",
        "            ae_errors = methods.get(f'{prefix}_test_reconstruction_errors')\n",
        "            if ae_errors is not None and len(ae_errors) == len(y_test):\n",
        "                ae_roc = roc_auc_score(y_test, ae_errors)\n",
        "                ae_ap = average_precision_score(y_test, ae_errors)\n",
        "                results[attack_type][f'{prefix}_ae_roc_auc_fixed'] = ae_roc\n",
        "                results[attack_type][f'{prefix}_ae_ap_fixed'] = ae_ap\n",
        "                fixed_summary.append((attack_type, prefix, 'Autoencoder', ae_roc, ae_ap))\n",
        "        except Exception:\n",
        "            print(f'Error evaluating {attack_type} - {prefix}:')\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Print a tidy DataFrame of the fixed metrics\n",
        "df_fixed = pd.DataFrame(fixed_summary, columns=['attack_type', 'feature_selection', 'model', 'roc_auc', 'avg_precision'])\n",
        "if df_fixed.empty:\n",
        "    print('No re-evaluated metrics generated (maybe many y_test arrays had single class).')\n",
        "else:\n",
        "    print(df_fixed.sort_values(['attack_type', 'model', 'feature_selection']).to_string(index=False))"
      ],
      "metadata": {
        "id": "dxCyb718FVvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic: show sample sizes and class balance per config\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "rows = []\n",
        "for attack_type, methods in results.items():\n",
        "    for prefix in ['heuristic','variance','pca']:\n",
        "        X_train = methods.get(f'{prefix}_X_train')\n",
        "        X_val = methods.get(f'{prefix}_X_val')\n",
        "        X_test = methods.get(f'{prefix}_X_test')\n",
        "        y_train = methods.get(f'{prefix}_y_train')\n",
        "        y_val = methods.get(f'{prefix}_y_val')\n",
        "        y_test = methods.get(f'{prefix}_y_test')\n",
        "        rows.append({\n",
        "            'attack_type': attack_type,\n",
        "            'prefix': prefix,\n",
        "            'n_train': None if X_train is None else len(X_train),\n",
        "            'n_val': None if X_val is None else len(X_val),\n",
        "            'n_test': None if X_test is None else len(X_test),\n",
        "            'anoms_train': None if y_train is None else int(np.sum(y_train==1)),\n",
        "            'anoms_val': None if y_val is None else int(np.sum(y_val==1)),\n",
        "            'anoms_test': None if y_test is None else int(np.sum(y_test==1)),\n",
        "        })\n",
        "pd.DataFrame(rows).sort_values(['attack_type','prefix']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "9Hul9e3rF9ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17cded08"
      },
      "source": [
        "# Task\n",
        "The previous execution in cell `ns3ewJZkejKZ` failed to find any features for splitting, leading to subsequent models also failing or being skipped. This was due to two issues in that cell:\n",
        "1.  It incorrectly reset the engineered feature DataFrames (`user_features`, `geo_features`, `priv_user_features`, `port_scan_features`) back to the raw `df` before feature selection. These DataFrames should retain their engineered state from earlier cells.\n",
        "2.  It lacked the core logic to perform heuristic, variance-based, and PCA-based feature selections and populate the `results` dictionary, as intended by the plan.\n",
        "\n",
        "To resolve this, I will perform the following actions:\n",
        "1.  **Modify cell `ns3ewJZkejKZ`**:\n",
        "    *   Remove the lines that overwrite the `user_features`, `geo_features`, `priv_user_features`, and `port_scan_features` DataFrames.\n",
        "    *   Remove the redundant ground truth DataFrame definitions as these are handled correctly in subsequent cells (`r6vK8FaCIdJh`).\n",
        "    *   Insert the feature selection code into the designated placeholder, which will use the already engineered feature DataFrames to populate the `results` dictionary with selected features, scaled feature arrays, and scaler objects for each attack type and selection method.\n",
        "\n",
        "This ensures that the feature engineering performed earlier is preserved and that the `results` dictionary is correctly populated, allowing `rebuild_stratified_splits()` to function as intended.\n",
        "\n",
        "```python\n",
        "# Clear everything and start fresh\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== STARTING FRESH WITH CONSISTENT DATA ===\")\n",
        "\n",
        "# 1. Make sure df is your 500K dataset\n",
        "print(f\"Using df with {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
        "\n",
        "# Removed: 2. Reset all feature DataFrames to df.\n",
        "# The DataFrames `user_features`, `geo_features`, `priv_user_features`, and `port_scan_features`\n",
        "# are expected to be populated with engineered features from previous cells.\n",
        "\n",
        "# Removed: Ground truth DataFrame definitions are handled globally in other cells.\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running feature selection with consistent data...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear old results\n",
        "results = {}\n",
        "\n",
        "# --- START OF INSERTED FEATURE SELECTION CODE ---\n",
        "attack_types_list = ['brute_force', 'geolocation', 'privilege', 'port_scan']\n",
        "\n",
        "for attack_type in attack_types_list:\n",
        "    results[attack_type] = {} # Initialize for this attack type\n",
        "\n",
        "    # Determine which base DataFrame to use for feature selection\n",
        "    # These DataFrames are expected to have been engineered in previous cells.\n",
        "    if attack_type == 'brute_force':\n",
        "        base_df = user_features\n",
        "    elif attack_type == 'geolocation':\n",
        "        base_df = geo_features\n",
        "    elif attack_type == 'privilege':\n",
        "        base_df = priv_user_features\n",
        "    elif attack_type == 'port_scan':\n",
        "        base_df = port_scan_features\n",
        "    else:\n",
        "        print(f\"Warning: Unknown attack type {attack_type}, skipping feature selection.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"  Performing feature selection for {attack_type} (using {base_df.shape[0]:,} rows)...\")\n",
        "\n",
        "    # Heuristic selection\n",
        "    heuristic_cols = select_features_by_attack_type(attack_type)\n",
        "    if heuristic_cols:\n",
        "        X_array, scaler_obj, cols_used = prepare_selected_features(base_df, heuristic_cols)\n",
        "        results[attack_type]['heuristic_cols'] = cols_used\n",
        "        results[attack_type]['heuristic_X_full'] = X_array\n",
        "        results[attack_type]['heuristic_scaler'] = scaler_obj\n",
        "        print(f\"    Heuristic: {len(cols_used)} features selected. Shape: {X_array.shape}\")\n",
        "    else:\n",
        "        print(f\"    Heuristic: No columns found for {attack_type}.\")\n",
        "\n",
        "    # Variance-based selection\n",
        "    variance_cols = variance_based_selection(base_df, top_k=10, corr_threshold=0.9)\n",
        "    if variance_cols:\n",
        "        X_array, scaler_obj, cols_used = prepare_selected_features(base_df, variance_cols)\n",
        "        results[attack_type]['variance_cols'] = cols_used\n",
        "        results[attack_type]['variance_X_full'] = X_array\n",
        "        results[attack_type]['variance_scaler'] = scaler_obj\n",
        "        print(f\"    Variance: {len(cols_used)} features selected. Shape: {X_array.shape}\")\n",
        "    else:\n",
        "        print(f\"    Variance: No columns found for {attack_type}.\")\n",
        "\n",
        "    # PCA-based selection\n",
        "    pca_cols = pca_loadings_selection(base_df, n_components=5, top_k=10)\n",
        "    if pca_cols:\n",
        "        X_array, scaler_obj, cols_used = prepare_selected_features(base_df, pca_cols)\n",
        "        results[attack_type]['pca_cols'] = cols_used\n",
        "        results[attack_type]['pca_X_full'] = X_array\n",
        "        results[attack_type]['pca_scaler'] = scaler_obj\n",
        "        print(f\"    PCA: {len(cols_used)} features selected. Shape: {X_array.shape}\")\n",
        "    else:\n",
        "        print(f\"    PCA: No columns found for {attack_type}.\")\n",
        "\n",
        "# --- END OF INSERTED FEATURE SELECTION CODE ---\n",
        "\n",
        "\n",
        "# 5. THEN run the stratified split\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running stratified splits...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run the rebuild_stratified_splits() function\n",
        "# This function expects user_ground_truth_df and port_scan_ground_truth_df to be globally defined\n",
        "# from prior cells like r6vK8FaCIdJh.\n",
        "results = rebuild_stratified_splits()\n",
        "\n",
        "print(\"\\n DONE! You should now have thousands of anomalies per split.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dce2ee19"
      },
      "source": [
        "## Insert Feature Selection Code\n",
        "\n",
        "### Subtask:\n",
        "Modify cell `ns3ewJZkejKZ` to correctly perform heuristic, variance-based, and PCA-based feature selections and populate the `results` dictionary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cd231ec"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying cell `ns3ewJZkejKZ` to correctly perform feature selection and populate the `results` dictionary. This involves removing the incorrect re-initialization of feature DataFrames and ground truth DataFrames, and inserting the feature selection logic that uses the already engineered feature DataFrames (`user_features`, `geo_features`, `priv_user_features`, `port_scan_features`) from previous steps. I will also add checks to ensure selected columns actually exist in the DataFrames and will populate the `results` dictionary with `_full_X` and corresponding `_scaler` for each method, as `rebuild_stratified_splits()` will later handle the train/val/test splits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd890d98"
      },
      "source": [
        "# Clear everything and start fresh\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== STARTING FEATURE SELECTION PROCESS ===\")\n",
        "\n",
        "# The feature DataFrames (user_features, geo_features, priv_user_features, port_scan_features)\n",
        "# are expected to be populated from previous cells (e.g., 4Ng7N4I7SXyr, OxI_SUmNoXpA, tDZ6Ig3hZI3c, TUWMpm3mZRLe).\n",
        "# We explicitly avoid resetting them here to preserve the engineered features.\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running feature selection...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear old results dictionary for fresh population\n",
        "results = {}\n",
        "\n",
        "# Iterate through each attack type and perform feature selection\n",
        "for attack_type, feature_df_name in [\n",
        "    ('brute_force', 'user_features'),\n",
        "    ('geolocation', 'geo_features'),\n",
        "    ('privilege', 'priv_user_features'),\n",
        "    ('port_scan', 'port_scan_features')\n",
        "]:\n",
        "    # Get the correct feature DataFrame based on its global variable name\n",
        "    feature_df = globals()[feature_df_name]\n",
        "\n",
        "    print(f\"\\nProcessing feature selection for: {attack_type} (using {feature_df_name})\")\n",
        "\n",
        "    results[attack_type] = {} # Initialize results for this specific attack type\n",
        "\n",
        "    # --- Heuristic Selection ---\n",
        "    heuristic_cols = select_features_by_attack_type(attack_type)\n",
        "    # Ensure all heuristic columns exist in the DataFrame before proceeding\n",
        "    if heuristic_cols and all(col in feature_df.columns for col in heuristic_cols):\n",
        "        results[attack_type]['heuristic_cols'] = heuristic_cols\n",
        "        X_heuristic, scaler_heuristic, _ = prepare_selected_features(feature_df, heuristic_cols)\n",
        "        # Store full X and scaler for heuristic method for this attack type\n",
        "        results[attack_type]['heuristic_full_X'] = X_heuristic\n",
        "        results[attack_type]['heuristic_scaler'] = scaler_heuristic\n",
        "        print(f\"  Heuristic selected {len(heuristic_cols)} features. X_full shape: {X_heuristic.shape}\")\n",
        "    else:\n",
        "        print(f\"  Heuristic selection found no valid columns or some columns are missing for {attack_type}.\")\n",
        "        results[attack_type]['heuristic_cols'] = [] # Ensure it's an empty list if no valid columns found\n",
        "\n",
        "\n",
        "    # --- Variance-based Selection ---\n",
        "    # Select top features by variance, then de-correlate\n",
        "    variance_cols = variance_based_selection(feature_df, top_k=10, corr_threshold=0.9)\n",
        "    # Ensure all variance columns exist in the DataFrame before proceeding\n",
        "    if variance_cols and all(col in feature_df.columns for col in variance_cols):\n",
        "        results[attack_type]['variance_cols'] = variance_cols\n",
        "        X_variance, scaler_variance, _ = prepare_selected_features(feature_df, variance_cols)\n",
        "        results[attack_type]['variance_full_X'] = X_variance\n",
        "        results[attack_type]['variance_scaler'] = scaler_variance\n",
        "        print(f\"  Variance selected {len(variance_cols)} features. X_full shape: {X_variance.shape}\")\n",
        "    else:\n",
        "        print(f\"  Variance selection found no valid columns or some columns are missing for {attack_type}.\")\n",
        "        results[attack_type]['variance_cols'] = [] # Ensure it's an empty list if no valid columns found\n",
        "\n",
        "    # --- PCA-based Selection ---\n",
        "    # Select top features based on PCA loadings\n",
        "    pca_cols = pca_loadings_selection(feature_df, n_components=5, top_k=10)\n",
        "    # Ensure all PCA columns exist in the DataFrame before proceeding\n",
        "    if pca_cols and all(col in feature_df.columns for col in pca_cols):\n",
        "        results[attack_type]['pca_cols'] = pca_cols\n",
        "        X_pca, scaler_pca, _ = prepare_selected_features(feature_df, pca_cols)\n",
        "        results[attack_type]['pca_full_X'] = X_pca\n",
        "        results[attack_type]['pca_scaler'] = scaler_pca\n",
        "        print(f\"  PCA selected {len(pca_cols)} features. X_full shape: {X_pca.shape}\")\n",
        "    else:\n",
        "        print(f\"  PCA selection found no valid columns or some columns are missing for {attack_type}.\")\n",
        "        results[attack_type]['pca_cols'] = [] # Ensure it's an empty list if no valid columns found\n",
        "\n",
        "print(\"\\nFeature selection complete. Results dictionary populated.\")\n",
        "\n",
        "# The creation of user_ground_truth_df and port_scan_ground_truth_df,\n",
        "# along with the initial stratified split, will be handled by rebuild_stratified_splits()\n",
        "# in the next step, using the original df and its 'anomaly_label' column.\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running stratified splits...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run the rebuild_stratified_splits() function, which will now use the 'results' dictionary\n",
        "# populated above with 'full_X' data for each selection method.\n",
        "results = rebuild_stratified_splits()\n",
        "\n",
        "print(\"\\n DONE! You should now have thousands of anomalies per split.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a0febe0"
      },
      "source": [
        "### Addressing the \"Heuristic selection found no valid columns\" Issue\n",
        "\n",
        "The previous execution showed that \"Heuristic selection found no valid columns or some columns are missing\" for all attack types, and that Variance and PCA selected only 3 features. This indicates that the DataFrames `user_features`, `geo_features`, `priv_user_features`, and `port_scan_features` were not correctly populated with the engineered features before cell `ns3ewJZkejKZ` was executed. Instead, they likely contained only the original `df` columns.\n",
        "\n",
        "To fix this, you need to re-run the cells that perform feature engineering for each attack type. Please go back and re-execute the following code cells in your notebook in order:\n",
        "\n",
        "*   **`4Ng7N4I7SXyr`**: Generates `user_features` (for brute-force attacks)\n",
        "*   **`OxI_SUmNoXpA`**: Generates `geo_features` (for geolocation attacks)\n",
        "*   **`tDZ6Ig3hZI3c`**: Generates `port_scan_features` (for port scanning attacks) - *Note: This cell might need `df_by_source` to be correctly defined from earlier in the notebook if `port_scan_features` was previously overwritten incorrectly.*\n",
        "*   **`TUWMpm3mZRLe`**: Generates `priv_user_features` (for privilege escalation attacks)\n",
        "\n",
        "After re-running these cells to ensure the feature DataFrames are correctly populated with engineered features, you should then re-execute cell `ns3ewJZkejKZ`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8012c439"
      },
      "source": [
        "### Addressing the \"Heuristic selection found no valid columns\" Issue\n",
        "\n",
        "The previous execution showed that \"Heuristic selection found no valid columns or some columns are missing\" for all attack types, and that Variance and PCA selected only 3 features. This indicates that the DataFrames `user_features`, `geo_features`, `priv_user_features`, and `port_scan_features` were not correctly populated with the engineered features before cell `ns3ewJZkejKZ` was executed. Instead, they likely contained only the original `df` columns.\n",
        "\n",
        "To fix this, you need to re-run the cells that perform feature engineering for each attack type. Please go back and re-execute the following code cells in your notebook in order:\n",
        "\n",
        "*   **`4Ng7N4I7SXyr`**: Generates `user_features` (for brute-force attacks)\n",
        "*   **`OxI_SUmNoXpA`**: Generates `geo_features` (for geolocation attacks)\n",
        "*   **`tDZ6Ig3hZI3c`**: Generates `port_scan_features` (for port scanning attacks) - *Note: This cell might need `df_by_source` to be correctly defined from earlier in the notebook if `port_scan_features` was previously overwritten incorrectly.*\n",
        "*   **`TUWMpm3mZRLe`**: Generates `priv_user_features` (for privilege escalation attacks)\n",
        "\n",
        "After re-running these cells to ensure the feature DataFrames are correctly populated with engineered features, you should then re-execute cell `ns3ewJZkejKZ`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d93a6af"
      },
      "source": [
        "## Re-execute Feature Engineering Cells\n",
        "\n",
        "### Subtask:\n",
        "Re-execute the cells responsible for feature engineering to ensure `user_features`, `geo_features`, `port_scan_features`, and `priv_user_features` DataFrames are correctly populated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9326d04"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires re-executing cell `4Ng7N4I7SXyr` to ensure the `user_features` DataFrame is correctly populated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afc02fc3"
      },
      "source": [
        "# User-level aggregated features\n",
        "user_features = df.groupby('username').agg({\n",
        "    'timestamp': ['count', 'min', 'max'],  # Total activities, first & last login\n",
        "    'status': lambda x: (x == 'Failed').sum(),  # Total failures\n",
        "    'attempts': ['sum', 'mean', 'max'],  # Attempt patterns\n",
        "    'source_ip': 'nunique',  # Unique IPs used\n",
        "    'service': 'nunique',  # Unique services accessed\n",
        "    'server': 'nunique',  # Unique servers accessed\n",
        "    'port': 'nunique'  # Unique ports used\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten the column names\n",
        "user_features.columns = ['username', 'total_activities', 'first_activity', 'last_activity',\n",
        "                        'total_failures', 'total_attempts', 'avg_attempts', 'max_attempts',\n",
        "                        'unique_ips', 'unique_services', 'unique_servers', 'unique_ports']\n",
        "\n",
        "# Calculate derived features\n",
        "user_features['failure_rate'] = user_features['total_failures'] / user_features['total_activities']\n",
        "user_features['activity_duration_days'] = (user_features['last_activity'] - user_features['first_activity']).dt.total_seconds() / (24 * 3600)\n",
        "user_features['activities_per_day'] = user_features['total_activities'] / user_features['activity_duration_days'].replace(0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e16f603"
      },
      "source": [
        "**Reasoning**:\n",
        "Continuing to address the subtask, the next step is to execute cell `OxI_SUmNoXpA` to generate the `geo_features` DataFrame, ensuring it's correctly populated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f08fee28"
      },
      "source": [
        "# Separate aggregations by data type\n",
        "basic_features = df_geo.groupby('username').agg({\n",
        "    'server': 'nunique',\n",
        "    'source_ip': 'nunique',\n",
        "    'service': 'nunique',\n",
        "    'timestamp': ['count', 'min', 'max'],\n",
        "}).reset_index()\n",
        "basic_features.columns = ['username', 'unique_servers', 'unique_ips', 'unique_services', 'total_logins', 'first_seen', 'last_seen']\n",
        "\n",
        "# For numeric features - handle NaN during aggregation\n",
        "numeric_features = df_geo.groupby('username').agg({\n",
        "    'time_since_last_login_hours': ['mean', 'std', 'min', 'max'],\n",
        "    'server_access_count': ['mean', 'max'],\n",
        "    'ip_access_count': ['mean', 'max'],\n",
        "    'attempts': ['mean', 'max', 'sum'],\n",
        "    'port': 'nunique',\n",
        "}).reset_index()\n",
        "numeric_features.columns = ['username', 'avg_time_between_logins', 'std_time_between_logins', 'min_time_between_logins',\n",
        "                           'max_time_between_logins', 'avg_server_access', 'max_server_access', 'avg_ip_access',\n",
        "                           'max_ip_access', 'avg_attempts', 'max_attempts', 'total_attempts', 'unique_ports']\n",
        "\n",
        "# Boolean features\n",
        "boolean_features = df_geo.groupby('username').agg({\n",
        "    'server_change': 'sum',\n",
        "    'ip_change': 'sum',\n",
        "    'consecutive_same_location': 'max',\n",
        "}).reset_index()\n",
        "boolean_features.columns = ['username', 'server_changes', 'ip_changes', 'max_consecutive_same_location']\n",
        "\n",
        "# Status features\n",
        "status_features = df_geo.groupby('username').agg({\n",
        "    'status': lambda x: (x == 'failure').sum(),\n",
        "}).reset_index()\n",
        "status_features.columns = ['username', 'failed_logins']\n",
        "\n",
        "# Merge all features\n",
        "geo_features = basic_features.merge(numeric_features, on='username', how='left')\n",
        "geo_features = geo_features.merge(boolean_features, on='username', how='left')\n",
        "geo_features = geo_features.merge(status_features, on='username', how='left')\n",
        "\n",
        "#handle devision by zero\n",
        "geo_features['active_days'] = (\n",
        "    (geo_features['last_seen'] - geo_features['first_seen']).dt.total_seconds() / (24 * 3600)\n",
        ").replace(0, 1)  # Avoid division by zero\n",
        "\n",
        "# Safe division to prevent NaN\n",
        "geo_features['logins_per_day'] = np.where(\n",
        "    geo_features['active_days'] > 0,\n",
        "    geo_features['total_logins'] / geo_features['active_days'],\n",
        "    0\n",
        ")\n",
        "geo_features['servers_per_login'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    geo_features['unique_servers'] / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "geo_features['ips_per_login'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    geo_features['unique_ips'] / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "geo_features['failure_rate'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    geo_features['failed_logins'] / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "\n",
        "# Handle NaN in std_time_between_logins (occurs when users have only one login)\n",
        "geo_features['std_time_between_logins'] = geo_features['std_time_between_logins'].fillna(0)\n",
        "\n",
        "# Risk scores\n",
        "geo_features['location_volatility'] = np.where(\n",
        "    geo_features['total_logins'] > 0,\n",
        "    (geo_features['server_changes'] + geo_features['ip_changes']) / geo_features['total_logins'],\n",
        "    0\n",
        ")\n",
        "\n",
        "geo_features['geo_risk_score'] = (\n",
        "    geo_features['unique_ips'] * 0.3 +\n",
        "    geo_features['unique_servers'] * 0.3 +\n",
        "    geo_features['location_volatility'] * 0.2 +\n",
        "    geo_features['failure_rate'] * 0.2\n",
        ")\n",
        "\n",
        "# Fill any remaining NaN values with median\n",
        "numeric_cols = geo_features.select_dtypes(include=np.number).columns\n",
        "geo_features[numeric_cols] = geo_features[numeric_cols].fillna(geo_features[numeric_cols].median())\n",
        "\n",
        "print(f\"\\nRemaining NaN values in geo_features: {geo_features.isnull().sum().sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44552f77"
      },
      "source": [
        "**Reasoning**:\n",
        "Continuing to address the subtask, the next step is to execute cell `tDZ6Ig3hZI3c` to generate the `port_scan_features` DataFrame, ensuring it's correctly populated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40a4e94f"
      },
      "source": [
        "port_scan_features = df_by_source.groupby(['source_ip', 'time_window']).agg({\n",
        "    'port': [\n",
        "        'count',  # total_port_attempts\n",
        "        'nunique',  # unique_ports_scanned\n",
        "        lambda x: (x > 10000).sum(),  # high_port_attempts (verify threshold)\n",
        "        'min',  # min_port\n",
        "        'max',  # max_port\n",
        "        lambda x: np.std(x) if len(x) > 1 else 0,  # port_std_dev\n",
        "    ],\n",
        "    'status': [\n",
        "        lambda x: (x == 'Failed').sum(),  # failed_attempts\n",
        "        lambda x: (x == 'Success').sum(),  # success_attempts\n",
        "    ],\n",
        "    'server': 'nunique',  # unique_servers_targeted\n",
        "    'service': 'nunique',  # unique_services_accessed\n",
        "    'username': 'nunique',  # unique_usernames_tried\n",
        "}).reset_index()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d8e62ae"
      },
      "source": [
        "**Reasoning**:\n",
        "Continuing to address the subtask, the next step is to execute cell `tDZ6Ig3hZI3c` to generate the `port_scan_features` DataFrame, ensuring it's correctly populated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "087c2045"
      },
      "source": [
        "port_scan_features = df_by_source.groupby(['source_ip', 'time_window']).agg({\n",
        "    'port': [\n",
        "        'count',  # total_port_attempts\n",
        "        'nunique',  # unique_ports_scanned\n",
        "        lambda x: (x > 10000).sum(),  # high_port_attempts (verify threshold)\n",
        "        'min',  # min_port\n",
        "        'max',  # max_port\n",
        "        lambda x: np.std(x) if len(x) > 1 else 0,  # port_std_dev\n",
        "    ],\n",
        "    'status': [\n",
        "        lambda x: (x == 'Failed').sum(),  # failed_attempts\n",
        "        lambda x: (x == 'Success').sum(),  # success_attempts\n",
        "    ],\n",
        "    'server': 'nunique',  # unique_servers_targeted\n",
        "    'service': 'nunique',  # unique_services_accessed\n",
        "    'username': 'nunique',  # unique_usernames_tried\n",
        "}).reset_index()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}